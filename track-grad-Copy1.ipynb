{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "#         self.gain = nn.Parameter(torch.ones(layersize))\n",
    "#         self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*Euv)\n",
    "        self.beta = (1-eta)*self.beta + eta * (Ev)\n",
    "        for n in self.alpha:\n",
    "            for gain in n:\n",
    "                if(gain != abs(gain)):\n",
    "                    print(\"FLAG!\")\n",
    "        \n",
    "#         self.eta = eta * 0.999\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        grad4 = torch.ones(v4.shape, requires_grad=False).to(device) - v4*v4\n",
    "        \n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o, grad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    avg_grad = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y, grads = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            avg_grad.append([episode, grads.mean()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker), np.transpose(avg_grad))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses, ip_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses, standard_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "        ip_losses += temp_losses\n",
    "        ip_grads += temp_grads\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "        standard_losses += temp_losses\n",
    "        standard_grads += temp_grads\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "    ip_grads = ip_grads/test_runs\n",
    "    standard_grads = standard_grads/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([0, 1])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_grads[0], ip_grads[1], label=\"IP\")\n",
    "    plt.plot(standard_grads[0], standard_grads[1], label=\"Standard\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Slope of activation function\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.598\n",
      "[2] loss: 0.272\n",
      "[3] loss: 0.212\n",
      "[4] loss: 0.182\n",
      "[5] loss: 0.161\n",
      "[6] loss: 0.146\n",
      "[7] loss: 0.130\n",
      "[8] loss: 0.121\n",
      "[9] loss: 0.112\n",
      "[10] loss: 0.104\n",
      "[11] loss: 0.098\n",
      "[12] loss: 0.093\n",
      "[13] loss: 0.088\n",
      "[14] loss: 0.083\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.074\n",
      "[17] loss: 0.073\n",
      "[18] loss: 0.069\n",
      "[19] loss: 0.068\n",
      "[20] loss: 0.066\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 2.165\n",
      "[2] loss: 2.265\n",
      "[3] loss: 2.310\n",
      "[4] loss: 2.310\n",
      "[5] loss: 2.310\n",
      "[6] loss: 2.310\n",
      "[7] loss: 2.310\n",
      "[8] loss: 2.310\n",
      "[9] loss: 2.310\n",
      "[10] loss: 2.310\n",
      "[11] loss: 2.310\n",
      "[12] loss: 2.310\n",
      "[13] loss: 2.310\n",
      "[14] loss: 2.310\n",
      "[15] loss: 2.310\n",
      "[16] loss: 2.310\n",
      "[17] loss: 2.310\n",
      "[18] loss: 2.310\n",
      "[19] loss: 2.310\n",
      "[20] loss: 2.310\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.495\n",
      "[2] loss: 0.234\n",
      "[3] loss: 0.187\n",
      "[4] loss: 0.163\n",
      "[5] loss: 0.146\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.121\n",
      "[8] loss: 0.112\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.096\n",
      "[11] loss: 0.095\n",
      "[12] loss: 0.085\n",
      "[13] loss: 0.084\n",
      "[14] loss: 0.080\n",
      "[15] loss: 0.073\n",
      "[16] loss: 0.072\n",
      "[17] loss: 0.066\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.062\n",
      "[20] loss: 0.060\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 0.769\n",
      "[2] loss: 0.278\n",
      "[3] loss: 0.201\n",
      "[4] loss: 0.168\n",
      "[5] loss: 0.151\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.132\n",
      "[8] loss: 0.129\n",
      "[9] loss: 0.118\n",
      "[10] loss: 0.115\n",
      "[11] loss: 0.121\n",
      "[12] loss: 0.112\n",
      "[13] loss: 0.111\n",
      "[14] loss: 0.105\n",
      "[15] loss: 0.113\n",
      "[16] loss: 0.106\n",
      "[17] loss: 0.104\n",
      "[18] loss: 0.101\n",
      "[19] loss: 0.111\n",
      "[20] loss: 0.107\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.502\n",
      "[2] loss: 0.244\n",
      "[3] loss: 0.188\n",
      "[4] loss: 0.165\n",
      "[5] loss: 0.143\n",
      "[6] loss: 0.131\n",
      "[7] loss: 0.122\n",
      "[8] loss: 0.112\n",
      "[9] loss: 0.107\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.093\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.083\n",
      "[14] loss: 0.081\n",
      "[15] loss: 0.074\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.068\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.063\n",
      "[20] loss: 0.060\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.597\n",
      "[2] loss: 0.296\n",
      "[3] loss: 0.234\n",
      "[4] loss: 0.207\n",
      "[5] loss: 0.189\n",
      "[6] loss: 0.169\n",
      "[7] loss: 0.145\n",
      "[8] loss: 0.151\n",
      "[9] loss: 0.148\n",
      "[10] loss: 0.135\n",
      "[11] loss: 0.131\n",
      "[12] loss: 0.136\n",
      "[13] loss: 0.128\n",
      "[14] loss: 0.128\n",
      "[15] loss: 0.131\n",
      "[16] loss: 0.124\n",
      "[17] loss: 0.119\n",
      "[18] loss: 0.119\n",
      "[19] loss: 0.115\n",
      "[20] loss: 0.111\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.508\n",
      "[2] loss: 0.244\n",
      "[3] loss: 0.194\n",
      "[4] loss: 0.164\n",
      "[5] loss: 0.145\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.120\n",
      "[8] loss: 0.111\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.099\n",
      "[11] loss: 0.093\n",
      "[12] loss: 0.086\n",
      "[13] loss: 0.082\n",
      "[14] loss: 0.080\n",
      "[15] loss: 0.073\n",
      "[16] loss: 0.074\n",
      "[17] loss: 0.069\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.065\n",
      "[20] loss: 0.062\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 0.612\n",
      "[2] loss: 0.316\n",
      "[3] loss: 0.265\n",
      "[4] loss: 0.238\n",
      "[5] loss: 0.211\n",
      "[6] loss: 0.200\n",
      "[7] loss: 0.181\n",
      "[8] loss: 0.178\n",
      "[9] loss: 0.158\n",
      "[10] loss: 0.148\n",
      "[11] loss: 0.145\n",
      "[12] loss: 0.145\n",
      "[13] loss: 0.132\n",
      "[14] loss: 0.144\n",
      "[15] loss: 0.134\n",
      "[16] loss: 0.152\n",
      "[17] loss: 0.145\n",
      "[18] loss: 0.146\n",
      "[19] loss: 0.132\n",
      "[20] loss: 0.137\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 0.489\n",
      "[2] loss: 0.225\n",
      "[3] loss: 0.183\n",
      "[4] loss: 0.159\n",
      "[5] loss: 0.143\n",
      "[6] loss: 0.128\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.114\n",
      "[9] loss: 0.107\n",
      "[10] loss: 0.101\n",
      "[11] loss: 0.097\n",
      "[12] loss: 0.092\n",
      "[13] loss: 0.088\n",
      "[14] loss: 0.082\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.077\n",
      "[17] loss: 0.072\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.067\n",
      "[20] loss: 0.064\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 1.359\n",
      "[2] loss: 0.579\n",
      "[3] loss: 0.350\n",
      "[4] loss: 0.282\n",
      "[5] loss: 0.243\n",
      "[6] loss: 0.218\n",
      "[7] loss: 0.208\n",
      "[8] loss: 0.207\n",
      "[9] loss: 0.190\n",
      "[10] loss: 0.181\n",
      "[11] loss: 0.172\n",
      "[12] loss: 0.181\n",
      "[13] loss: 0.172\n",
      "[14] loss: 0.183\n",
      "[15] loss: 0.180\n",
      "[16] loss: 0.158\n",
      "[17] loss: 0.150\n",
      "[18] loss: 0.151\n",
      "[19] loss: 0.148\n",
      "[20] loss: 0.154\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.637\n",
      "[2] loss: 0.292\n",
      "[3] loss: 0.221\n",
      "[4] loss: 0.183\n",
      "[5] loss: 0.160\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.128\n",
      "[8] loss: 0.122\n",
      "[9] loss: 0.113\n",
      "[10] loss: 0.107\n",
      "[11] loss: 0.099\n",
      "[12] loss: 0.096\n",
      "[13] loss: 0.090\n",
      "[14] loss: 0.086\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.079\n",
      "[17] loss: 0.074\n",
      "[18] loss: 0.074\n",
      "[19] loss: 0.070\n",
      "[20] loss: 0.066\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.876\n",
      "[2] loss: 0.412\n",
      "[3] loss: 0.291\n",
      "[4] loss: 0.233\n",
      "[5] loss: 0.222\n",
      "[6] loss: 0.198\n",
      "[7] loss: 0.186\n",
      "[8] loss: 0.179\n",
      "[9] loss: 0.158\n",
      "[10] loss: 0.159\n",
      "[11] loss: 0.153\n",
      "[12] loss: 0.142\n",
      "[13] loss: 0.147\n",
      "[14] loss: 0.149\n",
      "[15] loss: 0.129\n",
      "[16] loss: 0.146\n",
      "[17] loss: 0.139\n",
      "[18] loss: 0.140\n",
      "[19] loss: 0.129\n",
      "[20] loss: 0.138\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 0.574\n",
      "[2] loss: 0.258\n",
      "[3] loss: 0.201\n",
      "[4] loss: 0.170\n",
      "[5] loss: 0.146\n",
      "[6] loss: 0.131\n",
      "[7] loss: 0.122\n",
      "[8] loss: 0.113\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.100\n",
      "[11] loss: 0.091\n",
      "[12] loss: 0.088\n",
      "[13] loss: 0.086\n",
      "[14] loss: 0.080\n",
      "[15] loss: 0.075\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.071\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.065\n",
      "[20] loss: 0.061\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 1.426\n",
      "[2] loss: 1.126\n",
      "[3] loss: 0.924\n",
      "[4] loss: 0.763\n",
      "[5] loss: 0.653\n",
      "[6] loss: 0.632\n",
      "[7] loss: 0.611\n",
      "[8] loss: 0.605\n",
      "[9] loss: 0.482\n",
      "[10] loss: 0.403\n",
      "[11] loss: 0.241\n",
      "[12] loss: 0.151\n",
      "[13] loss: 0.149\n",
      "[14] loss: 0.138\n",
      "[15] loss: 0.138\n",
      "[16] loss: 0.144\n",
      "[17] loss: 0.137\n",
      "[18] loss: 0.136\n",
      "[19] loss: 0.130\n",
      "[20] loss: 0.143\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.562\n",
      "[2] loss: 0.258\n",
      "[3] loss: 0.199\n",
      "[4] loss: 0.172\n",
      "[5] loss: 0.149\n",
      "[6] loss: 0.134\n",
      "[7] loss: 0.121\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.107\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.092\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.085\n",
      "[14] loss: 0.081\n",
      "[15] loss: 0.075\n",
      "[16] loss: 0.074\n",
      "[17] loss: 0.070\n",
      "[18] loss: 0.067\n",
      "[19] loss: 0.063\n",
      "[20] loss: 0.061\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.734\n",
      "[2] loss: 0.301\n",
      "[3] loss: 0.235\n",
      "[4] loss: 0.206\n",
      "[5] loss: 0.173\n",
      "[6] loss: 0.161\n",
      "[7] loss: 0.144\n",
      "[8] loss: 0.133\n",
      "[9] loss: 0.125\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.126\n",
      "[12] loss: 0.118\n",
      "[13] loss: 0.114\n",
      "[14] loss: 0.119\n",
      "[15] loss: 0.113\n",
      "[16] loss: 0.115\n",
      "[17] loss: 0.099\n",
      "[18] loss: 0.097\n",
      "[19] loss: 0.119\n",
      "[20] loss: 0.114\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.479\n",
      "[2] loss: 0.218\n",
      "[3] loss: 0.177\n",
      "[4] loss: 0.155\n",
      "[5] loss: 0.139\n",
      "[6] loss: 0.122\n",
      "[7] loss: 0.117\n",
      "[8] loss: 0.108\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.095\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.081\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.076\n",
      "[16] loss: 0.069\n",
      "[17] loss: 0.067\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.064\n",
      "[20] loss: 0.061\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.816\n",
      "[2] loss: 0.376\n",
      "[3] loss: 0.248\n",
      "[4] loss: 0.208\n",
      "[5] loss: 0.187\n",
      "[6] loss: 0.173\n",
      "[7] loss: 0.170\n",
      "[8] loss: 0.156\n",
      "[9] loss: 0.142\n",
      "[10] loss: 0.142\n",
      "[11] loss: 0.147\n",
      "[12] loss: 0.138\n",
      "[13] loss: 0.142\n",
      "[14] loss: 0.140\n",
      "[15] loss: 0.135\n",
      "[16] loss: 0.142\n",
      "[17] loss: 0.131\n",
      "[18] loss: 0.142\n",
      "[19] loss: 0.149\n",
      "[20] loss: 0.140\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 0.507\n",
      "[2] loss: 0.238\n",
      "[3] loss: 0.186\n",
      "[4] loss: 0.158\n",
      "[5] loss: 0.141\n",
      "[6] loss: 0.128\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.108\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.097\n",
      "[11] loss: 0.093\n",
      "[12] loss: 0.085\n",
      "[13] loss: 0.084\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.075\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.069\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.065\n",
      "[20] loss: 0.061\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.878\n",
      "[2] loss: 0.372\n",
      "[3] loss: 0.248\n",
      "[4] loss: 0.204\n",
      "[5] loss: 0.179\n",
      "[6] loss: 0.163\n",
      "[7] loss: 0.148\n",
      "[8] loss: 0.151\n",
      "[9] loss: 0.137\n",
      "[10] loss: 0.133\n",
      "[11] loss: 0.134\n",
      "[12] loss: 0.140\n",
      "[13] loss: 0.125\n",
      "[14] loss: 0.122\n",
      "[15] loss: 0.130\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.131\n",
      "[18] loss: 0.108\n",
      "[19] loss: 0.107\n",
      "[20] loss: 0.121\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d8c26ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.1\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
