{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Adam(nn.Module):\n",
    "    def __init__(self, param, betas=(0.9, 0.999), eps=1e-8):\n",
    "        super(Adam, self).__init__()\n",
    "\n",
    "        self.register_buffer('beta1', torch.tensor(betas[0]))\n",
    "        self.register_buffer('beta2', torch.tensor(betas[1]))\n",
    "        self.register_buffer('eps', torch.tensor(eps))\n",
    "\n",
    "        self.register_buffer('m', torch.zeros_like(param))\n",
    "        self.register_buffer('v', torch.zeros_like(param))\n",
    "        self.register_buffer('t', torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, g):\n",
    "        self.m = self.beta1 * self.m + (1-self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1-self.beta2) * g**2\n",
    "        self.t += 1\n",
    "\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "\n",
    "        return m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta*2*Euv\n",
    "        self.beta = self.beta + eta*2*Ev*Euv\n",
    "\n",
    "\n",
    "class inc_BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(inc_BN, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            StDu = u.std()\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * StDu\n",
    "        self.beta = (1-eta)*self.beta + eta * Eu\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "        \n",
    "class og_IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(og_IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = self.alpha*x + self.beta\n",
    "        \n",
    "        return nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta * (1/self.alpha -2*Euv)\n",
    "        self.beta = self.beta + eta * (-2*Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = torch.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = torch.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = torch.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = torch.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = torch.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = torch.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = torch.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = torch.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = torch.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o\n",
    "    \n",
    "class CDNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(CDNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(3*32*32, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = torch.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = torch.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = torch.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = torch.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = torch.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = torch.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = torch.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = torch.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = torch.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs, filename):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cifar_experiment(int_lr, syn_lr, epochs, test_runs, filename):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.968\n",
      "[2] loss: 0.432\n",
      "[3] loss: 0.268\n",
      "[4] loss: 0.207\n",
      "[5] loss: 0.166\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.125\n",
      "[8] loss: 0.111\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.094\n",
      "[11] loss: 0.085\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.079\n",
      "[14] loss: 0.072\n",
      "[15] loss: 0.073\n",
      "[16] loss: 0.066\n",
      "[17] loss: 0.066\n",
      "[18] loss: 0.059\n",
      "[19] loss: 0.059\n",
      "[20] loss: 0.054\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 0.928\n",
      "[2] loss: 0.518\n",
      "[3] loss: 0.370\n",
      "[4] loss: 0.270\n",
      "[5] loss: 0.212\n",
      "[6] loss: 0.184\n",
      "[7] loss: 0.154\n",
      "[8] loss: 0.140\n",
      "[9] loss: 0.126\n",
      "[10] loss: 0.114\n",
      "[11] loss: 0.103\n",
      "[12] loss: 0.096\n",
      "[13] loss: 0.091\n",
      "[14] loss: 0.084\n",
      "[15] loss: 0.080\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.073\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.066\n",
      "[20] loss: 0.056\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 0.966\n",
      "[2] loss: 0.468\n",
      "[3] loss: 0.273\n",
      "[4] loss: 0.204\n",
      "[5] loss: 0.171\n",
      "[6] loss: 0.158\n",
      "[7] loss: 0.148\n",
      "[8] loss: 0.139\n",
      "[9] loss: 0.128\n",
      "[10] loss: 0.119\n",
      "[11] loss: 0.118\n",
      "[12] loss: 0.114\n",
      "[13] loss: 0.107\n",
      "[14] loss: 0.101\n",
      "[15] loss: 0.100\n",
      "[16] loss: 0.094\n",
      "[17] loss: 0.092\n",
      "[18] loss: 0.087\n",
      "[19] loss: 0.086\n",
      "[20] loss: 0.085\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 0.909\n",
      "[2] loss: 0.404\n",
      "[3] loss: 0.310\n",
      "[4] loss: 0.239\n",
      "[5] loss: 0.202\n",
      "[6] loss: 0.181\n",
      "[7] loss: 0.175\n",
      "[8] loss: 0.158\n",
      "[9] loss: 0.151\n",
      "[10] loss: 0.154\n",
      "[11] loss: 0.139\n",
      "[12] loss: 0.138\n",
      "[13] loss: 0.137\n",
      "[14] loss: 0.135\n",
      "[15] loss: 0.128\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.138\n",
      "[18] loss: 0.129\n",
      "[19] loss: 0.130\n",
      "[20] loss: 0.123\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.715\n",
      "[2] loss: 0.276\n",
      "[3] loss: 0.192\n",
      "[4] loss: 0.155\n",
      "[5] loss: 0.128\n",
      "[6] loss: 0.113\n",
      "[7] loss: 0.104\n",
      "[8] loss: 0.096\n",
      "[9] loss: 0.084\n",
      "[10] loss: 0.077\n",
      "[11] loss: 0.073\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.064\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.056\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.054\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 0.786\n",
      "[2] loss: 0.333\n",
      "[3] loss: 0.270\n",
      "[4] loss: 0.208\n",
      "[5] loss: 0.172\n",
      "[6] loss: 0.150\n",
      "[7] loss: 0.129\n",
      "[8] loss: 0.115\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.090\n",
      "[12] loss: 0.082\n",
      "[13] loss: 0.077\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.067\n",
      "[16] loss: 0.066\n",
      "[17] loss: 0.065\n",
      "[18] loss: 0.058\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.052\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 0.717\n",
      "[2] loss: 0.293\n",
      "[3] loss: 0.207\n",
      "[4] loss: 0.159\n",
      "[5] loss: 0.141\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.123\n",
      "[9] loss: 0.110\n",
      "[10] loss: 0.110\n",
      "[11] loss: 0.102\n",
      "[12] loss: 0.097\n",
      "[13] loss: 0.090\n",
      "[14] loss: 0.089\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.085\n",
      "[17] loss: 0.083\n",
      "[18] loss: 0.080\n",
      "[19] loss: 0.075\n",
      "[20] loss: 0.079\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 0.741\n",
      "[2] loss: 0.313\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.185\n",
      "[5] loss: 0.159\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.136\n",
      "[9] loss: 0.126\n",
      "[10] loss: 0.122\n",
      "[11] loss: 0.113\n",
      "[12] loss: 0.113\n",
      "[13] loss: 0.108\n",
      "[14] loss: 0.117\n",
      "[15] loss: 0.119\n",
      "[16] loss: 0.115\n",
      "[17] loss: 0.110\n",
      "[18] loss: 0.111\n",
      "[19] loss: 0.109\n",
      "[20] loss: 0.114\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.511\n",
      "[2] loss: 0.225\n",
      "[3] loss: 0.163\n",
      "[4] loss: 0.145\n",
      "[5] loss: 0.124\n",
      "[6] loss: 0.117\n",
      "[7] loss: 0.102\n",
      "[8] loss: 0.092\n",
      "[9] loss: 0.082\n",
      "[10] loss: 0.081\n",
      "[11] loss: 0.074\n",
      "[12] loss: 0.068\n",
      "[13] loss: 0.070\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.067\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.049\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 3\n",
      "[1] loss: 0.522\n",
      "[2] loss: 0.230\n",
      "[3] loss: 0.174\n",
      "[4] loss: 0.154\n",
      "[5] loss: 0.131\n",
      "[6] loss: 0.123\n",
      "[7] loss: 0.105\n",
      "[8] loss: 0.100\n",
      "[9] loss: 0.093\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.076\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.069\n",
      "[14] loss: 0.058\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.054\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.052\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.044\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 0.513\n",
      "[2] loss: 0.219\n",
      "[3] loss: 0.164\n",
      "[4] loss: 0.145\n",
      "[5] loss: 0.138\n",
      "[6] loss: 0.135\n",
      "[7] loss: 0.128\n",
      "[8] loss: 0.120\n",
      "[9] loss: 0.115\n",
      "[10] loss: 0.106\n",
      "[11] loss: 0.098\n",
      "[12] loss: 0.100\n",
      "[13] loss: 0.091\n",
      "[14] loss: 0.088\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.082\n",
      "[17] loss: 0.083\n",
      "[18] loss: 0.083\n",
      "[19] loss: 0.078\n",
      "[20] loss: 0.076\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.512\n",
      "[2] loss: 0.222\n",
      "[3] loss: 0.175\n",
      "[4] loss: 0.155\n",
      "[5] loss: 0.143\n",
      "[6] loss: 0.131\n",
      "[7] loss: 0.121\n",
      "[8] loss: 0.122\n",
      "[9] loss: 0.118\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.100\n",
      "[12] loss: 0.113\n",
      "[13] loss: 0.115\n",
      "[14] loss: 0.108\n",
      "[15] loss: 0.111\n",
      "[16] loss: 0.118\n",
      "[17] loss: 0.118\n",
      "[18] loss: 0.107\n",
      "[19] loss: 0.097\n",
      "[20] loss: 0.100\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.786\n",
      "[2] loss: 0.300\n",
      "[3] loss: 0.205\n",
      "[4] loss: 0.167\n",
      "[5] loss: 0.139\n",
      "[6] loss: 0.118\n",
      "[7] loss: 0.104\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.087\n",
      "[10] loss: 0.084\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.067\n",
      "[14] loss: 0.072\n",
      "[15] loss: 0.058\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.049\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 0.999\n",
      "[2] loss: 0.507\n",
      "[3] loss: 0.278\n",
      "[4] loss: 0.212\n",
      "[5] loss: 0.179\n",
      "[6] loss: 0.155\n",
      "[7] loss: 0.137\n",
      "[8] loss: 0.122\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.095\n",
      "[11] loss: 0.088\n",
      "[12] loss: 0.082\n",
      "[13] loss: 0.080\n",
      "[14] loss: 0.073\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.066\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.051\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 1.065\n",
      "[2] loss: 0.346\n",
      "[3] loss: 0.217\n",
      "[4] loss: 0.179\n",
      "[5] loss: 0.158\n",
      "[6] loss: 0.143\n",
      "[7] loss: 0.140\n",
      "[8] loss: 0.131\n",
      "[9] loss: 0.123\n",
      "[10] loss: 0.117\n",
      "[11] loss: 0.110\n",
      "[12] loss: 0.105\n",
      "[13] loss: 0.103\n",
      "[14] loss: 0.101\n",
      "[15] loss: 0.093\n",
      "[16] loss: 0.089\n",
      "[17] loss: 0.089\n",
      "[18] loss: 0.086\n",
      "[19] loss: 0.086\n",
      "[20] loss: 0.080\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 0.800\n",
      "[2] loss: 0.335\n",
      "[3] loss: 0.250\n",
      "[4] loss: 0.198\n",
      "[5] loss: 0.167\n",
      "[6] loss: 0.155\n",
      "[7] loss: 0.148\n",
      "[8] loss: 0.135\n",
      "[9] loss: 0.134\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.131\n",
      "[12] loss: 0.124\n",
      "[13] loss: 0.115\n",
      "[14] loss: 0.110\n",
      "[15] loss: 0.109\n",
      "[16] loss: 0.127\n",
      "[17] loss: 0.118\n",
      "[18] loss: 0.117\n",
      "[19] loss: 0.114\n",
      "[20] loss: 0.113\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 0.698\n",
      "[2] loss: 0.296\n",
      "[3] loss: 0.220\n",
      "[4] loss: 0.176\n",
      "[5] loss: 0.147\n",
      "[6] loss: 0.129\n",
      "[7] loss: 0.114\n",
      "[8] loss: 0.103\n",
      "[9] loss: 0.090\n",
      "[10] loss: 0.084\n",
      "[11] loss: 0.078\n",
      "[12] loss: 0.074\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.065\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.056\n",
      "[19] loss: 0.052\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 0.723\n",
      "[2] loss: 0.306\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.187\n",
      "[5] loss: 0.165\n",
      "[6] loss: 0.139\n",
      "[7] loss: 0.127\n",
      "[8] loss: 0.111\n",
      "[9] loss: 0.101\n",
      "[10] loss: 0.089\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.077\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.069\n",
      "[16] loss: 0.062\n",
      "[17] loss: 0.058\n",
      "[18] loss: 0.060\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.051\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 0.717\n",
      "[2] loss: 0.304\n",
      "[3] loss: 0.235\n",
      "[4] loss: 0.190\n",
      "[5] loss: 0.162\n",
      "[6] loss: 0.147\n",
      "[7] loss: 0.135\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.119\n",
      "[10] loss: 0.114\n",
      "[11] loss: 0.102\n",
      "[12] loss: 0.098\n",
      "[13] loss: 0.098\n",
      "[14] loss: 0.090\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.083\n",
      "[17] loss: 0.080\n",
      "[18] loss: 0.083\n",
      "[19] loss: 0.075\n",
      "[20] loss: 0.077\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 0.709\n",
      "[2] loss: 0.308\n",
      "[3] loss: 0.237\n",
      "[4] loss: 0.194\n",
      "[5] loss: 0.168\n",
      "[6] loss: 0.157\n",
      "[7] loss: 0.144\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.136\n",
      "[10] loss: 0.134\n",
      "[11] loss: 0.127\n",
      "[12] loss: 0.123\n",
      "[13] loss: 0.124\n",
      "[14] loss: 0.120\n",
      "[15] loss: 0.119\n",
      "[16] loss: 0.118\n",
      "[17] loss: 0.111\n",
      "[18] loss: 0.118\n",
      "[19] loss: 0.119\n",
      "[20] loss: 0.106\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.742\n",
      "[2] loss: 0.273\n",
      "[3] loss: 0.199\n",
      "[4] loss: 0.154\n",
      "[5] loss: 0.135\n",
      "[6] loss: 0.117\n",
      "[7] loss: 0.104\n",
      "[8] loss: 0.099\n",
      "[9] loss: 0.087\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.079\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.071\n",
      "[14] loss: 0.068\n",
      "[15] loss: 0.069\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.059\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.052\n",
      "[20] loss: 0.054\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 0.851\n",
      "[2] loss: 0.307\n",
      "[3] loss: 0.218\n",
      "[4] loss: 0.180\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.136\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.109\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.094\n",
      "[11] loss: 0.087\n",
      "[12] loss: 0.077\n",
      "[13] loss: 0.075\n",
      "[14] loss: 0.070\n",
      "[15] loss: 0.063\n",
      "[16] loss: 0.071\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.050\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.049\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 0.735\n",
      "[2] loss: 0.282\n",
      "[3] loss: 0.192\n",
      "[4] loss: 0.161\n",
      "[5] loss: 0.149\n",
      "[6] loss: 0.139\n",
      "[7] loss: 0.139\n",
      "[8] loss: 0.134\n",
      "[9] loss: 0.121\n",
      "[10] loss: 0.121\n",
      "[11] loss: 0.113\n",
      "[12] loss: 0.101\n",
      "[13] loss: 0.098\n",
      "[14] loss: 0.095\n",
      "[15] loss: 0.091\n",
      "[16] loss: 0.090\n",
      "[17] loss: 0.084\n",
      "[18] loss: 0.081\n",
      "[19] loss: 0.077\n",
      "[20] loss: 0.074\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.736\n",
      "[2] loss: 0.287\n",
      "[3] loss: 0.203\n",
      "[4] loss: 0.187\n",
      "[5] loss: 0.166\n",
      "[6] loss: 0.134\n",
      "[7] loss: 0.134\n",
      "[8] loss: 0.128\n",
      "[9] loss: 0.129\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.121\n",
      "[12] loss: 0.116\n",
      "[13] loss: 0.107\n",
      "[14] loss: 0.106\n",
      "[15] loss: 0.105\n",
      "[16] loss: 0.103\n",
      "[17] loss: 0.129\n",
      "[18] loss: 0.110\n",
      "[19] loss: 0.119\n",
      "[20] loss: 0.111\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 0.642\n",
      "[2] loss: 0.297\n",
      "[3] loss: 0.232\n",
      "[4] loss: 0.183\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.134\n",
      "[7] loss: 0.117\n",
      "[8] loss: 0.106\n",
      "[9] loss: 0.101\n",
      "[10] loss: 0.091\n",
      "[11] loss: 0.083\n",
      "[12] loss: 0.077\n",
      "[13] loss: 0.076\n",
      "[14] loss: 0.072\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.060\n",
      "[18] loss: 0.058\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.056\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 0.627\n",
      "[2] loss: 0.322\n",
      "[3] loss: 0.270\n",
      "[4] loss: 0.231\n",
      "[5] loss: 0.197\n",
      "[6] loss: 0.164\n",
      "[7] loss: 0.149\n",
      "[8] loss: 0.134\n",
      "[9] loss: 0.111\n",
      "[10] loss: 0.100\n",
      "[11] loss: 0.094\n",
      "[12] loss: 0.083\n",
      "[13] loss: 0.081\n",
      "[14] loss: 0.074\n",
      "[15] loss: 0.071\n",
      "[16] loss: 0.067\n",
      "[17] loss: 0.064\n",
      "[18] loss: 0.063\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.051\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 0.631\n",
      "[2] loss: 0.306\n",
      "[3] loss: 0.231\n",
      "[4] loss: 0.183\n",
      "[5] loss: 0.159\n",
      "[6] loss: 0.147\n",
      "[7] loss: 0.136\n",
      "[8] loss: 0.128\n",
      "[9] loss: 0.121\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.103\n",
      "[12] loss: 0.106\n",
      "[13] loss: 0.097\n",
      "[14] loss: 0.095\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.085\n",
      "[17] loss: 0.084\n",
      "[18] loss: 0.084\n",
      "[19] loss: 0.077\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 0.641\n",
      "[2] loss: 0.309\n",
      "[3] loss: 0.251\n",
      "[4] loss: 0.207\n",
      "[5] loss: 0.188\n",
      "[6] loss: 0.170\n",
      "[7] loss: 0.154\n",
      "[8] loss: 0.153\n",
      "[9] loss: 0.147\n",
      "[10] loss: 0.137\n",
      "[11] loss: 0.132\n",
      "[12] loss: 0.128\n",
      "[13] loss: 0.134\n",
      "[14] loss: 0.141\n",
      "[15] loss: 0.136\n",
      "[16] loss: 0.125\n",
      "[17] loss: 0.123\n",
      "[18] loss: 0.122\n",
      "[19] loss: 0.122\n",
      "[20] loss: 0.130\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.589\n",
      "[2] loss: 0.237\n",
      "[3] loss: 0.185\n",
      "[4] loss: 0.145\n",
      "[5] loss: 0.123\n",
      "[6] loss: 0.111\n",
      "[7] loss: 0.099\n",
      "[8] loss: 0.087\n",
      "[9] loss: 0.084\n",
      "[10] loss: 0.076\n",
      "[11] loss: 0.075\n",
      "[17] loss: 0.054\n",
      "[18] loss: 0.052\n",
      "[19] loss: 0.046\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 0.595\n",
      "[2] loss: 0.236\n",
      "[3] loss: 0.184\n",
      "[4] loss: 0.151\n",
      "[5] loss: 0.138\n",
      "[6] loss: 0.138\n",
      "[7] loss: 0.137\n",
      "[8] loss: 0.126\n",
      "[9] loss: 0.117\n",
      "[10] loss: 0.112\n",
      "[11] loss: 0.111\n",
      "[12] loss: 0.100\n",
      "[13] loss: 0.096\n",
      "[14] loss: 0.094\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.086\n",
      "[17] loss: 0.085\n",
      "[18] loss: 0.081\n",
      "[19] loss: 0.076\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.593\n",
      "[2] loss: 0.241\n",
      "[3] loss: 0.187\n",
      "[4] loss: 0.152\n",
      "[5] loss: 0.134\n",
      "[6] loss: 0.125\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.108\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.101\n",
      "[11] loss: 0.114\n",
      "[12] loss: 0.098\n",
      "[13] loss: 0.105\n",
      "[14] loss: 0.104\n",
      "[15] loss: 0.100\n",
      "[16] loss: 0.095\n",
      "[17] loss: 0.106\n",
      "[18] loss: 0.104\n",
      "[19] loss: 0.098\n",
      "[20] loss: 0.096\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.745\n",
      "[2] loss: 0.326\n",
      "[3] loss: 0.256\n",
      "[4] loss: 0.206\n",
      "[5] loss: 0.166\n",
      "[6] loss: 0.147\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.111\n",
      "[9] loss: 0.106\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.098\n",
      "[12] loss: 0.085\n",
      "[13] loss: 0.083\n",
      "[14] loss: 0.075\n",
      "[15] loss: 0.070\n",
      "[16] loss: 0.068\n",
      "[17] loss: 0.065\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.062\n",
      "[20] loss: 0.063\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 0.742\n",
      "[2] loss: 0.338\n",
      "[3] loss: 0.263\n",
      "[4] loss: 0.203\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.156\n",
      "[7] loss: 0.140\n",
      "[8] loss: 0.126\n",
      "[9] loss: 0.111\n",
      "[10] loss: 0.105\n",
      "[11] loss: 0.096\n",
      "[12] loss: 0.089\n",
      "[13] loss: 0.082\n",
      "[14] loss: 0.074\n",
      "[15] loss: 0.077\n",
      "[16] loss: 0.072\n",
      "[17] loss: 0.063\n",
      "[18] loss: 0.059\n",
      "[19] loss: 0.054\n",
      "[20] loss: 0.060\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 0.723\n",
      "[2] loss: 0.315\n",
      "[3] loss: 0.233\n",
      "[4] loss: 0.199\n",
      "[5] loss: 0.176\n",
      "[6] loss: 0.158\n",
      "[7] loss: 0.155\n",
      "[8] loss: 0.142\n",
      "[9] loss: 0.134\n",
      "[10] loss: 0.127\n",
      "[11] loss: 0.122\n",
      "[12] loss: 0.111\n",
      "[13] loss: 0.107\n",
      "[14] loss: 0.103\n",
      "[15] loss: 0.099\n",
      "[16] loss: 0.094\n",
      "[17] loss: 0.090\n",
      "[18] loss: 0.088\n",
      "[19] loss: 0.083\n",
      "[20] loss: 0.086\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.746\n",
      "[2] loss: 0.324\n",
      "[3] loss: 0.261\n",
      "[4] loss: 0.214\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.176\n",
      "[7] loss: 0.171\n",
      "[8] loss: 0.154\n",
      "[9] loss: 0.145\n",
      "[10] loss: 0.137\n",
      "[11] loss: 0.133\n",
      "[12] loss: 0.145\n",
      "[13] loss: 0.146\n",
      "[14] loss: 0.143\n",
      "[15] loss: 0.130\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.125\n",
      "[18] loss: 0.122\n",
      "[19] loss: 0.122\n",
      "[20] loss: 0.124\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 1.038\n",
      "[2] loss: 0.328\n",
      "[3] loss: 0.235\n",
      "[4] loss: 0.191\n",
      "[5] loss: 0.160\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.120\n",
      "[8] loss: 0.107\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.091\n",
      "[11] loss: 0.087\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.077\n",
      "[14] loss: 0.072\n",
      "[15] loss: 0.069\n",
      "[16] loss: 0.069\n",
      "[17] loss: 0.063\n",
      "[18] loss: 0.061\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.056\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 0.793\n",
      "[2] loss: 0.316\n",
      "[3] loss: 0.239\n",
      "[4] loss: 0.200\n",
      "[5] loss: 0.170\n",
      "[6] loss: 0.155\n",
      "[7] loss: 0.134\n",
      "[8] loss: 0.113\n",
      "[9] loss: 0.108\n",
      "[10] loss: 0.099\n",
      "[11] loss: 0.096\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.086\n",
      "[14] loss: 0.076\n",
      "[15] loss: 0.072\n",
      "[16] loss: 0.070\n",
      "[17] loss: 0.062\n",
      "[18] loss: 0.061\n",
      "[19] loss: 0.063\n",
      "[20] loss: 0.055\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 1.010\n",
      "[2] loss: 0.360\n",
      "[3] loss: 0.251\n",
      "[4] loss: 0.205\n",
      "[5] loss: 0.170\n",
      "[6] loss: 0.156\n",
      "[7] loss: 0.151\n",
      "[8] loss: 0.136\n",
      "[9] loss: 0.132\n",
      "[10] loss: 0.121\n",
      "[11] loss: 0.115\n",
      "[12] loss: 0.110\n",
      "[13] loss: 0.103\n",
      "[14] loss: 0.101\n",
      "[15] loss: 0.095\n",
      "[16] loss: 0.091\n",
      "[17] loss: 0.084\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.080\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.960\n",
      "[2] loss: 0.348\n",
      "[3] loss: 0.244\n",
      "[4] loss: 0.203\n",
      "[5] loss: 0.179\n",
      "[6] loss: 0.163\n",
      "[7] loss: 0.145\n",
      "[8] loss: 0.146\n",
      "[9] loss: 0.131\n",
      "[10] loss: 0.132\n",
      "[11] loss: 0.130\n",
      "[12] loss: 0.124\n",
      "[13] loss: 0.119\n",
      "[14] loss: 0.130\n",
      "[15] loss: 0.119\n",
      "[16] loss: 0.115\n",
      "[17] loss: 0.109\n",
      "[18] loss: 0.107\n",
      "[19] loss: 0.119\n",
      "[20] loss: 0.134\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs, '../images/compare1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 2.052\n",
      "[2] loss: 1.891\n",
      "[3] loss: 1.823\n",
      "[4] loss: 1.772\n",
      "[5] loss: 1.728\n",
      "[6] loss: 1.682\n",
      "[7] loss: 1.641\n",
      "[8] loss: 1.619\n",
      "[9] loss: 1.571\n",
      "[10] loss: 1.536\n",
      "[11] loss: 1.531\n",
      "[12] loss: 1.491\n",
      "[13] loss: 1.463\n",
      "[14] loss: 1.453\n",
      "[15] loss: 1.418\n",
      "[16] loss: 1.399\n",
      "[17] loss: 1.375\n",
      "[18] loss: 1.358\n",
      "[19] loss: 1.342\n",
      "[20] loss: 1.319\n",
      "[21] loss: 1.313\n",
      "[22] loss: 1.289\n",
      "[23] loss: 1.272\n",
      "[24] loss: 1.272\n",
      "[25] loss: 1.247\n",
      "[26] loss: 1.242\n",
      "[27] loss: 1.220\n",
      "[28] loss: 1.214\n",
      "[29] loss: 1.196\n",
      "[30] loss: 1.187\n",
      "[31] loss: 1.177\n",
      "[32] loss: 1.169\n",
      "[33] loss: 1.160\n",
      "[34] loss: 1.142\n",
      "[35] loss: 1.125\n",
      "[36] loss: 1.122\n",
      "[37] loss: 1.124\n",
      "[38] loss: 1.105\n",
      "[39] loss: 1.090\n",
      "[40] loss: 1.100\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 2.053\n",
      "[2] loss: 1.877\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.757\n",
      "[5] loss: 1.706\n",
      "[6] loss: 1.674\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.622\n",
      "[9] loss: 1.590\n",
      "[10] loss: 1.561\n",
      "[11] loss: 1.553\n",
      "[12] loss: 1.513\n",
      "[13] loss: 1.488\n",
      "[14] loss: 1.472\n",
      "[15] loss: 1.453\n",
      "[16] loss: 1.436\n",
      "[17] loss: 1.413\n",
      "[18] loss: 1.411\n",
      "[19] loss: 1.392\n",
      "[20] loss: 1.364\n",
      "[21] loss: 1.375\n",
      "[22] loss: 1.342\n",
      "[23] loss: 1.336\n",
      "[24] loss: 1.341\n",
      "[25] loss: 1.317\n",
      "[26] loss: 1.310\n",
      "[27] loss: 1.286\n",
      "[28] loss: 1.288\n",
      "[29] loss: 1.269\n",
      "[30] loss: 1.264\n",
      "[31] loss: 1.258\n",
      "[32] loss: 1.257\n",
      "[33] loss: 1.243\n",
      "[34] loss: 1.231\n",
      "[35] loss: 1.212\n",
      "[36] loss: 1.207\n",
      "[37] loss: 1.207\n",
      "[38] loss: 1.195\n",
      "[39] loss: 1.182\n",
      "[40] loss: 1.181\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 2.063\n",
      "[2] loss: 1.894\n",
      "[3] loss: 1.810\n",
      "[4] loss: 1.765\n",
      "[5] loss: 1.716\n",
      "[6] loss: 1.683\n",
      "[7] loss: 1.651\n",
      "[8] loss: 1.639\n",
      "[9] loss: 1.603\n",
      "[10] loss: 1.574\n",
      "[11] loss: 1.549\n",
      "[12] loss: 1.528\n",
      "[13] loss: 1.509\n",
      "[14] loss: 1.487\n",
      "[15] loss: 1.457\n",
      "[16] loss: 1.430\n",
      "[17] loss: 1.407\n",
      "[18] loss: 1.390\n",
      "[19] loss: 1.367\n",
      "[20] loss: 1.343\n",
      "[21] loss: 1.334\n",
      "[22] loss: 1.301\n",
      "[23] loss: 1.282\n",
      "[24] loss: 1.264\n",
      "[25] loss: 1.249\n",
      "[26] loss: 1.241\n",
      "[27] loss: 1.222\n",
      "[28] loss: 1.216\n",
      "[29] loss: 1.199\n",
      "[30] loss: 1.200\n",
      "[31] loss: 1.190\n",
      "[32] loss: 1.183\n",
      "[33] loss: 1.180\n",
      "[34] loss: 1.182\n",
      "[35] loss: 1.202\n",
      "[36] loss: 1.198\n",
      "[37] loss: 1.204\n",
      "[38] loss: 1.185\n",
      "[39] loss: 1.197\n",
      "[40] loss: 1.205\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 2.030\n",
      "[2] loss: 1.888\n",
      "[3] loss: 1.826\n",
      "[4] loss: 1.786\n",
      "[5] loss: 1.741\n",
      "[6] loss: 1.725\n",
      "[7] loss: 1.695\n",
      "[8] loss: 1.681\n",
      "[9] loss: 1.660\n",
      "[10] loss: 1.643\n",
      "[11] loss: 1.641\n",
      "[12] loss: 1.608\n",
      "[13] loss: 1.597\n",
      "[14] loss: 1.605\n",
      "[15] loss: 1.579\n",
      "[16] loss: 1.572\n",
      "[17] loss: 1.554\n",
      "[18] loss: 1.554\n",
      "[19] loss: 1.540\n",
      "[20] loss: 1.537\n",
      "[21] loss: 1.530\n",
      "[22] loss: 1.507\n",
      "[23] loss: 1.500\n",
      "[24] loss: 1.491\n",
      "[25] loss: 1.477\n",
      "[26] loss: 1.476\n",
      "[27] loss: 1.464\n",
      "[28] loss: 1.461\n",
      "[29] loss: 1.462\n",
      "[30] loss: 1.455\n",
      "[31] loss: 1.438\n",
      "[32] loss: 1.440\n",
      "[33] loss: 1.423\n",
      "[34] loss: 1.410\n",
      "[35] loss: 1.422\n",
      "[36] loss: 1.404\n",
      "[37] loss: 1.401\n",
      "[38] loss: 1.385\n",
      "[39] loss: 1.386\n",
      "[40] loss: 1.370\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 2.042\n",
      "[2] loss: 1.903\n",
      "[3] loss: 1.825\n",
      "[4] loss: 1.790\n",
      "[5] loss: 1.728\n",
      "[6] loss: 1.700\n",
      "[7] loss: 1.659\n",
      "[8] loss: 1.631\n",
      "[9] loss: 1.595\n",
      "[10] loss: 1.555\n",
      "[11] loss: 1.528\n",
      "[12] loss: 1.497\n",
      "[13] loss: 1.480\n",
      "[14] loss: 1.454\n",
      "[15] loss: 1.430\n",
      "[16] loss: 1.416\n",
      "[17] loss: 1.397\n",
      "[18] loss: 1.389\n",
      "[19] loss: 1.361\n",
      "[20] loss: 1.339\n",
      "[21] loss: 1.327\n",
      "[22] loss: 1.311\n",
      "[23] loss: 1.298\n",
      "[24] loss: 1.285\n",
      "[25] loss: 1.270\n",
      "[26] loss: 1.245\n",
      "[27] loss: 1.230\n",
      "[28] loss: 1.216\n",
      "[29] loss: 1.195\n",
      "[30] loss: 1.189\n",
      "[31] loss: 1.183\n",
      "[32] loss: 1.166\n",
      "[33] loss: 1.152\n",
      "[34] loss: 1.130\n",
      "[35] loss: 1.123\n",
      "[36] loss: 1.103\n",
      "[37] loss: 1.097\n",
      "[38] loss: 1.094\n",
      "[39] loss: 1.064\n",
      "[40] loss: 1.059\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 2.062\n",
      "[2] loss: 1.908\n",
      "[3] loss: 1.825\n",
      "[4] loss: 1.781\n",
      "[5] loss: 1.723\n",
      "[6] loss: 1.690\n",
      "[7] loss: 1.656\n",
      "[8] loss: 1.620\n",
      "[9] loss: 1.588\n",
      "[10] loss: 1.569\n",
      "[11] loss: 1.548\n",
      "[12] loss: 1.522\n",
      "[13] loss: 1.503\n",
      "[14] loss: 1.475\n",
      "[15] loss: 1.451\n",
      "[16] loss: 1.438\n",
      "[17] loss: 1.422\n",
      "[18] loss: 1.414\n",
      "[19] loss: 1.390\n",
      "[20] loss: 1.380\n",
      "[21] loss: 1.369\n",
      "[22] loss: 1.370\n",
      "[23] loss: 1.341\n",
      "[24] loss: 1.342\n",
      "[25] loss: 1.319\n",
      "[26] loss: 1.309\n",
      "[27] loss: 1.294\n",
      "[28] loss: 1.284\n",
      "[29] loss: 1.276\n",
      "[30] loss: 1.262\n",
      "[31] loss: 1.259\n",
      "[32] loss: 1.252\n",
      "[33] loss: 1.220\n",
      "[34] loss: 1.226\n",
      "[35] loss: 1.213\n",
      "[36] loss: 1.205\n",
      "[37] loss: 1.202\n",
      "[38] loss: 1.176\n",
      "[39] loss: 1.171\n",
      "[40] loss: 1.175\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 2.046\n",
      "[2] loss: 1.893\n",
      "[3] loss: 1.829\n",
      "[4] loss: 1.768\n",
      "[5] loss: 1.714\n",
      "[6] loss: 1.677\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.612\n",
      "[9] loss: 1.586\n",
      "[10] loss: 1.553\n",
      "[11] loss: 1.535\n",
      "[12] loss: 1.501\n",
      "[13] loss: 1.483\n",
      "[14] loss: 1.462\n",
      "[15] loss: 1.435\n",
      "[16] loss: 1.412\n",
      "[17] loss: 1.388\n",
      "[18] loss: 1.379\n",
      "[19] loss: 1.357\n",
      "[20] loss: 1.330\n",
      "[21] loss: 1.315\n",
      "[22] loss: 1.296\n",
      "[23] loss: 1.291\n",
      "[24] loss: 1.268\n",
      "[25] loss: 1.262\n",
      "[26] loss: 1.239\n",
      "[27] loss: 1.226\n",
      "[28] loss: 1.232\n",
      "[29] loss: 1.219\n",
      "[30] loss: 1.210\n",
      "[31] loss: 1.218\n",
      "[32] loss: 1.217\n",
      "[33] loss: 1.223\n",
      "[34] loss: 1.218\n",
      "[35] loss: 1.228\n",
      "[36] loss: 1.222\n",
      "[37] loss: 1.232\n",
      "[38] loss: 1.231\n",
      "[39] loss: 1.258\n",
      "[40] loss: 1.251\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 2.054\n",
      "[2] loss: 1.916\n",
      "[3] loss: 1.848\n",
      "[4] loss: 1.810\n",
      "[5] loss: 1.763\n",
      "[6] loss: 1.739\n",
      "[7] loss: 1.725\n",
      "[8] loss: 1.703\n",
      "[9] loss: 1.680\n",
      "[10] loss: 1.662\n",
      "[11] loss: 1.648\n",
      "[12] loss: 1.633\n",
      "[13] loss: 1.607\n",
      "[14] loss: 1.599\n",
      "[15] loss: 1.574\n",
      "[16] loss: 1.570\n",
      "[17] loss: 1.542\n",
      "[18] loss: 1.553\n",
      "[19] loss: 1.536\n",
      "[20] loss: 1.518\n",
      "[21] loss: 1.522\n",
      "[22] loss: 1.505\n",
      "[23] loss: 1.496\n",
      "[24] loss: 1.485\n",
      "[25] loss: 1.483\n",
      "[26] loss: 1.465\n",
      "[27] loss: 1.456\n",
      "[28] loss: 1.456\n",
      "[29] loss: 1.455\n",
      "[30] loss: 1.432\n",
      "[31] loss: 1.432\n",
      "[32] loss: 1.428\n",
      "[33] loss: 1.413\n",
      "[34] loss: 1.409\n",
      "[35] loss: 1.403\n",
      "[36] loss: 1.401\n",
      "[37] loss: 1.394\n",
      "[38] loss: 1.377\n",
      "[39] loss: 1.365\n",
      "[40] loss: 1.355\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 2.045\n",
      "[2] loss: 1.881\n",
      "[3] loss: 1.818\n",
      "[4] loss: 1.766\n",
      "[5] loss: 1.717\n",
      "[6] loss: 1.667\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.605\n",
      "[9] loss: 1.571\n",
      "[10] loss: 1.533\n",
      "[11] loss: 1.518\n",
      "[12] loss: 1.485\n",
      "[13] loss: 1.463\n",
      "[14] loss: 1.436\n",
      "[15] loss: 1.429\n",
      "[16] loss: 1.398\n",
      "[17] loss: 1.385\n",
      "[18] loss: 1.364\n",
      "[19] loss: 1.349\n",
      "[20] loss: 1.331\n",
      "[21] loss: 1.312\n",
      "[22] loss: 1.299\n",
      "[23] loss: 1.283\n",
      "[24] loss: 1.260\n",
      "[25] loss: 1.242\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.314\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.316\n",
      "[19] loss: 2.306\n",
      "[20] loss: 2.075\n",
      "[21] loss: 2.021\n",
      "[22] loss: 1.991\n",
      "[23] loss: 1.926\n",
      "[24] loss: 1.877\n",
      "[25] loss: 1.854\n",
      "[26] loss: 1.825\n",
      "[27] loss: 1.806\n",
      "[28] loss: 1.782\n",
      "[29] loss: 1.767\n",
      "[30] loss: 1.751\n",
      "[31] loss: 1.732\n",
      "[32] loss: 1.718\n",
      "[33] loss: 1.690\n",
      "[34] loss: 1.677\n",
      "[35] loss: 1.658\n",
      "[36] loss: 1.644\n",
      "[37] loss: 1.631\n",
      "[38] loss: 1.623\n",
      "[39] loss: 1.611\n",
      "[40] loss: 1.610\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 2.064\n",
      "[2] loss: 1.905\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.762\n",
      "[5] loss: 1.714\n",
      "[6] loss: 1.668\n",
      "[7] loss: 1.637\n",
      "[8] loss: 1.615\n",
      "[9] loss: 1.592\n",
      "[10] loss: 1.562\n",
      "[11] loss: 1.540\n",
      "[12] loss: 1.517\n",
      "[13] loss: 1.482\n",
      "[14] loss: 1.454\n",
      "[15] loss: 1.435\n",
      "[16] loss: 1.414\n",
      "[17] loss: 1.388\n",
      "[18] loss: 1.371\n",
      "[19] loss: 1.341\n",
      "[20] loss: 1.332\n",
      "[21] loss: 1.308\n",
      "[22] loss: 1.293\n",
      "[23] loss: 1.279\n",
      "[24] loss: 1.257\n",
      "[25] loss: 1.254\n",
      "[26] loss: 1.246\n",
      "[27] loss: 1.230\n",
      "[28] loss: 1.208\n",
      "[29] loss: 1.208\n",
      "[30] loss: 1.198\n",
      "[31] loss: 1.214\n",
      "[32] loss: 1.221\n",
      "[33] loss: 1.226\n",
      "[34] loss: 1.224\n",
      "[35] loss: 1.214\n",
      "[36] loss: 1.217\n",
      "[37] loss: 1.230\n",
      "[38] loss: 1.245\n",
      "[39] loss: 1.251\n",
      "[40] loss: 1.255\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 2.042\n",
      "[2] loss: 1.895\n",
      "[3] loss: 1.836\n",
      "[4] loss: 1.801\n",
      "[5] loss: 1.765\n",
      "[6] loss: 1.731\n",
      "[7] loss: 1.729\n",
      "[8] loss: 1.690\n",
      "[9] loss: 1.664\n",
      "[10] loss: 1.650\n",
      "[11] loss: 1.639\n",
      "[12] loss: 1.609\n",
      "[13] loss: 1.603\n",
      "[14] loss: 1.583\n",
      "[15] loss: 1.582\n",
      "[16] loss: 1.568\n",
      "[17] loss: 1.561\n",
      "[18] loss: 1.543\n",
      "[19] loss: 1.538\n",
      "[20] loss: 1.528\n",
      "[21] loss: 1.516\n",
      "[22] loss: 1.508\n",
      "[23] loss: 1.501\n",
      "[24] loss: 1.492\n",
      "[25] loss: 1.486\n",
      "[26] loss: 1.485\n",
      "[27] loss: 1.477\n",
      "[28] loss: 1.457\n",
      "[29] loss: 1.450\n",
      "[30] loss: 1.448\n",
      "[31] loss: 1.436\n",
      "[32] loss: 1.431\n",
      "[33] loss: 1.421\n",
      "[34] loss: 1.420\n",
      "[35] loss: 1.409\n",
      "[36] loss: 1.399\n",
      "[37] loss: 1.396\n",
      "[38] loss: 1.386\n",
      "[39] loss: 1.385\n",
      "[40] loss: 1.368\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 2.090\n",
      "[2] loss: 1.949\n",
      "[3] loss: 1.835\n",
      "[4] loss: 1.758\n",
      "[5] loss: 1.722\n",
      "[6] loss: 1.660\n",
      "[7] loss: 1.616\n",
      "[8] loss: 1.582\n",
      "[9] loss: 1.565\n",
      "[10] loss: 1.524\n",
      "[11] loss: 1.505\n",
      "[12] loss: 1.478\n",
      "[13] loss: 1.456\n",
      "[14] loss: 1.447\n",
      "[15] loss: 1.417\n",
      "[16] loss: 1.403\n",
      "[17] loss: 1.386\n",
      "[18] loss: 1.372\n",
      "[19] loss: 1.348\n",
      "[20] loss: 1.342\n",
      "[21] loss: 1.321\n",
      "[22] loss: 1.308\n",
      "[23] loss: 1.281\n",
      "[24] loss: 1.271\n",
      "[25] loss: 1.266\n",
      "[26] loss: 1.240\n",
      "[27] loss: 1.241\n",
      "[28] loss: 1.207\n",
      "[29] loss: 1.206\n",
      "[30] loss: 1.195\n",
      "[31] loss: 1.174\n",
      "[32] loss: 1.177\n",
      "[33] loss: 1.154\n",
      "[34] loss: 1.147\n",
      "[35] loss: 1.132\n",
      "[36] loss: 1.122\n",
      "[37] loss: 1.109\n",
      "[38] loss: 1.093\n",
      "[39] loss: 1.103\n",
      "[40] loss: 1.092\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 2.090\n",
      "[2] loss: 1.908\n",
      "[3] loss: 1.818\n",
      "[4] loss: 1.748\n",
      "[5] loss: 1.710\n",
      "[6] loss: 1.658\n",
      "[7] loss: 1.626\n",
      "[8] loss: 1.595\n",
      "[9] loss: 1.571\n",
      "[10] loss: 1.546\n",
      "[11] loss: 1.518\n",
      "[12] loss: 1.504\n",
      "[13] loss: 1.477\n",
      "[14] loss: 1.456\n",
      "[15] loss: 1.448\n",
      "[16] loss: 1.431\n",
      "[17] loss: 1.404\n",
      "[18] loss: 1.402\n",
      "[19] loss: 1.383\n",
      "[20] loss: 1.380\n",
      "[21] loss: 1.352\n",
      "[22] loss: 1.355\n",
      "[23] loss: 1.333\n",
      "[24] loss: 1.320\n",
      "[25] loss: 1.332\n",
      "[26] loss: 1.303\n",
      "[27] loss: 1.296\n",
      "[28] loss: 1.287\n",
      "[29] loss: 1.270\n",
      "[30] loss: 1.277\n",
      "[31] loss: 1.259\n",
      "[32] loss: 1.254\n",
      "[33] loss: 1.244\n",
      "[34] loss: 1.245\n",
      "[35] loss: 1.233\n",
      "[36] loss: 1.218\n",
      "[37] loss: 1.203\n",
      "[38] loss: 1.200\n",
      "[39] loss: 1.193\n",
      "[40] loss: 1.203\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 2.094\n",
      "[2] loss: 1.931\n",
      "[3] loss: 1.836\n",
      "[4] loss: 1.763\n",
      "[5] loss: 1.720\n",
      "[6] loss: 1.678\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.597\n",
      "[9] loss: 1.574\n",
      "[10] loss: 1.547\n",
      "[11] loss: 1.516\n",
      "[12] loss: 1.497\n",
      "[13] loss: 1.469\n",
      "[14] loss: 1.448\n",
      "[15] loss: 1.421\n",
      "[16] loss: 1.397\n",
      "[17] loss: 1.383\n",
      "[18] loss: 1.359\n",
      "[19] loss: 1.345\n",
      "[20] loss: 1.332\n",
      "[21] loss: 1.307\n",
      "[22] loss: 1.300\n",
      "[23] loss: 1.277\n",
      "[24] loss: 1.261\n",
      "[25] loss: 1.259\n",
      "[26] loss: 1.236\n",
      "[27] loss: 1.226\n",
      "[28] loss: 1.209\n",
      "[29] loss: 1.210\n",
      "[30] loss: 1.206\n",
      "[31] loss: 1.211\n",
      "[32] loss: 1.216\n",
      "[33] loss: 1.208\n",
      "[34] loss: 1.221\n",
      "[35] loss: 1.207\n",
      "[36] loss: 1.220\n",
      "[37] loss: 1.229\n",
      "[38] loss: 1.233\n",
      "[39] loss: 1.245\n",
      "[40] loss: 1.251\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 2.094\n",
      "[2] loss: 1.953\n",
      "[3] loss: 1.862\n",
      "[4] loss: 1.808\n",
      "[5] loss: 1.786\n",
      "[6] loss: 1.738\n",
      "[7] loss: 1.723\n",
      "[8] loss: 1.704\n",
      "[9] loss: 1.680\n",
      "[10] loss: 1.657\n",
      "[11] loss: 1.645\n",
      "[12] loss: 1.624\n",
      "[13] loss: 1.608\n",
      "[14] loss: 1.591\n",
      "[15] loss: 1.574\n",
      "[16] loss: 1.562\n",
      "[17] loss: 1.558\n",
      "[18] loss: 1.540\n",
      "[19] loss: 1.528\n",
      "[20] loss: 1.522\n",
      "[21] loss: 1.507\n",
      "[22] loss: 1.502\n",
      "[23] loss: 1.494\n",
      "[24] loss: 1.487\n",
      "[25] loss: 1.483\n",
      "[26] loss: 1.458\n",
      "[27] loss: 1.476\n",
      "[28] loss: 1.455\n",
      "[29] loss: 1.446\n",
      "[30] loss: 1.441\n",
      "[31] loss: 1.433\n",
      "[32] loss: 1.426\n",
      "[33] loss: 1.421\n",
      "[34] loss: 1.413\n",
      "[35] loss: 1.396\n",
      "[36] loss: 1.408\n",
      "[37] loss: 1.381\n",
      "[38] loss: 1.379\n",
      "[39] loss: 1.375\n",
      "[40] loss: 1.365\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 2.026\n",
      "[2] loss: 1.882\n",
      "[3] loss: 1.825\n",
      "[4] loss: 1.763\n",
      "[5] loss: 1.718\n",
      "[6] loss: 1.677\n",
      "[7] loss: 1.640\n",
      "[8] loss: 1.600\n",
      "[9] loss: 1.564\n",
      "[10] loss: 1.532\n",
      "[11] loss: 1.499\n",
      "[12] loss: 1.477\n",
      "[13] loss: 1.463\n",
      "[14] loss: 1.427\n",
      "[15] loss: 1.403\n",
      "[16] loss: 1.395\n",
      "[17] loss: 1.377\n",
      "[18] loss: 1.358\n",
      "[19] loss: 1.345\n",
      "[20] loss: 1.322\n",
      "[21] loss: 1.309\n",
      "[22] loss: 1.288\n",
      "[23] loss: 1.267\n",
      "[24] loss: 1.257\n",
      "[25] loss: 1.248\n",
      "[26] loss: 1.234\n",
      "[27] loss: 1.219\n",
      "[28] loss: 1.208\n",
      "[29] loss: 1.197\n",
      "[30] loss: 1.173\n",
      "[31] loss: 1.166\n",
      "[32] loss: 1.166\n",
      "[33] loss: 1.144\n",
      "[34] loss: 1.136\n",
      "[35] loss: 1.126\n",
      "[36] loss: 1.117\n",
      "[37] loss: 1.103\n",
      "[38] loss: 1.103\n",
      "[39] loss: 1.095\n",
      "[40] loss: 1.081\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 2.050\n",
      "[2] loss: 1.899\n",
      "[3] loss: 1.844\n",
      "[4] loss: 1.769\n",
      "[5] loss: 1.732\n",
      "[6] loss: 1.690\n",
      "[7] loss: 1.656\n",
      "[8] loss: 1.616\n",
      "[9] loss: 1.591\n",
      "[10] loss: 1.556\n",
      "[11] loss: 1.545\n",
      "[12] loss: 1.504\n",
      "[13] loss: 1.492\n",
      "[14] loss: 1.467\n",
      "[15] loss: 1.443\n",
      "[16] loss: 1.443\n",
      "[17] loss: 1.414\n",
      "[18] loss: 1.407\n",
      "[19] loss: 1.393\n",
      "[20] loss: 1.370\n",
      "[21] loss: 1.369\n",
      "[22] loss: 1.358\n",
      "[23] loss: 1.339\n",
      "[24] loss: 1.315\n",
      "[25] loss: 1.316\n",
      "[26] loss: 1.302\n",
      "[27] loss: 1.299\n",
      "[28] loss: 1.284\n",
      "[29] loss: 1.275\n",
      "[30] loss: 1.259\n",
      "[31] loss: 1.250\n",
      "[32] loss: 1.245\n",
      "[33] loss: 1.221\n",
      "[34] loss: 1.214\n",
      "[35] loss: 1.211\n",
      "[36] loss: 1.217\n",
      "[37] loss: 1.182\n",
      "[38] loss: 1.189\n",
      "[39] loss: 1.184\n",
      "[40] loss: 1.162\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 2.037\n",
      "[2] loss: 1.905\n",
      "[3] loss: 1.813\n",
      "[4] loss: 1.755\n",
      "[5] loss: 1.697\n",
      "[6] loss: 1.648\n",
      "[7] loss: 1.618\n",
      "[8] loss: 1.584\n",
      "[9] loss: 1.559\n",
      "[10] loss: 1.539\n",
      "[11] loss: 1.519\n",
      "[12] loss: 1.500\n",
      "[13] loss: 1.487\n",
      "[14] loss: 1.455\n",
      "[15] loss: 1.434\n",
      "[16] loss: 1.423\n",
      "[17] loss: 1.391\n",
      "[18] loss: 1.372\n",
      "[19] loss: 1.360\n",
      "[20] loss: 1.331\n",
      "[21] loss: 1.316\n",
      "[22] loss: 1.299\n",
      "[23] loss: 1.290\n",
      "[24] loss: 1.271\n",
      "[25] loss: 1.260\n",
      "[26] loss: 1.250\n",
      "[27] loss: 1.243\n",
      "[28] loss: 1.235\n",
      "[29] loss: 1.230\n",
      "[30] loss: 1.232\n",
      "[31] loss: 1.243\n",
      "[32] loss: 1.227\n",
      "[33] loss: 1.225\n",
      "[34] loss: 1.224\n",
      "[35] loss: 1.234\n",
      "[36] loss: 1.250\n",
      "[37] loss: 1.226\n",
      "[38] loss: 1.238\n",
      "[39] loss: 1.250\n",
      "[40] loss: 1.261\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 2.058\n",
      "[2] loss: 1.930\n",
      "[3] loss: 1.873\n",
      "[4] loss: 1.828\n",
      "[5] loss: 1.780\n",
      "[6] loss: 1.759\n",
      "[7] loss: 1.732\n",
      "[8] loss: 1.708\n",
      "[9] loss: 1.688\n",
      "[10] loss: 1.659\n",
      "[11] loss: 1.636\n",
      "[12] loss: 1.623\n",
      "[13] loss: 1.615\n",
      "[14] loss: 1.585\n",
      "[15] loss: 1.572\n",
      "[16] loss: 1.564\n",
      "[17] loss: 1.563\n",
      "[18] loss: 1.549\n",
      "[19] loss: 1.545\n",
      "[20] loss: 1.534\n",
      "[21] loss: 1.511\n",
      "[22] loss: 1.509\n",
      "[23] loss: 1.501\n",
      "[24] loss: 1.488\n",
      "[25] loss: 1.490\n",
      "[26] loss: 1.472\n",
      "[27] loss: 1.464\n",
      "[28] loss: 1.471\n",
      "[29] loss: 1.474\n",
      "[30] loss: 1.455\n",
      "[31] loss: 1.450\n",
      "[32] loss: 1.457\n",
      "[33] loss: 1.434\n",
      "[34] loss: 1.423\n",
      "[35] loss: 1.419\n",
      "[36] loss: 1.418\n",
      "[37] loss: 1.418\n",
      "[38] loss: 1.410\n",
      "[39] loss: 1.396\n",
      "[40] loss: 1.394\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.277\n",
      "[3] loss: 2.077\n",
      "[4] loss: 1.997\n",
      "[5] loss: 1.906\n",
      "[6] loss: 1.833\n",
      "[7] loss: 1.762\n",
      "[8] loss: 1.708\n",
      "[9] loss: 1.665\n",
      "[10] loss: 1.636\n",
      "[11] loss: 1.595\n",
      "[12] loss: 1.558\n",
      "[13] loss: 1.532\n",
      "[14] loss: 1.505\n",
      "[15] loss: 1.474\n",
      "[16] loss: 1.472\n",
      "[17] loss: 1.434\n",
      "[18] loss: 1.420\n",
      "[19] loss: 1.398\n",
      "[20] loss: 1.383\n",
      "[21] loss: 1.359\n",
      "[22] loss: 1.346\n",
      "[23] loss: 1.327\n",
      "[24] loss: 1.313\n",
      "[25] loss: 1.307\n",
      "[26] loss: 1.285\n",
      "[27] loss: 1.272\n",
      "[28] loss: 1.256\n",
      "[29] loss: 1.235\n",
      "[30] loss: 1.225\n",
      "[31] loss: 1.226\n",
      "[32] loss: 1.209\n",
      "[33] loss: 1.197\n",
      "[34] loss: 1.183\n",
      "[35] loss: 1.177\n",
      "[36] loss: 1.171\n",
      "[37] loss: 1.151\n",
      "[38] loss: 1.148\n",
      "[39] loss: 1.140\n",
      "[40] loss: 1.137\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.312\n",
      "[3] loss: 2.312\n",
      "[4] loss: 2.322\n",
      "[5] loss: 2.312\n",
      "[6] loss: 2.312\n",
      "[7] loss: 2.312\n",
      "[8] loss: 2.312\n",
      "[9] loss: 2.312\n",
      "[10] loss: 2.312\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.312\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.312\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.312\n",
      "[22] loss: 2.312\n",
      "[23] loss: 2.312\n",
      "[24] loss: 2.312\n",
      "[25] loss: 2.312\n",
      "[26] loss: 2.312\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.312\n",
      "[29] loss: 2.312\n",
      "[30] loss: 2.312\n",
      "[31] loss: 2.312\n",
      "[32] loss: 2.312\n",
      "[33] loss: 2.311\n",
      "[34] loss: 2.311\n",
      "[35] loss: 2.308\n",
      "[36] loss: 2.306\n",
      "[37] loss: 2.305\n",
      "[38] loss: 2.303\n",
      "[39] loss: 2.302\n",
      "[40] loss: 2.301\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 2.106\n",
      "[2] loss: 1.937\n",
      "[3] loss: 1.856\n",
      "[4] loss: 1.787\n",
      "[5] loss: 1.756\n",
      "[6] loss: 1.706\n",
      "[7] loss: 1.672\n",
      "[8] loss: 1.623\n",
      "[9] loss: 1.585\n",
      "[10] loss: 1.565\n",
      "[11] loss: 1.531\n",
      "[12] loss: 1.504\n",
      "[13] loss: 1.478\n",
      "[14] loss: 1.465\n",
      "[15] loss: 1.439\n",
      "[16] loss: 1.426\n",
      "[17] loss: 1.391\n",
      "[18] loss: 1.379\n",
      "[19] loss: 1.362\n",
      "[20] loss: 1.341\n",
      "[21] loss: 1.319\n",
      "[22] loss: 1.302\n",
      "[23] loss: 1.279\n",
      "[24] loss: 1.266\n",
      "[25] loss: 1.242\n",
      "[26] loss: 1.226\n",
      "[27] loss: 1.215\n",
      "[28] loss: 1.205\n",
      "[29] loss: 1.187\n",
      "[30] loss: 1.183\n",
      "[31] loss: 1.180\n",
      "[32] loss: 1.171\n",
      "[33] loss: 1.176\n",
      "[34] loss: 1.183\n",
      "[35] loss: 1.167\n",
      "[36] loss: 1.166\n",
      "[37] loss: 1.160\n",
      "[38] loss: 1.168\n",
      "[39] loss: 1.176\n",
      "[40] loss: 1.172\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 2.133\n",
      "[2] loss: 1.994\n",
      "[3] loss: 1.895\n",
      "[4] loss: 1.828\n",
      "[5] loss: 1.794\n",
      "[6] loss: 1.763\n",
      "[7] loss: 1.726\n",
      "[8] loss: 1.706\n",
      "[9] loss: 1.692\n",
      "[10] loss: 1.677\n",
      "[11] loss: 1.668\n",
      "[12] loss: 1.633\n",
      "[13] loss: 1.628\n",
      "[14] loss: 1.609\n",
      "[15] loss: 1.590\n",
      "[16] loss: 1.582\n",
      "[17] loss: 1.569\n",
      "[18] loss: 1.564\n",
      "[19] loss: 1.550\n",
      "[20] loss: 1.547\n",
      "[21] loss: 1.531\n",
      "[22] loss: 1.525\n",
      "[23] loss: 1.521\n",
      "[24] loss: 1.512\n",
      "[25] loss: 1.503\n",
      "[26] loss: 1.499\n",
      "[27] loss: 1.494\n",
      "[28] loss: 1.477\n",
      "[29] loss: 1.474\n",
      "[30] loss: 1.467\n",
      "[31] loss: 1.463\n",
      "[32] loss: 1.452\n",
      "[33] loss: 1.451\n",
      "[34] loss: 1.442\n",
      "[35] loss: 1.431\n",
      "[36] loss: 1.430\n",
      "[37] loss: 1.414\n",
      "[38] loss: 1.421\n",
      "[39] loss: 1.408\n",
      "[40] loss: 1.412\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 2.072\n",
      "[2] loss: 1.915\n",
      "[3] loss: 1.839\n",
      "[4] loss: 1.771\n",
      "[5] loss: 1.717\n",
      "[6] loss: 1.672\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.590\n",
      "[9] loss: 1.557\n",
      "[10] loss: 1.529\n",
      "[11] loss: 1.503\n",
      "[12] loss: 1.483\n",
      "[13] loss: 1.459\n",
      "[14] loss: 1.440\n",
      "[15] loss: 1.418\n",
      "[16] loss: 1.398\n",
      "[17] loss: 1.383\n",
      "[18] loss: 1.365\n",
      "[19] loss: 1.360\n",
      "[20] loss: 1.325\n",
      "[21] loss: 1.327\n",
      "[22] loss: 1.310\n",
      "[23] loss: 1.300\n",
      "[24] loss: 1.269\n",
      "[25] loss: 1.267\n",
      "[26] loss: 1.236\n",
      "[27] loss: 1.232\n",
      "[28] loss: 1.215\n",
      "[29] loss: 1.215\n",
      "[30] loss: 1.187\n",
      "[31] loss: 1.185\n",
      "[32] loss: 1.163\n",
      "[33] loss: 1.151\n",
      "[34] loss: 1.147\n",
      "[35] loss: 1.133\n",
      "[36] loss: 1.127\n",
      "[37] loss: 1.120\n",
      "[38] loss: 1.111\n",
      "[39] loss: 1.094\n",
      "[40] loss: 1.093\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 2.030\n",
      "[2] loss: 1.881\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.740\n",
      "[5] loss: 1.708\n",
      "[6] loss: 1.666\n",
      "[7] loss: 1.621\n",
      "[8] loss: 1.586\n",
      "[9] loss: 1.562\n",
      "[10] loss: 1.531\n",
      "[11] loss: 1.510\n",
      "[12] loss: 1.491\n",
      "[13] loss: 1.486\n",
      "[14] loss: 1.465\n",
      "[15] loss: 1.443\n",
      "[16] loss: 1.430\n",
      "[17] loss: 1.423\n",
      "[18] loss: 1.393\n",
      "[19] loss: 1.386\n",
      "[20] loss: 1.379\n",
      "[21] loss: 1.369\n",
      "[22] loss: 1.347\n",
      "[23] loss: 1.338\n",
      "[24] loss: 1.327\n",
      "[25] loss: 1.338\n",
      "[26] loss: 1.306\n",
      "[27] loss: 1.304\n",
      "[28] loss: 1.291\n",
      "[29] loss: 1.288\n",
      "[30] loss: 1.276\n",
      "[31] loss: 1.270\n",
      "[32] loss: 1.245\n",
      "[33] loss: 1.247\n",
      "[34] loss: 1.231\n",
      "[35] loss: 1.232\n",
      "[36] loss: 1.219\n",
      "[37] loss: 1.205\n",
      "[38] loss: 1.203\n",
      "[39] loss: 1.196\n",
      "[40] loss: 1.180\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 2.058\n",
      "[2] loss: 1.895\n",
      "[3] loss: 1.813\n",
      "[4] loss: 1.740\n",
      "[5] loss: 1.705\n",
      "[6] loss: 1.658\n",
      "[7] loss: 1.625\n",
      "[8] loss: 1.586\n",
      "[9] loss: 1.561\n",
      "[10] loss: 1.543\n",
      "[11] loss: 1.519\n",
      "[12] loss: 1.499\n",
      "[13] loss: 1.474\n",
      "[14] loss: 1.453\n",
      "[15] loss: 1.430\n",
      "[16] loss: 1.408\n",
      "[17] loss: 1.391\n",
      "[18] loss: 1.373\n",
      "[19] loss: 1.358\n",
      "[20] loss: 1.330\n",
      "[21] loss: 1.319\n",
      "[22] loss: 1.298\n",
      "[23] loss: 1.286\n",
      "[24] loss: 1.267\n",
      "[25] loss: 1.264\n",
      "[26] loss: 1.256\n",
      "[27] loss: 1.247\n",
      "[28] loss: 1.230\n",
      "[29] loss: 1.235\n",
      "[30] loss: 1.233\n",
      "[31] loss: 1.221\n",
      "[32] loss: 1.231\n",
      "[33] loss: 1.244\n",
      "[34] loss: 1.247\n",
      "[35] loss: 1.234\n",
      "[36] loss: 1.243\n",
      "[37] loss: 1.265\n",
      "[38] loss: 1.293\n",
      "[39] loss: 1.300\n",
      "[40] loss: 1.283\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 2.046\n",
      "[2] loss: 1.899\n",
      "[3] loss: 1.829\n",
      "[4] loss: 1.783\n",
      "[5] loss: 1.746\n",
      "[6] loss: 1.721\n",
      "[7] loss: 1.686\n",
      "[8] loss: 1.660\n",
      "[9] loss: 1.646\n",
      "[10] loss: 1.622\n",
      "[11] loss: 1.609\n",
      "[12] loss: 1.595\n",
      "[13] loss: 1.586\n",
      "[14] loss: 1.576\n",
      "[15] loss: 1.555\n",
      "[16] loss: 1.545\n",
      "[17] loss: 1.533\n",
      "[18] loss: 1.511\n",
      "[19] loss: 1.520\n",
      "[20] loss: 1.515\n",
      "[21] loss: 1.504\n",
      "[22] loss: 1.485\n",
      "[23] loss: 1.481\n",
      "[24] loss: 1.469\n",
      "[25] loss: 1.463\n",
      "[26] loss: 1.471\n",
      "[27] loss: 1.452\n",
      "[28] loss: 1.440\n",
      "[29] loss: 1.448\n",
      "[30] loss: 1.430\n",
      "[31] loss: 1.428\n",
      "[32] loss: 1.417\n",
      "[33] loss: 1.414\n",
      "[34] loss: 1.399\n",
      "[35] loss: 1.406\n",
      "[36] loss: 1.386\n",
      "[37] loss: 1.386\n",
      "[38] loss: 1.388\n",
      "[39] loss: 1.377\n",
      "[40] loss: 1.353\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 2.066\n",
      "[2] loss: 1.890\n",
      "[3] loss: 1.818\n",
      "[4] loss: 1.754\n",
      "[5] loss: 1.700\n",
      "[6] loss: 1.656\n",
      "[7] loss: 1.613\n",
      "[8] loss: 1.573\n",
      "[9] loss: 1.536\n",
      "[10] loss: 1.505\n",
      "[11] loss: 1.485\n",
      "[12] loss: 1.462\n",
      "[13] loss: 1.436\n",
      "[14] loss: 1.420\n",
      "[15] loss: 1.403\n",
      "[16] loss: 1.388\n",
      "[17] loss: 1.369\n",
      "[18] loss: 1.350\n",
      "[19] loss: 1.342\n",
      "[20] loss: 1.318\n",
      "[21] loss: 1.308\n",
      "[22] loss: 1.288\n",
      "[23] loss: 1.275\n",
      "[24] loss: 1.262\n",
      "[25] loss: 1.253\n",
      "[26] loss: 1.230\n",
      "[27] loss: 1.214\n",
      "[28] loss: 1.219\n",
      "[29] loss: 1.191\n",
      "[30] loss: 1.174\n",
      "[31] loss: 1.164\n",
      "[32] loss: 1.151\n",
      "[33] loss: 1.140\n",
      "[34] loss: 1.132\n",
      "[35] loss: 1.123\n",
      "[36] loss: 1.107\n",
      "[37] loss: 1.094\n",
      "[38] loss: 1.090\n",
      "[39] loss: 1.080\n",
      "[40] loss: 1.073\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 8\n",
      "[1] loss: 2.071\n",
      "[2] loss: 1.898\n",
      "[3] loss: 1.830\n",
      "[4] loss: 1.771\n",
      "[5] loss: 1.731\n",
      "[6] loss: 1.683\n",
      "[7] loss: 1.652\n",
      "[8] loss: 1.624\n",
      "[9] loss: 1.597\n",
      "[10] loss: 1.568\n",
      "[11] loss: 1.546\n",
      "[12] loss: 1.526\n",
      "[13] loss: 1.511\n",
      "[14] loss: 1.488\n",
      "[15] loss: 1.474\n",
      "[16] loss: 1.447\n",
      "[17] loss: 1.432\n",
      "[18] loss: 1.422\n",
      "[19] loss: 1.401\n",
      "[20] loss: 1.396\n",
      "[21] loss: 1.382\n",
      "[22] loss: 1.358\n",
      "[23] loss: 1.355\n",
      "[24] loss: 1.338\n",
      "[25] loss: 1.329\n",
      "[26] loss: 1.308\n",
      "[27] loss: 1.312\n",
      "[28] loss: 1.321\n",
      "[29] loss: 1.291\n",
      "[30] loss: 1.274\n",
      "[31] loss: 1.278\n",
      "[32] loss: 1.265\n",
      "[33] loss: 1.250\n",
      "[34] loss: 1.248\n",
      "[35] loss: 1.236\n",
      "[36] loss: 1.226\n",
      "[37] loss: 1.212\n",
      "[38] loss: 1.223\n",
      "[39] loss: 1.208\n",
      "[40] loss: 1.217\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 2.061\n",
      "[2] loss: 1.898\n",
      "[3] loss: 1.817\n",
      "[4] loss: 1.751\n",
      "[5] loss: 1.705\n",
      "[6] loss: 1.662\n",
      "[7] loss: 1.629\n",
      "[8] loss: 1.605\n",
      "[9] loss: 1.581\n",
      "[10] loss: 1.553\n",
      "[11] loss: 1.536\n",
      "[12] loss: 1.502\n",
      "[13] loss: 1.482\n",
      "[14] loss: 1.460\n",
      "[15] loss: 1.438\n",
      "[16] loss: 1.414\n",
      "[17] loss: 1.396\n",
      "[18] loss: 1.382\n",
      "[19] loss: 1.358\n",
      "[20] loss: 1.328\n",
      "[21] loss: 1.316\n",
      "[22] loss: 1.302\n",
      "[23] loss: 1.276\n",
      "[24] loss: 1.263\n",
      "[25] loss: 1.248\n",
      "[26] loss: 1.236\n",
      "[27] loss: 1.217\n",
      "[28] loss: 1.218\n",
      "[29] loss: 1.206\n",
      "[30] loss: 1.190\n",
      "[31] loss: 1.192\n",
      "[32] loss: 1.200\n",
      "[33] loss: 1.195\n",
      "[34] loss: 1.197\n",
      "[35] loss: 1.196\n",
      "[36] loss: 1.186\n",
      "[37] loss: 1.195\n",
      "[38] loss: 1.221\n",
      "[39] loss: 1.213\n",
      "[40] loss: 1.216\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 2.058\n",
      "[2] loss: 1.897\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.792\n",
      "[5] loss: 1.756\n",
      "[6] loss: 1.725\n",
      "[7] loss: 1.699\n",
      "[8] loss: 1.677\n",
      "[9] loss: 1.657\n",
      "[10] loss: 1.641\n",
      "[11] loss: 1.619\n",
      "[12] loss: 1.613\n",
      "[13] loss: 1.607\n",
      "[14] loss: 1.572\n",
      "[15] loss: 1.572\n",
      "[16] loss: 1.541\n",
      "[17] loss: 1.546\n",
      "[18] loss: 1.535\n",
      "[19] loss: 1.520\n",
      "[20] loss: 1.513\n",
      "[21] loss: 1.496\n",
      "[22] loss: 1.493\n",
      "[23] loss: 1.482\n",
      "[24] loss: 1.480\n",
      "[25] loss: 1.491\n",
      "[26] loss: 1.469\n",
      "[27] loss: 1.454\n",
      "[28] loss: 1.456\n",
      "[29] loss: 1.452\n",
      "[30] loss: 1.444\n",
      "[31] loss: 1.427\n",
      "[32] loss: 1.420\n",
      "[33] loss: 1.417\n",
      "[34] loss: 1.407\n",
      "[35] loss: 1.394\n",
      "[36] loss: 1.398\n",
      "[37] loss: 1.381\n",
      "[38] loss: 1.391\n",
      "[39] loss: 1.370\n",
      "[40] loss: 1.377\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 2.069\n",
      "[2] loss: 1.917\n",
      "[3] loss: 1.843\n",
      "[4] loss: 1.787\n",
      "[5] loss: 1.738\n",
      "[6] loss: 1.697\n",
      "[7] loss: 1.651\n",
      "[8] loss: 1.611\n",
      "[9] loss: 1.567\n",
      "[10] loss: 1.543\n",
      "[11] loss: 1.507\n",
      "[12] loss: 1.497\n",
      "[13] loss: 1.464\n",
      "[14] loss: 1.437\n",
      "[15] loss: 1.418\n",
      "[16] loss: 1.405\n",
      "[17] loss: 1.375\n",
      "[18] loss: 1.363\n",
      "[19] loss: 1.342\n",
      "[20] loss: 1.328\n",
      "[21] loss: 1.306\n",
      "[22] loss: 1.289\n",
      "[23] loss: 1.280\n",
      "[24] loss: 1.263\n",
      "[25] loss: 1.245\n",
      "[26] loss: 1.233\n",
      "[27] loss: 1.223\n",
      "[28] loss: 1.192\n",
      "[29] loss: 1.195\n",
      "[30] loss: 1.175\n",
      "[31] loss: 1.155\n",
      "[32] loss: 1.142\n",
      "[33] loss: 1.131\n",
      "[34] loss: 1.121\n",
      "[35] loss: 1.105\n",
      "[36] loss: 1.097\n",
      "[37] loss: 1.094\n",
      "[38] loss: 1.075\n",
      "[39] loss: 1.056\n",
      "[40] loss: 1.053\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 2.059\n",
      "[2] loss: 1.893\n",
      "[3] loss: 1.820\n",
      "[4] loss: 1.760\n",
      "[5] loss: 1.716\n",
      "[6] loss: 1.681\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.614\n",
      "[9] loss: 1.563\n",
      "[10] loss: 1.559\n",
      "[11] loss: 1.520\n",
      "[12] loss: 1.500\n",
      "[13] loss: 1.484\n",
      "[14] loss: 1.466\n",
      "[15] loss: 1.447\n",
      "[16] loss: 1.435\n",
      "[17] loss: 1.408\n",
      "[18] loss: 1.411\n",
      "[19] loss: 1.392\n",
      "[20] loss: 1.387\n",
      "[21] loss: 1.366\n",
      "[22] loss: 1.358\n",
      "[23] loss: 1.344\n",
      "[24] loss: 1.334\n",
      "[25] loss: 1.319\n",
      "[26] loss: 1.305\n",
      "[27] loss: 1.317\n",
      "[28] loss: 1.288\n",
      "[29] loss: 1.301\n",
      "[30] loss: 1.272\n",
      "[31] loss: 1.269\n",
      "[32] loss: 1.248\n",
      "[33] loss: 1.244\n",
      "[34] loss: 1.235\n",
      "[35] loss: 1.224\n",
      "[36] loss: 1.226\n",
      "[37] loss: 1.211\n",
      "[38] loss: 1.201\n",
      "[39] loss: 1.209\n",
      "[40] loss: 1.187\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 2.068\n",
      "[2] loss: 1.921\n",
      "[3] loss: 1.828\n",
      "[4] loss: 1.762\n",
      "[5] loss: 1.712\n",
      "[6] loss: 1.668\n",
      "[7] loss: 1.637\n",
      "[8] loss: 1.610\n",
      "[9] loss: 1.578\n",
      "[10] loss: 1.562\n",
      "[11] loss: 1.528\n",
      "[12] loss: 1.512\n",
      "[13] loss: 1.485\n",
      "[14] loss: 1.463\n",
      "[15] loss: 1.430\n",
      "[16] loss: 1.425\n",
      "[17] loss: 1.387\n",
      "[18] loss: 1.371\n",
      "[19] loss: 1.347\n",
      "[20] loss: 1.324\n",
      "[21] loss: 1.310\n",
      "[22] loss: 1.296\n",
      "[23] loss: 1.286\n",
      "[24] loss: 1.270\n",
      "[25] loss: 1.254\n",
      "[26] loss: 1.238\n",
      "[27] loss: 1.234\n",
      "[28] loss: 1.217\n",
      "[29] loss: 1.220\n",
      "[30] loss: 1.207\n",
      "[31] loss: 1.204\n",
      "[32] loss: 1.225\n",
      "[33] loss: 1.210\n",
      "[34] loss: 1.215\n",
      "[35] loss: 1.246\n",
      "[36] loss: 1.248\n",
      "[37] loss: 1.243\n",
      "[38] loss: 1.275\n",
      "[39] loss: 1.254\n",
      "[40] loss: 1.246\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 2.080\n",
      "[2] loss: 1.924\n",
      "[3] loss: 1.856\n",
      "[4] loss: 1.800\n",
      "[5] loss: 1.766\n",
      "[6] loss: 1.740\n",
      "[7] loss: 1.712\n",
      "[8] loss: 1.700\n",
      "[9] loss: 1.679\n",
      "[10] loss: 1.668\n",
      "[11] loss: 1.644\n",
      "[12] loss: 1.633\n",
      "[13] loss: 1.623\n",
      "[14] loss: 1.608\n",
      "[15] loss: 1.593\n",
      "[16] loss: 1.578\n",
      "[17] loss: 1.565\n",
      "[18] loss: 1.554\n",
      "[19] loss: 1.543\n",
      "[20] loss: 1.538\n",
      "[21] loss: 1.516\n",
      "[22] loss: 1.507\n",
      "[23] loss: 1.511\n",
      "[24] loss: 1.495\n",
      "[25] loss: 1.495\n",
      "[26] loss: 1.473\n",
      "[27] loss: 1.470\n",
      "[28] loss: 1.457\n",
      "[29] loss: 1.469\n",
      "[30] loss: 1.452\n",
      "[31] loss: 1.442\n",
      "[32] loss: 1.427\n",
      "[33] loss: 1.428\n",
      "[34] loss: 1.422\n",
      "[35] loss: 1.415\n",
      "[36] loss: 1.423\n",
      "[37] loss: 1.402\n",
      "[38] loss: 1.400\n",
      "[39] loss: 1.393\n",
      "[40] loss: 1.381\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 2.046\n",
      "[2] loss: 1.909\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.770\n",
      "[5] loss: 1.717\n",
      "[6] loss: 1.670\n",
      "[7] loss: 1.625\n",
      "[8] loss: 1.592\n",
      "[9] loss: 1.563\n",
      "[10] loss: 1.523\n",
      "[11] loss: 1.503\n",
      "[12] loss: 1.477\n",
      "[13] loss: 1.450\n",
      "[14] loss: 1.418\n",
      "[15] loss: 1.398\n",
      "[16] loss: 1.393\n",
      "[17] loss: 1.376\n",
      "[18] loss: 1.355\n",
      "[19] loss: 1.338\n",
      "[20] loss: 1.313\n",
      "[21] loss: 1.301\n",
      "[22] loss: 1.286\n",
      "[23] loss: 1.268\n",
      "[24] loss: 1.259\n",
      "[25] loss: 1.234\n",
      "[26] loss: 1.224\n",
      "[27] loss: 1.217\n",
      "[28] loss: 1.188\n",
      "[29] loss: 1.178\n",
      "[30] loss: 1.155\n",
      "[31] loss: 1.146\n",
      "[32] loss: 1.140\n",
      "[33] loss: 1.130\n",
      "[34] loss: 1.126\n",
      "[35] loss: 1.104\n",
      "[36] loss: 1.093\n",
      "[37] loss: 1.073\n",
      "[38] loss: 1.077\n",
      "[39] loss: 1.060\n",
      "[40] loss: 1.058\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 2.048\n",
      "[2] loss: 1.901\n",
      "[3] loss: 1.815\n",
      "[4] loss: 1.755\n",
      "[5] loss: 1.710\n",
      "[6] loss: 1.667\n",
      "[7] loss: 1.626\n",
      "[8] loss: 1.603\n",
      "[9] loss: 1.588\n",
      "[10] loss: 1.555\n",
      "[11] loss: 1.527\n",
      "[12] loss: 1.512\n",
      "[13] loss: 1.496\n",
      "[14] loss: 1.457\n",
      "[15] loss: 1.434\n",
      "[16] loss: 1.432\n",
      "[17] loss: 1.417\n",
      "[18] loss: 1.391\n",
      "[19] loss: 1.380\n",
      "[20] loss: 1.361\n",
      "[21] loss: 1.346\n",
      "[22] loss: 1.333\n",
      "[23] loss: 1.324\n",
      "[24] loss: 1.308\n",
      "[25] loss: 1.303\n",
      "[26] loss: 1.298\n",
      "[27] loss: 1.272\n",
      "[28] loss: 1.261\n",
      "[29] loss: 1.254\n",
      "[30] loss: 1.244\n",
      "[31] loss: 1.236\n",
      "[32] loss: 1.219\n",
      "[33] loss: 1.227\n",
      "[34] loss: 1.206\n",
      "[35] loss: 1.203\n",
      "[36] loss: 1.200\n",
      "[37] loss: 1.181\n",
      "[38] loss: 1.169\n",
      "[39] loss: 1.172\n",
      "[40] loss: 1.152\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 2.071\n",
      "[2] loss: 1.910\n",
      "[3] loss: 1.829\n",
      "[4] loss: 1.776\n",
      "[5] loss: 1.723\n",
      "[6] loss: 1.673\n",
      "[7] loss: 1.640\n",
      "[8] loss: 1.604\n",
      "[9] loss: 1.589\n",
      "[10] loss: 1.554\n",
      "[11] loss: 1.515\n",
      "[12] loss: 1.504\n",
      "[13] loss: 1.478\n",
      "[14] loss: 1.444\n",
      "[15] loss: 1.425\n",
      "[16] loss: 1.413\n",
      "[17] loss: 1.395\n",
      "[18] loss: 1.367\n",
      "[19] loss: 1.353\n",
      "[20] loss: 1.330\n",
      "[21] loss: 1.309\n",
      "[22] loss: 1.298\n",
      "[23] loss: 1.283\n",
      "[24] loss: 1.263\n",
      "[25] loss: 1.256\n",
      "[26] loss: 1.249\n",
      "[27] loss: 1.235\n",
      "[28] loss: 1.223\n",
      "[29] loss: 1.218\n",
      "[30] loss: 1.207\n",
      "[31] loss: 1.208\n",
      "[32] loss: 1.206\n",
      "[33] loss: 1.206\n",
      "[34] loss: 1.224\n",
      "[35] loss: 1.216\n",
      "[36] loss: 1.222\n",
      "[37] loss: 1.242\n",
      "[38] loss: 1.235\n",
      "[39] loss: 1.254\n",
      "[40] loss: 1.242\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 2.307\n",
      "[2] loss: 2.229\n",
      "[3] loss: 2.070\n",
      "[4] loss: 1.997\n",
      "[5] loss: 1.932\n",
      "[6] loss: 1.886\n",
      "[7] loss: 1.847\n",
      "[8] loss: 1.823\n",
      "[9] loss: 1.812\n",
      "[10] loss: 1.783\n",
      "[11] loss: 1.773\n",
      "[12] loss: 1.750\n",
      "[13] loss: 1.736\n",
      "[14] loss: 1.725\n",
      "[15] loss: 1.705\n",
      "[16] loss: 1.703\n",
      "[17] loss: 1.682\n",
      "[18] loss: 1.665\n",
      "[19] loss: 1.654\n",
      "[20] loss: 1.647\n",
      "[21] loss: 1.621\n",
      "[22] loss: 1.623\n",
      "[23] loss: 1.619\n",
      "[24] loss: 1.594\n",
      "[25] loss: 1.592\n",
      "[26] loss: 1.579\n",
      "[27] loss: 1.582\n",
      "[28] loss: 1.570\n",
      "[29] loss: 1.557\n",
      "[30] loss: 1.543\n",
      "[31] loss: 1.532\n",
      "[32] loss: 1.539\n",
      "[33] loss: 1.531\n",
      "[34] loss: 1.520\n",
      "[35] loss: 1.507\n",
      "[36] loss: 1.498\n",
      "[37] loss: 1.498\n",
      "[38] loss: 1.491\n",
      "[39] loss: 1.479\n",
      "[40] loss: 1.479\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 150\n",
    "epochs = 40\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.001\n",
    "\n",
    "run_cifar_experiment(int_lr, syn_lr, epochs, test_runs, '../images/compare2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
