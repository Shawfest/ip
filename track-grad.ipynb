{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class Adam(nn.Module):\n",
    "    def __init__(self, param, betas=(0.9, 0.999), eps=1e-8):\n",
    "        super(Adam, self).__init__()\n",
    "\n",
    "        self.register_buffer('beta1', torch.tensor(betas[0]))\n",
    "        self.register_buffer('beta2', torch.tensor(betas[1]))\n",
    "        self.register_buffer('eps', torch.tensor(eps))\n",
    "\n",
    "        self.register_buffer('m', torch.zeros_like(param))\n",
    "        self.register_buffer('v', torch.zeros_like(param))\n",
    "        self.register_buffer('t', torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, g):\n",
    "        self.m = self.beta1 * self.m + (1-self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1-self.beta2) * g**2\n",
    "        self.t += 1\n",
    "\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "\n",
    "        return m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta*self.adjust_a(2*Euv)\n",
    "        self.beta = self.beta + eta*self.adjust_b(Ev)\n",
    "#         for n in self.alpha:\n",
    "#             for gain in n:\n",
    "#                 if(gain != abs(gain)):\n",
    "#                     print(\"FLAG!\")\n",
    "        \n",
    "#         self.eta = eta * 0.999\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        grad4 = torch.ones(v4.shape, requires_grad=False).to(device) - v4*v4\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o, grad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    avg_grad = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y, grads = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            avg_grad.append([episode, grads.mean()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker), np.transpose(avg_grad))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses, ip_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses, standard_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "        ip_losses += temp_losses\n",
    "        ip_grads += temp_grads\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "        standard_losses += temp_losses\n",
    "        standard_grads += temp_grads\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "    ip_grads = ip_grads/test_runs\n",
    "    standard_grads = standard_grads/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([0, 1])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_grads[0], ip_grads[1], label=\"IP\")\n",
    "    plt.plot(standard_grads[0], standard_grads[1], label=\"Standard\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Slope of activation function\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.944\n",
      "[2] loss: 0.456\n",
      "[3] loss: 0.303\n",
      "[4] loss: 0.230\n",
      "[5] loss: 0.194\n",
      "[6] loss: 0.175\n",
      "[7] loss: 0.148\n",
      "[8] loss: 0.134\n",
      "[9] loss: 0.120\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.105\n",
      "[12] loss: 0.098\n",
      "[13] loss: 0.093\n",
      "[14] loss: 0.088\n",
      "[15] loss: 0.084\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.074\n",
      "[18] loss: 0.067\n",
      "[19] loss: 0.065\n",
      "[20] loss: 0.064\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 1.281\n",
      "[2] loss: 0.779\n",
      "[3] loss: 0.361\n",
      "[4] loss: 0.279\n",
      "[5] loss: 0.238\n",
      "[6] loss: 0.207\n",
      "[7] loss: 0.192\n",
      "[8] loss: 0.179\n",
      "[9] loss: 0.169\n",
      "[10] loss: 0.170\n",
      "[11] loss: 0.171\n",
      "[12] loss: 0.152\n",
      "[13] loss: 0.142\n",
      "[14] loss: 0.134\n",
      "[15] loss: 0.141\n",
      "[16] loss: 0.145\n",
      "[17] loss: 0.142\n",
      "[18] loss: 0.154\n",
      "[19] loss: 0.141\n",
      "[20] loss: 0.133\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.742\n",
      "[2] loss: 0.312\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.193\n",
      "[5] loss: 0.154\n",
      "[6] loss: 0.136\n",
      "[7] loss: 0.112\n",
      "[8] loss: 0.112\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.093\n",
      "[11] loss: 0.094\n",
      "[12] loss: 0.078\n",
      "[13] loss: 0.082\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.069\n",
      "[16] loss: 0.068\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.060\n",
      "[19] loss: 0.057\n",
      "[20] loss: 0.058\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 0.662\n",
      "[2] loss: 0.327\n",
      "[3] loss: 0.274\n",
      "[4] loss: 0.216\n",
      "[5] loss: 0.174\n",
      "[6] loss: 0.162\n",
      "[7] loss: 0.150\n",
      "[8] loss: 0.141\n",
      "[9] loss: 0.138\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.123\n",
      "[12] loss: 0.113\n",
      "[13] loss: 0.116\n",
      "[14] loss: 0.125\n",
      "[15] loss: 0.121\n",
      "[16] loss: 0.116\n",
      "[17] loss: 0.108\n",
      "[18] loss: 0.116\n",
      "[19] loss: 0.107\n",
      "[20] loss: 0.112\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.804\n",
      "[2] loss: 0.324\n",
      "[3] loss: 0.252\n",
      "[4] loss: 0.191\n",
      "[5] loss: 0.167\n",
      "[6] loss: 0.150\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.119\n",
      "[9] loss: 0.112\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.084\n",
      "[13] loss: 0.085\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.072\n",
      "[16] loss: 0.069\n",
      "[17] loss: 0.066\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.066\n",
      "[20] loss: 0.058\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.670\n",
      "[2] loss: 0.314\n",
      "[3] loss: 0.277\n",
      "[4] loss: 0.214\n",
      "[5] loss: 0.187\n",
      "[6] loss: 0.171\n",
      "[7] loss: 0.156\n",
      "[8] loss: 0.148\n",
      "[9] loss: 0.145\n",
      "[10] loss: 0.130\n",
      "[11] loss: 0.129\n",
      "[12] loss: 0.138\n",
      "[13] loss: 0.130\n",
      "[14] loss: 0.133\n",
      "[15] loss: 0.124\n",
      "[16] loss: 0.120\n",
      "[17] loss: 0.126\n",
      "[18] loss: 0.124\n",
      "[19] loss: 0.141\n",
      "[20] loss: 0.125\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.927\n",
      "[2] loss: 0.377\n",
      "[3] loss: 0.251\n",
      "[4] loss: 0.199\n",
      "[5] loss: 0.170\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.138\n",
      "[8] loss: 0.112\n",
      "[9] loss: 0.109\n",
      "[10] loss: 0.101\n",
      "[11] loss: 0.091\n",
      "[12] loss: 0.091\n",
      "[13] loss: 0.082\n",
      "[14] loss: 0.080\n",
      "[15] loss: 0.077\n",
      "[16] loss: 0.074\n",
      "[17] loss: 0.069\n",
      "[18] loss: 0.061\n",
      "[19] loss: 0.062\n",
      "[20] loss: 0.062\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 0.880\n",
      "[2] loss: 0.353\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.163\n",
      "[6] loss: 0.154\n",
      "[7] loss: 0.151\n",
      "[8] loss: 0.139\n",
      "[9] loss: 0.134\n",
      "[10] loss: 0.124\n",
      "[11] loss: 0.117\n",
      "[12] loss: 0.117\n",
      "[13] loss: 0.109\n",
      "[14] loss: 0.106\n",
      "[15] loss: 0.111\n",
      "[16] loss: 0.112\n",
      "[17] loss: 0.116\n",
      "[18] loss: 0.114\n",
      "[19] loss: 0.112\n",
      "[20] loss: 0.106\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 0.883\n",
      "[2] loss: 0.363\n",
      "[3] loss: 0.242\n",
      "[4] loss: 0.193\n",
      "[5] loss: 0.163\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.118\n",
      "[9] loss: 0.099\n",
      "[10] loss: 0.096\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.088\n",
      "[13] loss: 0.084\n",
      "[14] loss: 0.078\n",
      "[15] loss: 0.071\n",
      "[16] loss: 0.067\n",
      "[17] loss: 0.066\n",
      "[18] loss: 0.068\n",
      "[19] loss: 0.066\n",
      "[20] loss: 0.057\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 0.890\n",
      "[2] loss: 0.397\n",
      "[3] loss: 0.259\n",
      "[4] loss: 0.206\n",
      "[5] loss: 0.174\n",
      "[6] loss: 0.162\n",
      "[7] loss: 0.146\n",
      "[8] loss: 0.139\n",
      "[9] loss: 0.139\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.127\n",
      "[12] loss: 0.127\n",
      "[13] loss: 0.118\n",
      "[14] loss: 0.109\n",
      "[15] loss: 0.118\n",
      "[16] loss: 0.127\n",
      "[17] loss: 0.109\n",
      "[18] loss: 0.128\n",
      "[19] loss: 0.122\n",
      "[20] loss: 0.100\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.906\n",
      "[2] loss: 0.341\n",
      "[3] loss: 0.215\n",
      "[4] loss: 0.171\n",
      "[5] loss: 0.142\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.115\n",
      "[8] loss: 0.102\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.092\n",
      "[11] loss: 0.084\n",
      "[12] loss: 0.074\n",
      "[13] loss: 0.071\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.068\n",
      "[16] loss: 0.060\n",
      "[17] loss: 0.057\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.053\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.820\n",
      "[2] loss: 0.300\n",
      "[3] loss: 0.220\n",
      "[4] loss: 0.187\n",
      "[5] loss: 0.176\n",
      "[6] loss: 0.147\n",
      "[7] loss: 0.137\n",
      "[8] loss: 0.135\n",
      "[9] loss: 0.133\n",
      "[10] loss: 0.119\n",
      "[11] loss: 0.120\n",
      "[12] loss: 0.111\n",
      "[13] loss: 0.106\n",
      "[14] loss: 0.107\n",
      "[15] loss: 0.109\n",
      "[16] loss: 0.110\n",
      "[17] loss: 0.105\n",
      "[18] loss: 0.099\n",
      "[19] loss: 0.111\n",
      "[20] loss: 0.106\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 0.885\n",
      "[2] loss: 0.441\n",
      "[3] loss: 0.291\n",
      "[4] loss: 0.232\n",
      "[5] loss: 0.197\n",
      "[6] loss: 0.165\n",
      "[7] loss: 0.149\n",
      "[8] loss: 0.128\n",
      "[9] loss: 0.120\n",
      "[10] loss: 0.110\n",
      "[11] loss: 0.100\n",
      "[12] loss: 0.100\n",
      "[13] loss: 0.089\n",
      "[14] loss: 0.086\n",
      "[15] loss: 0.080\n",
      "[16] loss: 0.080\n",
      "[17] loss: 0.075\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.063\n",
      "[20] loss: 0.065\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 0.917\n",
      "[2] loss: 0.422\n",
      "[3] loss: 0.278\n",
      "[4] loss: 0.226\n",
      "[5] loss: 0.204\n",
      "[6] loss: 0.177\n",
      "[7] loss: 0.160\n",
      "[8] loss: 0.150\n",
      "[9] loss: 0.153\n",
      "[10] loss: 0.141\n",
      "[11] loss: 0.142\n",
      "[12] loss: 0.139\n",
      "[13] loss: 0.124\n",
      "[14] loss: 0.115\n",
      "[15] loss: 0.113\n",
      "[16] loss: 0.125\n",
      "[17] loss: 0.127\n",
      "[18] loss: 0.128\n",
      "[19] loss: 0.121\n",
      "[20] loss: 0.125\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.807\n",
      "[2] loss: 0.312\n",
      "[3] loss: 0.224\n",
      "[4] loss: 0.180\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.138\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.115\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.094\n",
      "[12] loss: 0.084\n",
      "[13] loss: 0.088\n",
      "[14] loss: 0.082\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.069\n",
      "[17] loss: 0.071\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.060\n",
      "[20] loss: 0.059\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.844\n",
      "[2] loss: 0.332\n",
      "[3] loss: 0.231\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.167\n",
      "[6] loss: 0.149\n",
      "[7] loss: 0.142\n",
      "[8] loss: 0.126\n",
      "[9] loss: 0.131\n",
      "[10] loss: 0.125\n",
      "[11] loss: 0.128\n",
      "[12] loss: 0.120\n",
      "[13] loss: 0.122\n",
      "[14] loss: 0.109\n",
      "[15] loss: 0.110\n",
      "[16] loss: 0.107\n",
      "[17] loss: 0.112\n",
      "[18] loss: 0.105\n",
      "[19] loss: 0.112\n",
      "[20] loss: 0.093\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.846\n",
      "[2] loss: 0.309\n",
      "[3] loss: 0.213\n",
      "[4] loss: 0.163\n",
      "[5] loss: 0.138\n",
      "[6] loss: 0.121\n",
      "[7] loss: 0.111\n",
      "[8] loss: 0.098\n",
      "[9] loss: 0.090\n",
      "[10] loss: 0.090\n",
      "[11] loss: 0.085\n",
      "[12] loss: 0.080\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.066\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.054\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.923\n",
      "[2] loss: 0.296\n",
      "[3] loss: 0.211\n",
      "[4] loss: 0.176\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.142\n",
      "[7] loss: 0.134\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.120\n",
      "[10] loss: 0.111\n",
      "[11] loss: 0.112\n",
      "[12] loss: 0.110\n",
      "[13] loss: 0.109\n",
      "[14] loss: 0.107\n",
      "[15] loss: 0.114\n",
      "[16] loss: 0.108\n",
      "[17] loss: 0.099\n",
      "[18] loss: 0.106\n",
      "[19] loss: 0.098\n",
      "[20] loss: 0.101\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 0.593\n",
      "[2] loss: 0.229\n",
      "[3] loss: 0.177\n",
      "[4] loss: 0.151\n",
      "[5] loss: 0.129\n",
      "[6] loss: 0.121\n",
      "[7] loss: 0.111\n",
      "[8] loss: 0.100\n",
      "[9] loss: 0.085\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.077\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.062\n",
      "[16] loss: 0.058\n",
      "[17] loss: 0.057\n",
      "[18] loss: 0.053\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.598\n",
      "[2] loss: 0.220\n",
      "[3] loss: 0.183\n",
      "[4] loss: 0.160\n",
      "[5] loss: 0.151\n",
      "[6] loss: 0.135\n",
      "[7] loss: 0.124\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.124\n",
      "[10] loss: 0.114\n",
      "[11] loss: 0.104\n",
      "[12] loss: 0.115\n",
      "[13] loss: 0.126\n",
      "[14] loss: 0.112\n",
      "[15] loss: 0.111\n",
      "[16] loss: 0.113\n",
      "[17] loss: 0.106\n",
      "[18] loss: 0.108\n",
      "[19] loss: 0.101\n",
      "[20] loss: 0.105\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82b82ff780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
