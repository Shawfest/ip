{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class Adam(nn.Module):\n",
    "    def __init__(self, param, betas=(0.9, 0.999), eps=1e-8):\n",
    "        super(Adam, self).__init__()\n",
    "\n",
    "        self.register_buffer('beta1', torch.tensor(betas[0]))\n",
    "        self.register_buffer('beta2', torch.tensor(betas[1]))\n",
    "        self.register_buffer('eps', torch.tensor(eps))\n",
    "\n",
    "        self.register_buffer('m', torch.zeros_like(param))\n",
    "        self.register_buffer('v', torch.zeros_like(param))\n",
    "        self.register_buffer('t', torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, g):\n",
    "        self.m = self.beta1 * self.m + (1-self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1-self.beta2) * g**2\n",
    "        self.t += 1\n",
    "\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "\n",
    "        return m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta*2*Euv\n",
    "        self.beta = self.beta + eta*2*Ev*Euv\n",
    "#         for n in self.alpha:\n",
    "#             for gain in n:\n",
    "#                 if(gain != abs(gain)):\n",
    "#                     print(\"FLAG!\")\n",
    "        \n",
    "#         self.eta = eta * 0.999\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = torch.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = torch.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = torch.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = torch.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = torch.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = torch.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = torch.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = torch.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = torch.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = torch.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        grad4 = torch.ones(v4.shape, requires_grad=False).to(device) - v4*v4\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o, grad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    avg_grad = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y, grads = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            avg_grad.append([episode, grads.mean()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker), np.transpose(avg_grad))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses, ip_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses, standard_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "        ip_losses += temp_losses\n",
    "        ip_grads += temp_grads\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        temp_losses, temp_grads = train_deep_model(net, optimizer2, seed, epochs)\n",
    "        standard_losses += temp_losses\n",
    "        standard_grads += temp_grads\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "    ip_grads = ip_grads/test_runs\n",
    "    standard_grads = standard_grads/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([0, 1])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_grads[0], ip_grads[1], label=\"IP\")\n",
    "    plt.plot(standard_grads[0], standard_grads[1], label=\"Standard\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Slope of activation function\")\n",
    "    plt.legend()\n",
    "    plt.savefig('../images/main_grads.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.841\n",
      "[2] loss: 0.317\n",
      "[3] loss: 0.221\n",
      "[4] loss: 0.179\n",
      "[5] loss: 0.153\n",
      "[6] loss: 0.139\n",
      "[7] loss: 0.118\n",
      "[8] loss: 0.103\n",
      "[9] loss: 0.098\n",
      "[10] loss: 0.092\n",
      "[11] loss: 0.088\n",
      "[12] loss: 0.078\n",
      "[13] loss: 0.074\n",
      "[14] loss: 0.073\n",
      "[15] loss: 0.061\n",
      "[16] loss: 0.060\n",
      "[17] loss: 0.060\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.050\n",
      "[21] loss: 0.048\n",
      "[22] loss: 0.045\n",
      "[23] loss: 0.044\n",
      "[24] loss: 0.045\n",
      "[25] loss: 0.044\n",
      "[26] loss: 0.040\n",
      "[27] loss: 0.039\n",
      "[28] loss: 0.038\n",
      "[29] loss: 0.040\n",
      "[30] loss: 0.038\n",
      "[31] loss: 0.034\n",
      "[32] loss: 0.033\n",
      "[33] loss: 0.032\n",
      "[34] loss: 0.035\n",
      "[35] loss: 0.032\n",
      "[36] loss: 0.035\n",
      "[37] loss: 0.032\n",
      "[38] loss: 0.027\n",
      "[39] loss: 0.029\n",
      "[40] loss: 0.031\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 0.890\n",
      "[2] loss: 0.312\n",
      "[3] loss: 0.228\n",
      "[4] loss: 0.198\n",
      "[5] loss: 0.179\n",
      "[6] loss: 0.164\n",
      "[7] loss: 0.150\n",
      "[8] loss: 0.150\n",
      "[9] loss: 0.136\n",
      "[10] loss: 0.129\n",
      "[11] loss: 0.120\n",
      "[12] loss: 0.129\n",
      "[13] loss: 0.130\n",
      "[14] loss: 0.121\n",
      "[15] loss: 0.117\n",
      "[16] loss: 0.138\n",
      "[17] loss: 0.132\n",
      "[18] loss: 0.110\n",
      "[19] loss: 0.121\n",
      "[20] loss: 0.112\n",
      "[21] loss: 0.120\n",
      "[22] loss: 0.115\n",
      "[23] loss: 0.128\n",
      "[24] loss: 0.122\n",
      "[25] loss: 0.129\n",
      "[26] loss: 0.114\n",
      "[27] loss: 0.119\n",
      "[28] loss: 0.133\n",
      "[29] loss: 0.131\n",
      "[30] loss: 0.125\n",
      "[31] loss: 0.149\n",
      "[32] loss: 0.134\n",
      "[33] loss: 0.126\n",
      "[34] loss: 0.121\n",
      "[35] loss: 0.124\n",
      "[36] loss: 0.117\n",
      "[37] loss: 0.136\n",
      "[38] loss: 0.126\n",
      "[39] loss: 0.123\n",
      "[40] loss: 0.129\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.711\n",
      "[2] loss: 0.285\n",
      "[3] loss: 0.225\n",
      "[4] loss: 0.178\n",
      "[5] loss: 0.158\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.117\n",
      "[8] loss: 0.106\n",
      "[9] loss: 0.098\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.082\n",
      "[12] loss: 0.076\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.063\n",
      "[16] loss: 0.062\n",
      "[17] loss: 0.057\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.054\n",
      "[20] loss: 0.050\n",
      "[21] loss: 0.048\n",
      "[22] loss: 0.051\n",
      "[23] loss: 0.043\n",
      "[24] loss: 0.049\n",
      "[25] loss: 0.045\n",
      "[26] loss: 0.042\n",
      "[27] loss: 0.040\n",
      "[28] loss: 0.040\n",
      "[29] loss: 0.038\n",
      "[30] loss: 0.036\n",
      "[31] loss: 0.037\n",
      "[32] loss: 0.037\n",
      "[33] loss: 0.036\n",
      "[34] loss: 0.032\n",
      "[35] loss: 0.033\n",
      "[36] loss: 0.035\n",
      "[37] loss: 0.034\n",
      "[38] loss: 0.033\n",
      "[39] loss: 0.029\n",
      "[40] loss: 0.031\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 0.697\n",
      "[2] loss: 0.316\n",
      "[3] loss: 0.257\n",
      "[4] loss: 0.222\n",
      "[5] loss: 0.185\n",
      "[6] loss: 0.177\n",
      "[7] loss: 0.156\n",
      "[8] loss: 0.149\n",
      "[9] loss: 0.148\n",
      "[10] loss: 0.148\n",
      "[11] loss: 0.132\n",
      "[12] loss: 0.134\n",
      "[13] loss: 0.127\n",
      "[14] loss: 0.125\n",
      "[15] loss: 0.118\n",
      "[16] loss: 0.114\n",
      "[17] loss: 0.118\n",
      "[18] loss: 0.112\n",
      "[19] loss: 0.113\n",
      "[20] loss: 0.119\n",
      "[21] loss: 0.125\n",
      "[22] loss: 0.115\n",
      "[23] loss: 0.111\n",
      "[24] loss: 0.111\n",
      "[25] loss: 0.115\n",
      "[26] loss: 0.113\n",
      "[27] loss: 0.113\n",
      "[28] loss: 0.105\n",
      "[29] loss: 0.105\n",
      "[30] loss: 0.110\n",
      "[31] loss: 0.104\n",
      "[32] loss: 0.110\n",
      "[33] loss: 0.115\n",
      "[34] loss: 0.115\n",
      "[35] loss: 0.112\n",
      "[36] loss: 0.127\n",
      "[37] loss: 0.126\n",
      "[38] loss: 0.122\n",
      "[39] loss: 0.112\n",
      "[40] loss: 0.128\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.830\n",
      "[2] loss: 0.372\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.181\n",
      "[5] loss: 0.151\n",
      "[6] loss: 0.129\n",
      "[7] loss: 0.116\n",
      "[8] loss: 0.102\n",
      "[9] loss: 0.096\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.076\n",
      "[12] loss: 0.072\n",
      "[13] loss: 0.071\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.061\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.060\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.048\n",
      "[21] loss: 0.053\n",
      "[22] loss: 0.048\n",
      "[23] loss: 0.044\n",
      "[24] loss: 0.043\n",
      "[25] loss: 0.041\n",
      "[26] loss: 0.043\n",
      "[27] loss: 0.041\n",
      "[28] loss: 0.039\n",
      "[29] loss: 0.037\n",
      "[30] loss: 0.039\n",
      "[31] loss: 0.035\n",
      "[32] loss: 0.035\n",
      "[33] loss: 0.034\n",
      "[34] loss: 0.036\n",
      "[35] loss: 0.034\n",
      "[36] loss: 0.035\n",
      "[37] loss: 0.033\n",
      "[38] loss: 0.032\n",
      "[39] loss: 0.031\n",
      "[40] loss: 0.032\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.844\n",
      "[2] loss: 0.415\n",
      "[3] loss: 0.258\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.170\n",
      "[6] loss: 0.152\n",
      "[7] loss: 0.141\n",
      "[8] loss: 0.135\n",
      "[9] loss: 0.124\n",
      "[10] loss: 0.115\n",
      "[11] loss: 0.117\n",
      "[12] loss: 0.115\n",
      "[13] loss: 0.111\n",
      "[14] loss: 0.117\n",
      "[15] loss: 0.106\n",
      "[16] loss: 0.102\n",
      "[17] loss: 0.099\n",
      "[18] loss: 0.106\n",
      "[19] loss: 0.100\n",
      "[20] loss: 0.101\n",
      "[21] loss: 0.120\n",
      "[22] loss: 0.103\n",
      "[23] loss: 0.108\n",
      "[24] loss: 0.105\n",
      "[25] loss: 0.100\n",
      "[26] loss: 0.099\n",
      "[27] loss: 0.112\n",
      "[28] loss: 0.096\n",
      "[29] loss: 0.106\n",
      "[30] loss: 0.110\n",
      "[31] loss: 0.113\n",
      "[32] loss: 0.109\n",
      "[33] loss: 0.102\n",
      "[34] loss: 0.109\n",
      "[35] loss: 0.104\n",
      "[36] loss: 0.106\n",
      "[37] loss: 0.106\n",
      "[38] loss: 0.113\n",
      "[39] loss: 0.111\n",
      "[40] loss: 0.103\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.830\n",
      "[2] loss: 0.283\n",
      "[3] loss: 0.207\n",
      "[4] loss: 0.170\n",
      "[5] loss: 0.140\n",
      "[6] loss: 0.116\n",
      "[7] loss: 0.105\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.086\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.077\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.070\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.061\n",
      "[16] loss: 0.056\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.050\n",
      "[21] loss: 0.048\n",
      "[22] loss: 0.048\n",
      "[23] loss: 0.046\n",
      "[24] loss: 0.042\n",
      "[25] loss: 0.042\n",
      "[26] loss: 0.043\n",
      "[27] loss: 0.040\n",
      "[28] loss: 0.038\n",
      "[29] loss: 0.038\n",
      "[30] loss: 0.036\n",
      "[31] loss: 0.039\n",
      "[32] loss: 0.033\n",
      "[33] loss: 0.033\n",
      "[34] loss: 0.035\n",
      "[35] loss: 0.034\n",
      "[36] loss: 0.033\n",
      "[37] loss: 0.032\n",
      "[38] loss: 0.032\n",
      "[39] loss: 0.032\n",
      "[40] loss: 0.030\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 0.849\n",
      "[2] loss: 0.304\n",
      "[3] loss: 0.231\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.169\n",
      "[6] loss: 0.150\n",
      "[7] loss: 0.145\n",
      "[8] loss: 0.141\n",
      "[9] loss: 0.128\n",
      "[10] loss: 0.122\n",
      "[11] loss: 0.125\n",
      "[12] loss: 0.116\n",
      "[13] loss: 0.128\n",
      "[14] loss: 0.121\n",
      "[15] loss: 0.112\n",
      "[16] loss: 0.127\n",
      "[17] loss: 0.116\n",
      "[18] loss: 0.113\n",
      "[19] loss: 0.119\n",
      "[20] loss: 0.111\n",
      "[21] loss: 0.113\n",
      "[22] loss: 0.119\n",
      "[23] loss: 0.117\n",
      "[24] loss: 0.127\n",
      "[25] loss: 0.125\n",
      "[26] loss: 0.107\n",
      "[27] loss: 0.108\n",
      "[28] loss: 0.108\n",
      "[29] loss: 0.111\n",
      "[30] loss: 0.115\n",
      "[31] loss: 0.113\n",
      "[32] loss: 0.104\n",
      "[33] loss: 0.102\n",
      "[34] loss: 0.124\n",
      "[35] loss: 0.111\n",
      "[36] loss: 0.118\n",
      "[37] loss: 0.110\n",
      "[38] loss: 0.124\n",
      "[39] loss: 0.127\n",
      "[40] loss: 0.121\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 1.131\n",
      "[2] loss: 0.558\n",
      "[3] loss: 0.475\n",
      "[4] loss: 0.430\n",
      "[5] loss: 0.408\n",
      "[6] loss: 0.388\n",
      "[7] loss: 0.372\n",
      "[8] loss: 0.366\n",
      "[9] loss: 0.355\n",
      "[10] loss: 0.343\n",
      "[11] loss: 0.342\n",
      "[12] loss: 0.335\n",
      "[13] loss: 0.329\n",
      "[14] loss: 0.326\n",
      "[15] loss: 0.320\n",
      "[16] loss: 0.317\n",
      "[17] loss: 0.316\n",
      "[18] loss: 0.315\n",
      "[19] loss: 0.308\n",
      "[20] loss: 0.309\n",
      "[21] loss: 0.313\n",
      "[22] loss: 0.308\n",
      "[23] loss: 0.304\n",
      "[24] loss: 0.302\n",
      "[25] loss: 0.303\n",
      "[26] loss: 0.304\n",
      "[27] loss: 0.300\n",
      "[28] loss: 0.296\n",
      "[29] loss: 0.297\n",
      "[30] loss: 0.295\n",
      "[31] loss: 0.297\n",
      "[32] loss: 0.294\n",
      "[33] loss: 0.294\n",
      "[34] loss: 0.293\n",
      "[35] loss: 0.292\n",
      "[36] loss: 0.294\n",
      "[37] loss: 0.293\n",
      "[38] loss: 0.293\n",
      "[39] loss: 0.290\n",
      "[40] loss: 0.292\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 1.497\n",
      "[2] loss: 0.614\n",
      "[3] loss: 0.362\n",
      "[4] loss: 0.233\n",
      "[5] loss: 0.202\n",
      "[6] loss: 0.174\n",
      "[7] loss: 0.154\n",
      "[8] loss: 0.149\n",
      "[9] loss: 0.139\n",
      "[10] loss: 0.140\n",
      "[11] loss: 0.131\n",
      "[12] loss: 0.129\n",
      "[13] loss: 0.141\n",
      "[14] loss: 0.126\n",
      "[15] loss: 0.126\n",
      "[16] loss: 0.120\n",
      "[17] loss: 0.124\n",
      "[18] loss: 0.143\n",
      "[19] loss: 0.143\n",
      "[20] loss: 0.126\n",
      "[21] loss: 0.129\n",
      "[22] loss: 0.118\n",
      "[23] loss: 0.111\n",
      "[24] loss: 0.113\n",
      "[25] loss: 0.127\n",
      "[26] loss: 0.128\n",
      "[27] loss: 0.121\n",
      "[28] loss: 0.118\n",
      "[29] loss: 0.117\n",
      "[30] loss: 0.131\n",
      "[31] loss: 0.121\n",
      "[32] loss: 0.114\n",
      "[33] loss: 0.107\n",
      "[34] loss: 0.117\n",
      "[35] loss: 0.117\n",
      "[36] loss: 0.121\n",
      "[37] loss: 0.117\n",
      "[38] loss: 0.119\n",
      "[39] loss: 0.113\n",
      "[40] loss: 0.115\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.668\n",
      "[2] loss: 0.302\n",
      "[3] loss: 0.219\n",
      "[4] loss: 0.177\n",
      "[5] loss: 0.140\n",
      "[6] loss: 0.122\n",
      "[7] loss: 0.107\n",
      "[8] loss: 0.100\n",
      "[9] loss: 0.089\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.074\n",
      "[12] loss: 0.070\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.066\n",
      "[15] loss: 0.062\n",
      "[16] loss: 0.058\n",
      "[17] loss: 0.058\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.051\n",
      "[21] loss: 0.049\n",
      "[22] loss: 0.050\n",
      "[23] loss: 0.044\n",
      "[24] loss: 0.043\n",
      "[25] loss: 0.048\n",
      "[26] loss: 0.042\n",
      "[27] loss: 0.043\n",
      "[28] loss: 0.042\n",
      "[29] loss: 0.039\n",
      "[30] loss: 0.039\n",
      "[31] loss: 0.039\n",
      "[32] loss: 0.040\n",
      "[33] loss: 0.037\n",
      "[34] loss: 0.036\n",
      "[35] loss: 0.036\n",
      "[36] loss: 0.033\n",
      "[37] loss: 0.032\n",
      "[38] loss: 0.039\n",
      "[39] loss: 0.035\n",
      "[40] loss: 0.035\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.669\n",
      "[2] loss: 0.319\n",
      "[3] loss: 0.248\n",
      "[4] loss: 0.224\n",
      "[5] loss: 0.183\n",
      "[6] loss: 0.167\n",
      "[7] loss: 0.159\n",
      "[8] loss: 0.150\n",
      "[9] loss: 0.141\n",
      "[10] loss: 0.139\n",
      "[11] loss: 0.139\n",
      "[12] loss: 0.134\n",
      "[13] loss: 0.126\n",
      "[14] loss: 0.126\n",
      "[15] loss: 0.128\n",
      "[16] loss: 0.122\n",
      "[17] loss: 0.127\n",
      "[18] loss: 0.126\n",
      "[19] loss: 0.124\n",
      "[20] loss: 0.124\n",
      "[7] loss: 0.121\n",
      "[8] loss: 0.107\n",
      "[9] loss: 0.098\n",
      "[10] loss: 0.091\n",
      "[11] loss: 0.087\n",
      "[12] loss: 0.076\n",
      "[13] loss: 0.078\n",
      "[14] loss: 0.070\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.062\n",
      "[18] loss: 0.060\n",
      "[19] loss: 0.058\n",
      "[20] loss: 0.057\n",
      "[21] loss: 0.057\n",
      "[22] loss: 0.052\n",
      "[23] loss: 0.049\n",
      "[24] loss: 0.048\n",
      "[25] loss: 0.049\n",
      "[26] loss: 0.046\n",
      "[27] loss: 0.047\n",
      "[28] loss: 0.043\n",
      "[29] loss: 0.041\n",
      "[30] loss: 0.039\n",
      "[31] loss: 0.042\n",
      "[32] loss: 0.040\n",
      "[33] loss: 0.039\n",
      "[34] loss: 0.040\n",
      "[35] loss: 0.036\n",
      "[36] loss: 0.035\n",
      "[37] loss: 0.034\n",
      "[38] loss: 0.035\n",
      "[39] loss: 0.038\n",
      "[40] loss: 0.036\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 0.691\n",
      "[2] loss: 0.303\n",
      "[3] loss: 0.245\n",
      "[4] loss: 0.209\n",
      "[5] loss: 0.192\n",
      "[6] loss: 0.181\n",
      "[7] loss: 0.164\n",
      "[8] loss: 0.156\n",
      "[9] loss: 0.160\n",
      "[10] loss: 0.145\n",
      "[11] loss: 0.141\n",
      "[12] loss: 0.150\n",
      "[13] loss: 0.142\n",
      "[14] loss: 0.134\n",
      "[15] loss: 0.133\n",
      "[16] loss: 0.126\n",
      "[17] loss: 0.137\n",
      "[18] loss: 0.136\n",
      "[19] loss: 0.138\n",
      "[20] loss: 0.126\n",
      "[21] loss: 0.118\n",
      "[22] loss: 0.123\n",
      "[23] loss: 0.139\n",
      "[24] loss: 0.120\n",
      "[25] loss: 0.123\n",
      "[26] loss: 0.123\n",
      "[27] loss: 0.128\n",
      "[28] loss: 0.135\n",
      "[29] loss: 0.130\n",
      "[30] loss: 0.135\n",
      "[31] loss: 0.117\n",
      "[32] loss: 0.135\n",
      "[33] loss: 0.136\n",
      "[34] loss: 0.127\n",
      "[35] loss: 0.128\n",
      "[36] loss: 0.125\n",
      "[37] loss: 0.124\n",
      "[38] loss: 0.130\n",
      "[39] loss: 0.118\n",
      "[40] loss: 0.118\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.811\n",
      "[2] loss: 0.294\n",
      "[3] loss: 0.201\n",
      "[4] loss: 0.160\n",
      "[5] loss: 0.139\n",
      "[6] loss: 0.123\n",
      "[7] loss: 0.115\n",
      "[8] loss: 0.098\n",
      "[9] loss: 0.099\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.079\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.063\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.057\n",
      "[20] loss: 0.052\n",
      "[21] loss: 0.051\n",
      "[22] loss: 0.051\n",
      "[23] loss: 0.045\n",
      "[24] loss: 0.043\n",
      "[25] loss: 0.042\n",
      "[26] loss: 0.041\n",
      "[27] loss: 0.044\n",
      "[28] loss: 0.042\n",
      "[29] loss: 0.039\n",
      "[30] loss: 0.037\n",
      "[31] loss: 0.041\n",
      "[32] loss: 0.034\n",
      "[33] loss: 0.038\n",
      "[34] loss: 0.033\n",
      "[35] loss: 0.036\n",
      "[36] loss: 0.033\n",
      "[37] loss: 0.037\n",
      "[38] loss: 0.030\n",
      "[39] loss: 0.031\n",
      "[40] loss: 0.032\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.854\n",
      "[2] loss: 0.310\n",
      "[3] loss: 0.228\n",
      "[4] loss: 0.193\n",
      "[5] loss: 0.170\n",
      "[6] loss: 0.163\n",
      "[7] loss: 0.144\n",
      "[8] loss: 0.136\n",
      "[9] loss: 0.131\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.134\n",
      "[12] loss: 0.123\n",
      "[13] loss: 0.131\n",
      "[14] loss: 0.116\n",
      "[15] loss: 0.121\n",
      "[16] loss: 0.116\n",
      "[17] loss: 0.138\n",
      "[18] loss: 0.125\n",
      "[19] loss: 0.121\n",
      "[20] loss: 0.111\n",
      "[21] loss: 0.116\n",
      "[22] loss: 0.123\n",
      "[23] loss: 0.115\n",
      "[24] loss: 0.111\n",
      "[25] loss: 0.114\n",
      "[26] loss: 0.108\n",
      "[27] loss: 0.115\n",
      "[28] loss: 0.121\n",
      "[29] loss: 0.109\n",
      "[30] loss: 0.110\n",
      "[31] loss: 0.120\n",
      "[32] loss: 0.114\n",
      "[33] loss: 0.130\n",
      "[34] loss: 0.113\n",
      "[35] loss: 0.129\n",
      "[36] loss: 0.124\n",
      "[37] loss: 0.108\n",
      "[38] loss: 0.118\n",
      "[39] loss: 0.124\n",
      "[40] loss: 0.120\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.807\n",
      "[2] loss: 0.315\n",
      "[3] loss: 0.220\n",
      "[4] loss: 0.177\n",
      "[5] loss: 0.141\n",
      "[6] loss: 0.123\n",
      "[7] loss: 0.112\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.092\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.079\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.067\n",
      "[14] loss: 0.068\n",
      "[15] loss: 0.064\n",
      "[16] loss: 0.062\n",
      "[17] loss: 0.060\n",
      "[18] loss: 0.058\n",
      "[19] loss: 0.057\n",
      "[20] loss: 0.050\n",
      "[21] loss: 0.054\n",
      "[22] loss: 0.051\n",
      "[23] loss: 0.047\n",
      "[24] loss: 0.049\n",
      "[25] loss: 0.046\n",
      "[26] loss: 0.047\n",
      "[27] loss: 0.044\n",
      "[28] loss: 0.045\n",
      "[29] loss: 0.040\n",
      "[30] loss: 0.044\n",
      "[31] loss: 0.038\n",
      "[32] loss: 0.036\n",
      "[33] loss: 0.040\n",
      "[34] loss: 0.042\n",
      "[35] loss: 0.035\n",
      "[36] loss: 0.036\n",
      "[37] loss: 0.035\n",
      "[38] loss: 0.036\n",
      "[39] loss: 0.035\n",
      "[40] loss: 0.035\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.820\n",
      "[2] loss: 0.337\n",
      "[3] loss: 0.265\n",
      "[4] loss: 0.214\n",
      "[5] loss: 0.204\n",
      "[6] loss: 0.162\n",
      "[7] loss: 0.154\n",
      "[8] loss: 0.148\n",
      "[9] loss: 0.150\n",
      "[10] loss: 0.139\n",
      "[11] loss: 0.125\n",
      "[12] loss: 0.121\n",
      "[13] loss: 0.122\n",
      "[14] loss: 0.125\n",
      "[15] loss: 0.125\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.107\n",
      "[18] loss: 0.118\n",
      "[19] loss: 0.120\n",
      "[20] loss: 0.128\n",
      "[21] loss: 0.129\n",
      "[22] loss: 0.113\n",
      "[23] loss: 0.107\n",
      "[24] loss: 0.113\n",
      "[25] loss: 0.106\n",
      "[26] loss: 0.117\n",
      "[27] loss: 0.115\n",
      "[28] loss: 0.118\n",
      "[29] loss: 0.113\n",
      "[30] loss: 0.105\n",
      "[31] loss: 0.114\n",
      "[32] loss: 0.111\n",
      "[33] loss: 0.113\n",
      "[34] loss: 0.121\n",
      "[35] loss: 0.119\n",
      "[36] loss: 0.111\n",
      "[37] loss: 0.112\n",
      "[38] loss: 0.127\n",
      "[39] loss: 0.119\n",
      "[40] loss: 0.112\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 0.616\n",
      "[2] loss: 0.286\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.179\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.131\n",
      "[7] loss: 0.115\n",
      "[8] loss: 0.106\n",
      "[9] loss: 0.095\n",
      "[10] loss: 0.089\n",
      "[11] loss: 0.081\n",
      "[12] loss: 0.075\n",
      "[13] loss: 0.069\n",
      "[14] loss: 0.068\n",
      "[15] loss: 0.064\n",
      "[16] loss: 0.063\n",
      "[17] loss: 0.060\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.050\n",
      "[21] loss: 0.050\n",
      "[22] loss: 0.052\n",
      "[23] loss: 0.048\n",
      "[24] loss: 0.044\n",
      "[25] loss: 0.044\n",
      "[26] loss: 0.045\n",
      "[27] loss: 0.043\n",
      "[28] loss: 0.041\n",
      "[29] loss: 0.042\n",
      "[30] loss: 0.037\n",
      "[31] loss: 0.039\n",
      "[32] loss: 0.038\n",
      "[33] loss: 0.033\n",
      "[34] loss: 0.039\n",
      "[35] loss: 0.037\n",
      "[36] loss: 0.035\n",
      "[37] loss: 0.039\n",
      "[38] loss: 0.034\n",
      "[39] loss: 0.032\n",
      "[40] loss: 0.033\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.621\n",
      "[2] loss: 0.290\n",
      "[3] loss: 0.244\n",
      "[4] loss: 0.212\n",
      "[5] loss: 0.186\n",
      "[6] loss: 0.162\n",
      "[7] loss: 0.152\n",
      "[8] loss: 0.142\n",
      "[9] loss: 0.134\n",
      "[10] loss: 0.143\n",
      "[11] loss: 0.128\n",
      "[12] loss: 0.131\n",
      "[13] loss: 0.137\n",
      "[14] loss: 0.116\n",
      "[15] loss: 0.112\n",
      "[16] loss: 0.121\n",
      "[17] loss: 0.116\n",
      "[18] loss: 0.116\n",
      "[19] loss: 0.116\n",
      "[20] loss: 0.113\n",
      "[21] loss: 0.118\n",
      "[22] loss: 0.123\n",
      "[23] loss: 0.117\n",
      "[24] loss: 0.111\n",
      "[25] loss: 0.120\n",
      "[26] loss: 0.116\n",
      "[27] loss: 0.116\n",
      "[28] loss: 0.106\n",
      "[29] loss: 0.117\n",
      "[30] loss: 0.121\n",
      "[31] loss: 0.119\n",
      "[32] loss: 0.131\n",
      "[33] loss: 0.141\n",
      "[34] loss: 0.135\n",
      "[35] loss: 0.133\n",
      "[36] loss: 0.131\n",
      "[37] loss: 0.127\n",
      "[38] loss: 0.122\n",
      "[39] loss: 0.118\n",
      "[40] loss: 0.116\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 40\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
