{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Adam(nn.Module):\n",
    "    def __init__(self, param, betas=(0.9, 0.999), eps=1e-8):\n",
    "        super(Adam, self).__init__()\n",
    "\n",
    "        self.register_buffer('beta1', torch.tensor(betas[0]))\n",
    "        self.register_buffer('beta2', torch.tensor(betas[1]))\n",
    "        self.register_buffer('eps', torch.tensor(eps))\n",
    "\n",
    "        self.register_buffer('m', torch.zeros_like(param))\n",
    "        self.register_buffer('v', torch.zeros_like(param))\n",
    "        self.register_buffer('t', torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, g):\n",
    "        self.m = self.beta1 * self.m + (1-self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1-self.beta2) * g**2\n",
    "        self.t += 1\n",
    "\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "\n",
    "        return m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta*self.adjust_a(2*Euv)\n",
    "        self.beta = self.beta + eta*self.adjust_b(Ev)\n",
    "\n",
    "\n",
    "class inc_BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(inc_BN, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "        self.adjust_a = Adam(self.alpha)\n",
    "        self.adjust_b = Adam(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta * self.adjust_a((torch.sqrt((Euu - Eu**2))))\n",
    "        self.beta = self.beta + eta * self.adjust_b((Eu))\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "        \n",
    "class og_IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(og_IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = self.alpha*x + self.beta\n",
    "        \n",
    "        return nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta * (1/self.alpha -2*Euv)\n",
    "        self.beta = self.beta + eta * (-2*Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o\n",
    "    \n",
    "class CDNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(CDNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(3*32*32, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cifar_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.938\n",
      "[2] loss: 0.332\n",
      "[3] loss: 0.226\n",
      "[4] loss: 0.177\n",
      "[5] loss: 0.141\n",
      "[6] loss: 0.125\n",
      "[7] loss: 0.108\n",
      "[8] loss: 0.103\n",
      "[9] loss: 0.089\n",
      "[10] loss: 0.084\n",
      "[11] loss: 0.077\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.060\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 2.310\n",
      "[2] loss: 2.310\n",
      "[3] loss: 2.311\n",
      "[4] loss: 2.310\n",
      "[5] loss: 2.310\n",
      "[6] loss: 2.310\n",
      "[7] loss: 2.310\n",
      "[8] loss: 2.310\n",
      "[9] loss: 2.310\n",
      "[10] loss: 2.310\n",
      "[11] loss: 2.310\n",
      "[12] loss: 2.310\n",
      "[13] loss: 2.310\n",
      "[14] loss: 2.310\n",
      "[15] loss: 2.310\n",
      "[16] loss: 2.310\n",
      "[17] loss: 2.310\n",
      "[18] loss: 2.310\n",
      "[19] loss: 2.310\n",
      "[20] loss: 2.310\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 1.146\n",
      "[2] loss: 0.486\n",
      "[3] loss: 0.262\n",
      "[4] loss: 0.209\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.158\n",
      "[7] loss: 0.147\n",
      "[8] loss: 0.139\n",
      "[9] loss: 0.132\n",
      "[10] loss: 0.118\n",
      "[11] loss: 0.112\n",
      "[12] loss: 0.103\n",
      "[13] loss: 0.099\n",
      "[14] loss: 0.096\n",
      "[15] loss: 0.092\n",
      "[16] loss: 0.088\n",
      "[17] loss: 0.085\n",
      "[18] loss: 0.081\n",
      "[19] loss: 0.078\n",
      "[20] loss: 0.074\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 0.969\n",
      "[2] loss: 0.332\n",
      "[3] loss: 0.232\n",
      "[4] loss: 0.181\n",
      "[5] loss: 0.156\n",
      "[6] loss: 0.138\n",
      "[7] loss: 0.130\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.108\n",
      "[10] loss: 0.106\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.095\n",
      "[13] loss: 0.095\n",
      "[14] loss: 0.092\n",
      "[15] loss: 0.092\n",
      "[16] loss: 0.087\n",
      "[17] loss: 0.080\n",
      "[18] loss: 0.079\n",
      "[19] loss: 0.082\n",
      "[20] loss: 0.080\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.924\n",
      "[2] loss: 0.434\n",
      "[3] loss: 0.275\n",
      "[4] loss: 0.215\n",
      "[5] loss: 0.169\n",
      "[6] loss: 0.140\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.115\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.093\n",
      "[11] loss: 0.090\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.075\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.068\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 0.958\n",
      "[2] loss: 0.303\n",
      "[3] loss: 0.212\n",
      "[4] loss: 0.178\n",
      "[5] loss: 0.147\n",
      "[6] loss: 0.128\n",
      "[7] loss: 0.115\n",
      "[8] loss: 0.103\n",
      "[9] loss: 0.092\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.077\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.063\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.064\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.049\n",
      "[18] loss: 0.050\n",
      "[19] loss: 0.052\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 0.897\n",
      "[2] loss: 0.377\n",
      "[3] loss: 0.242\n",
      "[4] loss: 0.190\n",
      "[5] loss: 0.159\n",
      "[6] loss: 0.144\n",
      "[7] loss: 0.138\n",
      "[8] loss: 0.131\n",
      "[9] loss: 0.128\n",
      "[10] loss: 0.119\n",
      "[11] loss: 0.116\n",
      "[12] loss: 0.109\n",
      "[13] loss: 0.104\n",
      "[14] loss: 0.098\n",
      "[15] loss: 0.092\n",
      "[16] loss: 0.093\n",
      "[17] loss: 0.088\n",
      "[18] loss: 0.086\n",
      "[19] loss: 0.081\n",
      "[20] loss: 0.079\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 0.903\n",
      "[2] loss: 0.382\n",
      "[3] loss: 0.259\n",
      "[4] loss: 0.212\n",
      "[5] loss: 0.166\n",
      "[6] loss: 0.148\n",
      "[7] loss: 0.137\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.120\n",
      "[10] loss: 0.107\n",
      "[11] loss: 0.103\n",
      "[12] loss: 0.093\n",
      "[13] loss: 0.093\n",
      "[14] loss: 0.093\n",
      "[15] loss: 0.084\n",
      "[16] loss: 0.086\n",
      "[17] loss: 0.092\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.082\n",
      "[20] loss: 0.080\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.677\n",
      "[2] loss: 0.273\n",
      "[3] loss: 0.202\n",
      "[4] loss: 0.158\n",
      "[5] loss: 0.139\n",
      "[6] loss: 0.118\n",
      "[7] loss: 0.103\n",
      "[8] loss: 0.092\n",
      "[9] loss: 0.087\n",
      "[10] loss: 0.080\n",
      "[11] loss: 0.071\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.064\n",
      "[14] loss: 0.061\n",
      "[15] loss: 0.054\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.051\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 3\n",
      "[1] loss: 0.735\n",
      "[2] loss: 0.284\n",
      "[3] loss: 0.220\n",
      "[4] loss: 0.172\n",
      "[5] loss: 0.147\n",
      "[6] loss: 0.123\n",
      "[7] loss: 0.107\n",
      "[8] loss: 0.100\n",
      "[9] loss: 0.087\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.076\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.049\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.044\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 0.679\n",
      "[2] loss: 0.278\n",
      "[3] loss: 0.209\n",
      "[4] loss: 0.162\n",
      "[5] loss: 0.142\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.130\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.116\n",
      "[10] loss: 0.115\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.099\n",
      "[13] loss: 0.096\n",
      "[14] loss: 0.091\n",
      "[15] loss: 0.090\n",
      "[16] loss: 0.086\n",
      "[17] loss: 0.082\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.079\n",
      "[20] loss: 0.076\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.672\n",
      "[2] loss: 0.277\n",
      "[3] loss: 0.210\n",
      "[4] loss: 0.162\n",
      "[5] loss: 0.145\n",
      "[6] loss: 0.125\n",
      "[7] loss: 0.111\n",
      "[8] loss: 0.103\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.092\n",
      "[11] loss: 0.084\n",
      "[12] loss: 0.088\n",
      "[13] loss: 0.089\n",
      "[14] loss: 0.078\n",
      "[15] loss: 0.083\n",
      "[16] loss: 0.083\n",
      "[17] loss: 0.074\n",
      "[18] loss: 0.075\n",
      "[19] loss: 0.073\n",
      "[20] loss: 0.072\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 1.944\n",
      "[2] loss: 0.941\n",
      "[3] loss: 0.432\n",
      "[4] loss: 0.310\n",
      "[5] loss: 0.259\n",
      "[6] loss: 0.214\n",
      "[7] loss: 0.183\n",
      "[8] loss: 0.172\n",
      "[9] loss: 0.150\n",
      "[10] loss: 0.136\n",
      "[11] loss: 0.133\n",
      "[12] loss: 0.118\n",
      "[13] loss: 0.110\n",
      "[14] loss: 0.105\n",
      "[15] loss: 0.103\n",
      "[16] loss: 0.087\n",
      "[17] loss: 0.086\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.072\n",
      "[20] loss: 0.070\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 0.894\n",
      "[2] loss: 0.337\n",
      "[3] loss: 0.232\n",
      "[4] loss: 0.186\n",
      "[5] loss: 0.159\n",
      "[6] loss: 0.140\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.114\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.095\n",
      "[11] loss: 0.084\n",
      "[12] loss: 0.085\n",
      "[13] loss: 0.077\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.063\n",
      "[16] loss: 0.061\n",
      "[17] loss: 0.058\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 1.490\n",
      "[2] loss: 0.491\n",
      "[3] loss: 0.305\n",
      "[4] loss: 0.216\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.156\n",
      "[7] loss: 0.141\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.132\n",
      "[10] loss: 0.119\n",
      "[11] loss: 0.113\n",
      "[12] loss: 0.114\n",
      "[13] loss: 0.107\n",
      "[14] loss: 0.102\n",
      "[15] loss: 0.097\n",
      "[16] loss: 0.095\n",
      "[17] loss: 0.090\n",
      "[18] loss: 0.087\n",
      "[19] loss: 0.085\n",
      "[20] loss: 0.080\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 1.050\n",
      "[2] loss: 0.321\n",
      "[3] loss: 0.217\n",
      "[4] loss: 0.179\n",
      "[5] loss: 0.155\n",
      "[6] loss: 0.137\n",
      "[7] loss: 0.134\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.109\n",
      "[10] loss: 0.105\n",
      "[11] loss: 0.098\n",
      "[12] loss: 0.093\n",
      "[13] loss: 0.100\n",
      "[14] loss: 0.090\n",
      "[15] loss: 0.092\n",
      "[16] loss: 0.083\n",
      "[17] loss: 0.081\n",
      "[18] loss: 0.084\n",
      "[19] loss: 0.087\n",
      "[20] loss: 0.085\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 0.882\n",
      "[2] loss: 0.407\n",
      "[3] loss: 0.245\n",
      "[4] loss: 0.189\n",
      "[5] loss: 0.158\n",
      "[6] loss: 0.137\n",
      "[7] loss: 0.122\n",
      "[8] loss: 0.111\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.093\n",
      "[11] loss: 0.081\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.074\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.068\n",
      "[16] loss: 0.062\n",
      "[17] loss: 0.058\n",
      "[18] loss: 0.056\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 0.867\n",
      "[2] loss: 0.388\n",
      "[3] loss: 0.269\n",
      "[4] loss: 0.207\n",
      "[5] loss: 0.175\n",
      "[6] loss: 0.147\n",
      "[7] loss: 0.132\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.108\n",
      "[10] loss: 0.097\n",
      "[11] loss: 0.086\n",
      "[12] loss: 0.083\n",
      "[13] loss: 0.078\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.068\n",
      "[16] loss: 0.063\n",
      "[17] loss: 0.062\n",
      "[18] loss: 0.057\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.051\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 0.869\n",
      "[2] loss: 0.374\n",
      "[3] loss: 0.235\n",
      "[4] loss: 0.180\n",
      "[5] loss: 0.164\n",
      "[6] loss: 0.160\n",
      "[7] loss: 0.149\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.133\n",
      "[10] loss: 0.123\n",
      "[11] loss: 0.116\n",
      "[12] loss: 0.113\n",
      "[13] loss: 0.105\n",
      "[14] loss: 0.099\n",
      "[15] loss: 0.097\n",
      "[16] loss: 0.091\n",
      "[17] loss: 0.087\n",
      "[18] loss: 0.086\n",
      "[19] loss: 0.085\n",
      "[20] loss: 0.077\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 0.886\n",
      "[2] loss: 0.379\n",
      "[3] loss: 0.253\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.163\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.129\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.114\n",
      "[10] loss: 0.109\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.099\n",
      "[13] loss: 0.092\n",
      "[14] loss: 0.094\n",
      "[15] loss: 0.083\n",
      "[16] loss: 0.082\n",
      "[17] loss: 0.086\n",
      "[18] loss: 0.086\n",
      "[19] loss: 0.083\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.801\n",
      "[2] loss: 0.341\n",
      "[3] loss: 0.241\n",
      "[4] loss: 0.190\n",
      "[5] loss: 0.164\n",
      "[6] loss: 0.136\n",
      "[7] loss: 0.129\n",
      "[8] loss: 0.120\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.100\n",
      "[11] loss: 0.086\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.076\n",
      "[14] loss: 0.070\n",
      "[15] loss: 0.068\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.060\n",
      "[19] loss: 0.054\n",
      "[20] loss: 0.052\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 0.898\n",
      "[2] loss: 0.452\n",
      "[3] loss: 0.287\n",
      "[4] loss: 0.218\n",
      "[5] loss: 0.182\n",
      "[6] loss: 0.151\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.120\n",
      "[9] loss: 0.108\n",
      "[10] loss: 0.099\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.082\n",
      "[13] loss: 0.079\n",
      "[14] loss: 0.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] loss: 0.067\n",
      "[16] loss: 0.067\n",
      "[17] loss: 0.059\n",
      "[18] loss: 0.058\n",
      "[19] loss: 0.054\n",
      "[20] loss: 0.053\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 0.802\n",
      "[2] loss: 0.345\n",
      "[3] loss: 0.240\n",
      "[4] loss: 0.199\n",
      "[5] loss: 0.163\n",
      "[6] loss: 0.146\n",
      "[7] loss: 0.139\n",
      "[8] loss: 0.135\n",
      "[9] loss: 0.125\n",
      "[10] loss: 0.119\n",
      "[11] loss: 0.112\n",
      "[12] loss: 0.103\n",
      "[13] loss: 0.101\n",
      "[14] loss: 0.097\n",
      "[15] loss: 0.092\n",
      "[16] loss: 0.090\n",
      "[17] loss: 0.086\n",
      "[18] loss: 0.083\n",
      "[19] loss: 0.083\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.816\n",
      "[2] loss: 0.375\n",
      "[3] loss: 0.259\n",
      "[4] loss: 0.207\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.160\n",
      "[7] loss: 0.145\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.124\n",
      "[10] loss: 0.112\n",
      "[11] loss: 0.111\n",
      "[12] loss: 0.109\n",
      "[13] loss: 0.111\n",
      "[14] loss: 0.103\n",
      "[15] loss: 0.093\n",
      "[16] loss: 0.102\n",
      "[17] loss: 0.103\n",
      "[18] loss: 0.097\n",
      "[19] loss: 0.091\n",
      "[20] loss: 0.092\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 0.546\n",
      "[2] loss: 0.225\n",
      "[3] loss: 0.175\n",
      "[4] loss: 0.149\n",
      "[5] loss: 0.127\n",
      "[6] loss: 0.113\n",
      "[7] loss: 0.105\n",
      "[8] loss: 0.094\n",
      "[9] loss: 0.086\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.073\n",
      "[12] loss: 0.074\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.056\n",
      "[16] loss: 0.056\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 0.535\n",
      "[2] loss: 0.219\n",
      "[3] loss: 0.172\n",
      "[4] loss: 0.146\n",
      "[5] loss: 0.137\n",
      "[6] loss: 0.118\n",
      "[7] loss: 0.102\n",
      "[8] loss: 0.093\n",
      "[9] loss: 0.088\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.074\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.058\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.052\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 0.546\n",
      "[2] loss: 0.223\n",
      "[3] loss: 0.171\n",
      "[4] loss: 0.146\n",
      "[5] loss: 0.140\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.123\n",
      "[8] loss: 0.119\n",
      "[9] loss: 0.121\n",
      "[10] loss: 0.116\n",
      "[11] loss: 0.105\n",
      "[12] loss: 0.101\n",
      "[13] loss: 0.094\n",
      "[14] loss: 0.093\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.088\n",
      "[17] loss: 0.084\n",
      "[18] loss: 0.083\n",
      "[19] loss: 0.080\n",
      "[20] loss: 0.078\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 0.547\n",
      "[2] loss: 0.225\n",
      "[3] loss: 0.174\n",
      "[4] loss: 0.151\n",
      "[5] loss: 0.134\n",
      "[6] loss: 0.118\n",
      "[7] loss: 0.118\n",
      "[8] loss: 0.104\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.097\n",
      "[11] loss: 0.091\n",
      "[12] loss: 0.088\n",
      "[13] loss: 0.085\n",
      "[14] loss: 0.076\n",
      "[15] loss: 0.075\n",
      "[16] loss: 0.083\n",
      "[17] loss: 0.091\n",
      "[18] loss: 0.087\n",
      "[19] loss: 0.081\n",
      "[20] loss: 0.072\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.843\n",
      "[2] loss: 0.273\n",
      "[3] loss: 0.192\n",
      "[4] loss: 0.157\n",
      "[5] loss: 0.134\n",
      "[6] loss: 0.118\n",
      "[7] loss: 0.108\n",
      "[8] loss: 0.098\n",
      "[9] loss: 0.091\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.069\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.062\n",
      "[15] loss: 0.061\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.054\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.044\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 8\n",
      "[1] loss: 0.946\n",
      "[2] loss: 0.391\n",
      "[3] loss: 0.261\n",
      "[4] loss: 0.204\n",
      "[5] loss: 0.161\n",
      "[6] loss: 0.144\n",
      "[7] loss: 0.124\n",
      "[8] loss: 0.113\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.088\n",
      "[11] loss: 0.083\n",
      "[12] loss: 0.074\n",
      "[13] loss: 0.075\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.065\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.052\n",
      "[19] loss: 0.049\n",
      "[20] loss: 0.043\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 0.846\n",
      "[2] loss: 0.278\n",
      "[3] loss: 0.194\n",
      "[4] loss: 0.155\n",
      "[5] loss: 0.138\n",
      "[6] loss: 0.122\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.115\n",
      "[9] loss: 0.110\n",
      "[10] loss: 0.105\n",
      "[11] loss: 0.099\n",
      "[12] loss: 0.097\n",
      "[13] loss: 0.094\n",
      "[14] loss: 0.090\n",
      "[15] loss: 0.084\n",
      "[16] loss: 0.082\n",
      "[17] loss: 0.083\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.075\n",
      "[20] loss: 0.077\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.883\n",
      "[2] loss: 0.280\n",
      "[3] loss: 0.202\n",
      "[4] loss: 0.165\n",
      "[5] loss: 0.145\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.118\n",
      "[8] loss: 0.113\n",
      "[9] loss: 0.107\n",
      "[10] loss: 0.101\n",
      "[11] loss: 0.101\n",
      "[12] loss: 0.102\n",
      "[13] loss: 0.091\n",
      "[14] loss: 0.092\n",
      "[15] loss: 0.084\n",
      "[16] loss: 0.090\n",
      "[17] loss: 0.089\n",
      "[18] loss: 0.079\n",
      "[19] loss: 0.077\n",
      "[20] loss: 0.075\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.821\n",
      "[2] loss: 0.275\n",
      "[3] loss: 0.192\n",
      "[4] loss: 0.152\n",
      "[5] loss: 0.128\n",
      "[6] loss: 0.116\n",
      "[7] loss: 0.101\n",
      "[8] loss: 0.095\n",
      "[9] loss: 0.085\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.071\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.061\n",
      "[14] loss: 0.060\n",
      "[15] loss: 0.054\n",
      "[16] loss: 0.052\n",
      "[17] loss: 0.048\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.043\n",
      "[20] loss: 0.041\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 0.820\n",
      "[2] loss: 0.276\n",
      "[3] loss: 0.213\n",
      "[4] loss: 0.173\n",
      "[5] loss: 0.144\n",
      "[6] loss: 0.128\n",
      "[7] loss: 0.112\n",
      "[8] loss: 0.098\n",
      "[9] loss: 0.097\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.074\n",
      "[13] loss: 0.067\n",
      "[14] loss: 0.062\n",
      "[15] loss: 0.064\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.059\n",
      "[18] loss: 0.050\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.049\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 0.850\n",
      "[2] loss: 0.284\n",
      "[3] loss: 0.201\n",
      "[4] loss: 0.164\n",
      "[5] loss: 0.156\n",
      "[6] loss: 0.139\n",
      "[7] loss: 0.133\n",
      "[8] loss: 0.125\n",
      "[9] loss: 0.121\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.102\n",
      "[13] loss: 0.096\n",
      "[14] loss: 0.090\n",
      "[15] loss: 0.089\n",
      "[16] loss: 0.083\n",
      "[17] loss: 0.079\n",
      "[18] loss: 0.077\n",
      "[19] loss: 0.075\n",
      "[20] loss: 0.072\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.852\n",
      "[2] loss: 0.295\n",
      "[3] loss: 0.200\n",
      "[4] loss: 0.172\n",
      "[5] loss: 0.146\n",
      "[6] loss: 0.129\n",
      "[7] loss: 0.120\n",
      "[8] loss: 0.104\n",
      "[9] loss: 0.103\n",
      "[10] loss: 0.097\n",
      "[11] loss: 0.094\n",
      "[12] loss: 0.086\n",
      "[13] loss: 0.086\n",
      "[14] loss: 0.084\n",
      "[15] loss: 0.085\n",
      "[16] loss: 0.077\n",
      "[17] loss: 0.085\n",
      "[18] loss: 0.078\n",
      "[19] loss: 0.073\n",
      "[20] loss: 0.070\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 0.790\n",
      "[2] loss: 0.277\n",
      "[3] loss: 0.203\n",
      "[4] loss: 0.157\n",
      "[5] loss: 0.130\n",
      "[6] loss: 0.117\n",
      "[7] loss: 0.102\n",
      "[8] loss: 0.096\n",
      "[9] loss: 0.088\n",
      "[10] loss: 0.080\n",
      "[11] loss: 0.072\n",
      "[12] loss: 0.069\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.060\n",
      "[16] loss: 0.053\n",
      "[17] loss: 0.052\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.046\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 0.827\n",
      "[2] loss: 0.303\n",
      "[3] loss: 0.217\n",
      "[4] loss: 0.182\n",
      "[5] loss: 0.150\n",
      "[6] loss: 0.133\n",
      "[7] loss: 0.115\n",
      "[8] loss: 0.104\n",
      "[9] loss: 0.093\n",
      "[10] loss: 0.088\n",
      "[11] loss: 0.076\n",
      "[12] loss: 0.072\n",
      "[13] loss: 0.070\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.058\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.052\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 0.829\n",
      "[2] loss: 0.304\n",
      "[3] loss: 0.215\n",
      "[4] loss: 0.177\n",
      "[5] loss: 0.152\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.129\n",
      "[8] loss: 0.121\n",
      "[9] loss: 0.114\n",
      "[10] loss: 0.108\n",
      "[11] loss: 0.106\n",
      "[12] loss: 0.101\n",
      "[13] loss: 0.093\n",
      "[14] loss: 0.091\n",
      "[15] loss: 0.086\n",
      "[16] loss: 0.084\n",
      "[17] loss: 0.081\n",
      "[18] loss: 0.078\n",
      "[19] loss: 0.074\n",
      "[20] loss: 0.074\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.799\n",
      "[2] loss: 0.289\n",
      "[3] loss: 0.205\n",
      "[4] loss: 0.172\n",
      "[5] loss: 0.150\n",
      "[6] loss: 0.133\n",
      "[7] loss: 0.119\n",
      "[8] loss: 0.106\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.100\n",
      "[11] loss: 0.092\n",
      "[12] loss: 0.093\n",
      "[13] loss: 0.088\n",
      "[14] loss: 0.092\n",
      "[15] loss: 0.082\n",
      "[16] loss: 0.076\n",
      "[17] loss: 0.081\n",
      "[18] loss: 0.071\n",
      "[19] loss: 0.078\n",
      "[20] loss: 0.076\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH3UnzawLy62VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e93c83ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.004\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 2.033\n",
      "[2] loss: 1.869\n",
      "[3] loss: 1.816\n",
      "[4] loss: 1.773\n",
      "[5] loss: 1.740\n",
      "[6] loss: 1.716\n",
      "[7] loss: 1.681\n",
      "[8] loss: 1.654\n",
      "[9] loss: 1.644\n",
      "[10] loss: 1.611\n",
      "[11] loss: 1.591\n",
      "[12] loss: 1.574\n",
      "[13] loss: 1.546\n",
      "[14] loss: 1.531\n",
      "[15] loss: 1.518\n",
      "[16] loss: 1.505\n",
      "[17] loss: 1.486\n",
      "[18] loss: 1.485\n",
      "[19] loss: 1.464\n",
      "[20] loss: 1.465\n",
      "[21] loss: 1.434\n",
      "[22] loss: 1.430\n",
      "[23] loss: 1.425\n",
      "[24] loss: 1.400\n",
      "[25] loss: 1.411\n",
      "[26] loss: 1.382\n",
      "[27] loss: 1.381\n",
      "[28] loss: 1.368\n",
      "[29] loss: 1.366\n",
      "[30] loss: 1.345\n",
      "[31] loss: 1.350\n",
      "[32] loss: 1.337\n",
      "[33] loss: 1.326\n",
      "[34] loss: 1.318\n",
      "[35] loss: 1.311\n",
      "[36] loss: 1.300\n",
      "[37] loss: 1.298\n",
      "[38] loss: 1.288\n",
      "[39] loss: 1.283\n",
      "[40] loss: 1.281\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 2.037\n",
      "[2] loss: 1.882\n",
      "[3] loss: 1.826\n",
      "[4] loss: 1.770\n",
      "[5] loss: 1.754\n",
      "[6] loss: 1.710\n",
      "[7] loss: 1.673\n",
      "[8] loss: 1.648\n",
      "[9] loss: 1.620\n",
      "[10] loss: 1.596\n",
      "[11] loss: 1.575\n",
      "[12] loss: 1.555\n",
      "[13] loss: 1.529\n",
      "[14] loss: 1.521\n",
      "[15] loss: 1.501\n",
      "[16] loss: 1.492\n",
      "[17] loss: 1.471\n",
      "[18] loss: 1.462\n",
      "[19] loss: 1.443\n",
      "[20] loss: 1.443\n",
      "[21] loss: 1.418\n",
      "[22] loss: 1.404\n",
      "[23] loss: 1.393\n",
      "[24] loss: 1.382\n",
      "[25] loss: 1.386\n",
      "[26] loss: 1.358\n",
      "[27] loss: 1.349\n",
      "[28] loss: 1.340\n",
      "[29] loss: 1.340\n",
      "[30] loss: 1.317\n",
      "[31] loss: 1.327\n",
      "[32] loss: 1.309\n",
      "[33] loss: 1.298\n",
      "[34] loss: 1.291\n",
      "[35] loss: 1.279\n",
      "[36] loss: 1.272\n",
      "[37] loss: 1.274\n",
      "[38] loss: 1.259\n",
      "[39] loss: 1.265\n",
      "[40] loss: 1.261\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 2.042\n",
      "[2] loss: 1.866\n",
      "[3] loss: 1.792\n",
      "[4] loss: 1.736\n",
      "[5] loss: 1.705\n",
      "[6] loss: 1.680\n",
      "[7] loss: 1.649\n",
      "[8] loss: 1.618\n",
      "[9] loss: 1.600\n",
      "[10] loss: 1.567\n",
      "[11] loss: 1.543\n",
      "[12] loss: 1.513\n",
      "[13] loss: 1.482\n",
      "[14] loss: 1.459\n",
      "[15] loss: 1.439\n",
      "[16] loss: 1.417\n",
      "[17] loss: 1.391\n",
      "[18] loss: 1.372\n",
      "[19] loss: 1.354\n",
      "[20] loss: 1.334\n",
      "[21] loss: 1.315\n",
      "[22] loss: 1.310\n",
      "[23] loss: 1.286\n",
      "[24] loss: 1.261\n",
      "[25] loss: 1.256\n",
      "[26] loss: 1.245\n",
      "[27] loss: 1.232\n",
      "[28] loss: 1.218\n",
      "[29] loss: 1.202\n",
      "[30] loss: 1.208\n",
      "[31] loss: 1.212\n",
      "[32] loss: 1.214\n",
      "[33] loss: 1.192\n",
      "[34] loss: 1.212\n",
      "[35] loss: 1.201\n",
      "[36] loss: 1.202\n",
      "[37] loss: 1.200\n",
      "[38] loss: 1.223\n",
      "[39] loss: 1.212\n",
      "[40] loss: 1.215\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 2.037\n",
      "[2] loss: 1.890\n",
      "[3] loss: 1.825\n",
      "[4] loss: 1.788\n",
      "[5] loss: 1.759\n",
      "[6] loss: 1.728\n",
      "[7] loss: 1.708\n",
      "[8] loss: 1.688\n",
      "[9] loss: 1.664\n",
      "[10] loss: 1.638\n",
      "[11] loss: 1.623\n",
      "[12] loss: 1.615\n",
      "[13] loss: 1.593\n",
      "[14] loss: 1.586\n",
      "[15] loss: 1.569\n",
      "[16] loss: 1.548\n",
      "[17] loss: 1.537\n",
      "[18] loss: 1.536\n",
      "[19] loss: 1.525\n",
      "[20] loss: 1.521\n",
      "[21] loss: 1.507\n",
      "[22] loss: 1.507\n",
      "[23] loss: 1.482\n",
      "[24] loss: 1.489\n",
      "[25] loss: 1.476\n",
      "[26] loss: 1.469\n",
      "[27] loss: 1.474\n",
      "[28] loss: 1.458\n",
      "[29] loss: 1.453\n",
      "[30] loss: 1.444\n",
      "[31] loss: 1.434\n",
      "[32] loss: 1.431\n",
      "[33] loss: 1.426\n",
      "[34] loss: 1.422\n",
      "[35] loss: 1.408\n",
      "[36] loss: 1.398\n",
      "[37] loss: 1.413\n",
      "[38] loss: 1.379\n",
      "[39] loss: 1.384\n",
      "[40] loss: 1.382\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 2.061\n",
      "[2] loss: 1.909\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.795\n",
      "[5] loss: 1.761\n",
      "[6] loss: 1.720\n",
      "[7] loss: 1.698\n",
      "[8] loss: 1.662\n",
      "[9] loss: 1.647\n",
      "[10] loss: 1.620\n",
      "[11] loss: 1.597\n",
      "[12] loss: 1.590\n",
      "[13] loss: 1.567\n",
      "[14] loss: 1.556\n",
      "[15] loss: 1.541\n",
      "[16] loss: 1.529\n",
      "[17] loss: 1.519\n",
      "[18] loss: 1.513\n",
      "[19] loss: 1.500\n",
      "[20] loss: 1.489\n",
      "[21] loss: 1.481\n",
      "[22] loss: 1.470\n",
      "[23] loss: 1.459\n",
      "[24] loss: 1.447\n",
      "[25] loss: 1.442\n",
      "[26] loss: 1.425\n",
      "[27] loss: 1.425\n",
      "[28] loss: 1.418\n",
      "[29] loss: 1.405\n",
      "[30] loss: 1.404\n",
      "[31] loss: 1.383\n",
      "[32] loss: 1.380\n",
      "[33] loss: 1.380\n",
      "[34] loss: 1.369\n",
      "[35] loss: 1.361\n",
      "[36] loss: 1.353\n",
      "[37] loss: 1.351\n",
      "[38] loss: 1.336\n",
      "[39] loss: 1.334\n",
      "[40] loss: 1.331\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 2.032\n",
      "[2] loss: 1.894\n",
      "[3] loss: 1.834\n",
      "[4] loss: 1.779\n",
      "[5] loss: 1.742\n",
      "[6] loss: 1.707\n",
      "[7] loss: 1.685\n",
      "[8] loss: 1.656\n",
      "[9] loss: 1.634\n",
      "[10] loss: 1.618\n",
      "[11] loss: 1.596\n",
      "[12] loss: 1.581\n",
      "[13] loss: 1.553\n",
      "[14] loss: 1.544\n",
      "[15] loss: 1.529\n",
      "[16] loss: 1.514\n",
      "[17] loss: 1.503\n",
      "[18] loss: 1.481\n",
      "[19] loss: 1.478\n",
      "[20] loss: 1.462\n",
      "[21] loss: 1.458\n",
      "[22] loss: 1.444\n",
      "[23] loss: 1.434\n",
      "[24] loss: 1.429\n",
      "[25] loss: 1.410\n",
      "[26] loss: 1.399\n",
      "[27] loss: 1.405\n",
      "[28] loss: 1.394\n",
      "[29] loss: 1.378\n",
      "[30] loss: 1.380\n",
      "[31] loss: 1.359\n",
      "[32] loss: 1.357\n",
      "[33] loss: 1.348\n",
      "[34] loss: 1.342\n",
      "[35] loss: 1.343\n",
      "[36] loss: 1.328\n",
      "[37] loss: 1.336\n",
      "[38] loss: 1.310\n",
      "[39] loss: 1.315\n",
      "[40] loss: 1.303\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 2.040\n",
      "[2] loss: 1.889\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.743\n",
      "[5] loss: 1.690\n",
      "[6] loss: 1.668\n",
      "[7] loss: 1.631\n",
      "[8] loss: 1.604\n",
      "[9] loss: 1.561\n",
      "[10] loss: 1.541\n",
      "[11] loss: 1.518\n",
      "[12] loss: 1.497\n",
      "[13] loss: 1.478\n",
      "[14] loss: 1.459\n",
      "[15] loss: 1.434\n",
      "[16] loss: 1.418\n",
      "[17] loss: 1.385\n",
      "[18] loss: 1.362\n",
      "[19] loss: 1.348\n",
      "[20] loss: 1.327\n",
      "[21] loss: 1.312\n",
      "[22] loss: 1.291\n",
      "[23] loss: 1.272\n",
      "[24] loss: 1.258\n",
      "[25] loss: 1.241\n",
      "[26] loss: 1.236\n",
      "[27] loss: 1.222\n",
      "[28] loss: 1.213\n",
      "[29] loss: 1.206\n",
      "[30] loss: 1.207\n",
      "[31] loss: 1.197\n",
      "[32] loss: 1.193\n",
      "[33] loss: 1.190\n",
      "[34] loss: 1.178\n",
      "[35] loss: 1.202\n",
      "[36] loss: 1.221\n",
      "[37] loss: 1.229\n",
      "[38] loss: 1.230\n",
      "[39] loss: 1.228\n",
      "[40] loss: 1.232\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 2.037\n",
      "[2] loss: 1.887\n",
      "[3] loss: 1.823\n",
      "[4] loss: 1.775\n",
      "[5] loss: 1.746\n",
      "[6] loss: 1.713\n",
      "[7] loss: 1.679\n",
      "[8] loss: 1.656\n",
      "[9] loss: 1.624\n",
      "[10] loss: 1.618\n",
      "[11] loss: 1.588\n",
      "[12] loss: 1.584\n",
      "[13] loss: 1.559\n",
      "[14] loss: 1.557\n",
      "[15] loss: 1.529\n",
      "[16] loss: 1.528\n",
      "[17] loss: 1.509\n",
      "[18] loss: 1.505\n",
      "[19] loss: 1.496\n",
      "[20] loss: 1.493\n",
      "[21] loss: 1.478\n",
      "[22] loss: 1.463\n",
      "[23] loss: 1.457\n",
      "[24] loss: 1.456\n",
      "[25] loss: 1.443\n",
      "[26] loss: 1.441\n",
      "[27] loss: 1.419\n",
      "[28] loss: 1.430\n",
      "[29] loss: 1.408\n",
      "[30] loss: 1.406\n",
      "[31] loss: 1.389\n",
      "[32] loss: 1.391\n",
      "[33] loss: 1.387\n",
      "[34] loss: 1.378\n",
      "[35] loss: 1.366\n",
      "[36] loss: 1.355\n",
      "[37] loss: 1.361\n",
      "[38] loss: 1.336\n",
      "[39] loss: 1.340\n",
      "[40] loss: 1.338\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 2.051\n",
      "[2] loss: 1.886\n",
      "[3] loss: 1.810\n",
      "[4] loss: 1.782\n",
      "[5] loss: 1.729\n",
      "[6] loss: 1.704\n",
      "[7] loss: 1.687\n",
      "[8] loss: 1.661\n",
      "[9] loss: 1.632\n",
      "[10] loss: 1.609\n",
      "[11] loss: 1.592\n",
      "[12] loss: 1.573\n",
      "[13] loss: 1.560\n",
      "[14] loss: 1.537\n",
      "[15] loss: 1.531\n",
      "[16] loss: 1.526\n",
      "[17] loss: 1.502\n",
      "[18] loss: 1.485\n",
      "[19] loss: 1.474\n",
      "[20] loss: 1.464\n",
      "[21] loss: 1.463\n",
      "[22] loss: 1.447\n",
      "[23] loss: 1.431\n",
      "[24] loss: 1.423\n",
      "[25] loss: 1.414\n",
      "[26] loss: 1.414\n",
      "[27] loss: 1.389\n",
      "[28] loss: 1.380\n",
      "[29] loss: 1.377\n",
      "[30] loss: 1.368\n",
      "[31] loss: 1.359\n",
      "[32] loss: 1.347\n",
      "[33] loss: 1.341\n",
      "[34] loss: 1.332\n",
      "[35] loss: 1.330\n",
      "[36] loss: 1.315\n",
      "[37] loss: 1.319\n",
      "[38] loss: 1.322\n",
      "[39] loss: 1.294\n",
      "[40] loss: 1.293\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 3\n",
      "[1] loss: 2.055\n",
      "[2] loss: 1.886\n",
      "[3] loss: 1.810\n",
      "[4] loss: 1.773\n",
      "[5] loss: 1.725\n",
      "[6] loss: 1.702\n",
      "[7] loss: 1.673\n",
      "[8] loss: 1.639\n",
      "[9] loss: 1.621\n",
      "[10] loss: 1.595\n",
      "[11] loss: 1.579\n",
      "[12] loss: 1.563\n",
      "[13] loss: 1.545\n",
      "[14] loss: 1.523\n",
      "[15] loss: 1.517\n",
      "[16] loss: 1.497\n",
      "[17] loss: 1.475\n",
      "[18] loss: 1.464\n",
      "[19] loss: 1.457\n",
      "[20] loss: 1.445\n",
      "[21] loss: 1.432\n",
      "[22] loss: 1.426\n",
      "[23] loss: 1.418\n",
      "[24] loss: 1.411\n",
      "[25] loss: 1.392\n",
      "[26] loss: 1.384\n",
      "[27] loss: 1.376\n",
      "[28] loss: 1.371\n",
      "[29] loss: 1.365\n",
      "[30] loss: 1.356\n",
      "[31] loss: 1.337\n",
      "[32] loss: 1.333\n",
      "[33] loss: 1.331\n",
      "[34] loss: 1.323\n",
      "[35] loss: 1.314\n",
      "[36] loss: 1.306\n",
      "[37] loss: 1.298\n",
      "[38] loss: 1.298\n",
      "[39] loss: 1.273\n",
      "[40] loss: 1.271\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 2.045\n",
      "[2] loss: 1.857\n",
      "[3] loss: 1.790\n",
      "[4] loss: 1.725\n",
      "[5] loss: 1.685\n",
      "[6] loss: 1.665\n",
      "[7] loss: 1.622\n",
      "[8] loss: 1.607\n",
      "[9] loss: 1.574\n",
      "[10] loss: 1.556\n",
      "[11] loss: 1.524\n",
      "[12] loss: 1.511\n",
      "[13] loss: 1.484\n",
      "[14] loss: 1.460\n",
      "[15] loss: 1.448\n",
      "[16] loss: 1.418\n",
      "[17] loss: 1.394\n",
      "[18] loss: 1.379\n",
      "[19] loss: 1.355\n",
      "[20] loss: 1.334\n",
      "[21] loss: 1.313\n",
      "[22] loss: 1.302\n",
      "[23] loss: 1.278\n",
      "[24] loss: 1.270\n",
      "[25] loss: 1.251\n",
      "[26] loss: 1.242\n",
      "[27] loss: 1.234\n",
      "[28] loss: 1.221\n",
      "[29] loss: 1.216\n",
      "[30] loss: 1.210\n",
      "[31] loss: 1.198\n",
      "[32] loss: 1.201\n",
      "[33] loss: 1.199\n",
      "[34] loss: 1.202\n",
      "[35] loss: 1.222\n",
      "[36] loss: 1.216\n",
      "[37] loss: 1.222\n",
      "[38] loss: 1.244\n",
      "[39] loss: 1.231\n",
      "[40] loss: 1.235\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 2.042\n",
      "[2] loss: 1.887\n",
      "[3] loss: 1.838\n",
      "[4] loss: 1.792\n",
      "[5] loss: 1.752\n",
      "[6] loss: 1.739\n",
      "[7] loss: 1.713\n",
      "[8] loss: 1.687\n",
      "[9] loss: 1.662\n",
      "[10] loss: 1.638\n",
      "[11] loss: 1.620\n",
      "[12] loss: 1.602\n",
      "[13] loss: 1.587\n",
      "[14] loss: 1.587\n",
      "[15] loss: 1.569\n",
      "[16] loss: 1.549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] loss: 1.540\n",
      "[18] loss: 1.524\n",
      "[19] loss: 1.521\n",
      "[20] loss: 1.504\n",
      "[21] loss: 1.500\n",
      "[22] loss: 1.487\n",
      "[23] loss: 1.492\n",
      "[24] loss: 1.476\n",
      "[25] loss: 1.469\n",
      "[26] loss: 1.465\n",
      "[27] loss: 1.461\n",
      "[28] loss: 1.450\n",
      "[29] loss: 1.444\n",
      "[30] loss: 1.426\n",
      "[31] loss: 1.427\n",
      "[32] loss: 1.421\n",
      "[33] loss: 1.408\n",
      "[34] loss: 1.424\n",
      "[35] loss: 1.404\n",
      "[36] loss: 1.406\n",
      "[37] loss: 1.399\n",
      "[38] loss: 1.394\n",
      "[39] loss: 1.384\n",
      "[40] loss: 1.366\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 2.023\n",
      "[2] loss: 1.883\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.769\n",
      "[5] loss: 1.740\n",
      "[6] loss: 1.712\n",
      "[7] loss: 1.681\n",
      "[8] loss: 1.660\n",
      "[9] loss: 1.639\n",
      "[10] loss: 1.609\n",
      "[11] loss: 1.595\n",
      "[12] loss: 1.566\n",
      "[13] loss: 1.551\n",
      "[14] loss: 1.526\n",
      "[15] loss: 1.517\n",
      "[16] loss: 1.509\n",
      "[17] loss: 1.487\n",
      "[18] loss: 1.470\n",
      "[19] loss: 1.468\n",
      "[20] loss: 1.452\n",
      "[21] loss: 1.433\n",
      "[22] loss: 1.426\n",
      "[23] loss: 1.418\n",
      "[24] loss: 1.407\n",
      "[25] loss: 1.398\n",
      "[26] loss: 1.388\n",
      "[27] loss: 1.379\n",
      "[28] loss: 1.355\n",
      "[29] loss: 1.362\n",
      "[30] loss: 1.351\n",
      "[31] loss: 1.349\n",
      "[32] loss: 1.334\n",
      "[33] loss: 1.326\n",
      "[34] loss: 1.315\n",
      "[35] loss: 1.317\n",
      "[36] loss: 1.310\n",
      "[37] loss: 1.297\n",
      "[38] loss: 1.288\n",
      "[39] loss: 1.283\n",
      "[40] loss: 1.267\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 2.044\n",
      "[2] loss: 1.902\n",
      "[3] loss: 1.829\n",
      "[4] loss: 1.779\n",
      "[5] loss: 1.728\n",
      "[6] loss: 1.701\n",
      "[7] loss: 1.666\n",
      "[8] loss: 1.642\n",
      "[9] loss: 1.620\n",
      "[10] loss: 1.593\n",
      "[11] loss: 1.572\n",
      "[12] loss: 1.552\n",
      "[13] loss: 1.514\n",
      "[14] loss: 1.505\n",
      "[15] loss: 1.483\n",
      "[16] loss: 1.483\n",
      "[17] loss: 1.467\n",
      "[18] loss: 1.443\n",
      "[19] loss: 1.445\n",
      "[20] loss: 1.428\n",
      "[21] loss: 1.401\n",
      "[22] loss: 1.403\n",
      "[23] loss: 1.393\n",
      "[24] loss: 1.386\n",
      "[25] loss: 1.371\n",
      "[26] loss: 1.364\n",
      "[27] loss: 1.355\n",
      "[28] loss: 1.347\n",
      "[29] loss: 1.339\n",
      "[30] loss: 1.326\n",
      "[31] loss: 1.321\n",
      "[32] loss: 1.314\n",
      "[33] loss: 1.312\n",
      "[34] loss: 1.296\n",
      "[35] loss: 1.294\n",
      "[36] loss: 1.276\n",
      "[37] loss: 1.268\n",
      "[38] loss: 1.267\n",
      "[39] loss: 1.264\n",
      "[40] loss: 1.240\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 2.023\n",
      "[2] loss: 1.883\n",
      "[3] loss: 1.793\n",
      "[4] loss: 1.732\n",
      "[5] loss: 1.702\n",
      "[6] loss: 1.667\n",
      "[7] loss: 1.635\n",
      "[8] loss: 1.612\n",
      "[9] loss: 1.586\n",
      "[10] loss: 1.552\n",
      "[11] loss: 1.539\n",
      "[12] loss: 1.512\n",
      "[13] loss: 1.486\n",
      "[14] loss: 1.466\n",
      "[15] loss: 1.442\n",
      "[16] loss: 1.439\n",
      "[17] loss: 1.415\n",
      "[18] loss: 1.385\n",
      "[19] loss: 1.379\n",
      "[20] loss: 1.355\n",
      "[21] loss: 1.329\n",
      "[22] loss: 1.312\n",
      "[23] loss: 1.300\n",
      "[24] loss: 1.287\n",
      "[25] loss: 1.279\n",
      "[26] loss: 1.265\n",
      "[27] loss: 1.258\n",
      "[28] loss: 1.244\n",
      "[29] loss: 1.233\n",
      "[30] loss: 1.237\n",
      "[31] loss: 1.222\n",
      "[32] loss: 1.220\n",
      "[33] loss: 1.230\n",
      "[34] loss: 1.233\n",
      "[35] loss: 1.234\n",
      "[36] loss: 1.258\n",
      "[37] loss: 1.274\n",
      "[38] loss: 1.284\n",
      "[39] loss: 1.260\n",
      "[40] loss: 1.253\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 2.063\n",
      "[2] loss: 1.918\n",
      "[3] loss: 1.848\n",
      "[4] loss: 1.817\n",
      "[5] loss: 1.770\n",
      "[6] loss: 1.740\n",
      "[7] loss: 1.720\n",
      "[8] loss: 1.695\n",
      "[9] loss: 1.666\n",
      "[10] loss: 1.646\n",
      "[11] loss: 1.629\n",
      "[12] loss: 1.621\n",
      "[13] loss: 1.588\n",
      "[14] loss: 1.588\n",
      "[15] loss: 1.567\n",
      "[16] loss: 1.557\n",
      "[17] loss: 1.550\n",
      "[18] loss: 1.543\n",
      "[19] loss: 1.528\n",
      "[20] loss: 1.522\n",
      "[21] loss: 1.511\n",
      "[22] loss: 1.501\n",
      "[23] loss: 1.487\n",
      "[24] loss: 1.476\n",
      "[25] loss: 1.472\n",
      "[26] loss: 1.464\n",
      "[27] loss: 1.466\n",
      "[28] loss: 1.442\n",
      "[29] loss: 1.432\n",
      "[30] loss: 1.435\n",
      "[31] loss: 1.427\n",
      "[32] loss: 1.412\n",
      "[33] loss: 1.412\n",
      "[34] loss: 1.400\n",
      "[35] loss: 1.410\n",
      "[36] loss: 1.392\n",
      "[37] loss: 1.384\n",
      "[38] loss: 1.376\n",
      "[39] loss: 1.366\n",
      "[40] loss: 1.357\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 2.083\n",
      "[2] loss: 1.911\n",
      "[3] loss: 1.850\n",
      "[4] loss: 1.810\n",
      "[5] loss: 1.766\n",
      "[6] loss: 1.743\n",
      "[7] loss: 1.718\n",
      "[8] loss: 1.688\n",
      "[9] loss: 1.668\n",
      "[10] loss: 1.657\n",
      "[11] loss: 1.629\n",
      "[12] loss: 1.609\n",
      "[13] loss: 1.596\n",
      "[14] loss: 1.588\n",
      "[15] loss: 1.563\n",
      "[16] loss: 1.547\n",
      "[17] loss: 1.540\n",
      "[18] loss: 1.521\n",
      "[19] loss: 1.515\n",
      "[20] loss: 1.501\n",
      "[21] loss: 1.488\n",
      "[22] loss: 1.474\n",
      "[23] loss: 1.462\n",
      "[24] loss: 1.453\n",
      "[25] loss: 1.453\n",
      "[26] loss: 1.440\n",
      "[27] loss: 1.432\n",
      "[28] loss: 1.423\n",
      "[29] loss: 1.406\n",
      "[30] loss: 1.401\n",
      "[31] loss: 1.393\n",
      "[32] loss: 1.388\n",
      "[33] loss: 1.371\n",
      "[34] loss: 1.367\n",
      "[35] loss: 1.353\n",
      "[36] loss: 1.365\n",
      "[37] loss: 1.345\n",
      "[38] loss: 1.347\n",
      "[39] loss: 1.331\n",
      "[40] loss: 1.326\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 2.051\n",
      "[2] loss: 1.883\n",
      "[3] loss: 1.832\n",
      "[4] loss: 1.783\n",
      "[5] loss: 1.742\n",
      "[6] loss: 1.711\n",
      "[7] loss: 1.691\n",
      "[8] loss: 1.658\n",
      "[9] loss: 1.631\n",
      "[10] loss: 1.607\n",
      "[11] loss: 1.581\n",
      "[12] loss: 1.557\n",
      "[13] loss: 1.543\n",
      "[14] loss: 1.534\n",
      "[15] loss: 1.512\n",
      "[16] loss: 1.497\n",
      "[17] loss: 1.474\n",
      "[18] loss: 1.462\n",
      "[19] loss: 1.457\n",
      "[20] loss: 1.448\n",
      "[21] loss: 1.427\n",
      "[22] loss: 1.418\n",
      "[23] loss: 1.412\n",
      "[24] loss: 1.407\n",
      "[25] loss: 1.404\n",
      "[26] loss: 1.403\n",
      "[27] loss: 1.378\n",
      "[28] loss: 1.369\n",
      "[29] loss: 1.357\n",
      "[30] loss: 1.353\n",
      "[31] loss: 1.348\n",
      "[32] loss: 1.341\n",
      "[33] loss: 1.334\n",
      "[34] loss: 1.331\n",
      "[35] loss: 1.310\n",
      "[36] loss: 1.316\n",
      "[37] loss: 1.306\n",
      "[38] loss: 1.301\n",
      "[39] loss: 1.293\n",
      "[40] loss: 1.289\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 2.076\n",
      "[2] loss: 1.899\n",
      "[3] loss: 1.827\n",
      "[4] loss: 1.764\n",
      "[5] loss: 1.712\n",
      "[6] loss: 1.679\n",
      "[7] loss: 1.633\n",
      "[8] loss: 1.606\n",
      "[9] loss: 1.574\n",
      "[10] loss: 1.548\n",
      "[11] loss: 1.526\n",
      "[12] loss: 1.500\n",
      "[13] loss: 1.477\n",
      "[14] loss: 1.450\n",
      "[15] loss: 1.431\n",
      "[16] loss: 1.402\n",
      "[17] loss: 1.378\n",
      "[18] loss: 1.356\n",
      "[19] loss: 1.351\n",
      "[20] loss: 1.323\n",
      "[21] loss: 1.298\n",
      "[22] loss: 1.282\n",
      "[23] loss: 1.275\n",
      "[24] loss: 1.256\n",
      "[25] loss: 1.246\n",
      "[26] loss: 1.234\n",
      "[27] loss: 1.217\n",
      "[28] loss: 1.213\n",
      "[29] loss: 1.194\n",
      "[30] loss: 1.198\n",
      "[31] loss: 1.194\n",
      "[32] loss: 1.204\n",
      "[33] loss: 1.206\n",
      "[34] loss: 1.208\n",
      "[35] loss: 1.192\n",
      "[36] loss: 1.211\n",
      "[37] loss: 1.206\n",
      "[38] loss: 1.228\n",
      "[39] loss: 1.215\n",
      "[40] loss: 1.230\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 2.077\n",
      "[2] loss: 1.915\n",
      "[3] loss: 1.865\n",
      "[4] loss: 1.813\n",
      "[5] loss: 1.775\n",
      "[6] loss: 1.750\n",
      "[7] loss: 1.712\n",
      "[8] loss: 1.694\n",
      "[9] loss: 1.677\n",
      "[10] loss: 1.662\n",
      "[11] loss: 1.637\n",
      "[12] loss: 1.619\n",
      "[13] loss: 1.599\n",
      "[14] loss: 1.591\n",
      "[15] loss: 1.568\n",
      "[16] loss: 1.556\n",
      "[17] loss: 1.544\n",
      "[18] loss: 1.525\n",
      "[19] loss: 1.523\n",
      "[20] loss: 1.518\n",
      "[21] loss: 1.498\n",
      "[22] loss: 1.490\n",
      "[23] loss: 1.475\n",
      "[24] loss: 1.470\n",
      "[25] loss: 1.463\n",
      "[26] loss: 1.451\n",
      "[27] loss: 1.439\n",
      "[28] loss: 1.442\n",
      "[29] loss: 1.427\n",
      "[30] loss: 1.422\n",
      "[31] loss: 1.410\n",
      "[32] loss: 1.403\n",
      "[33] loss: 1.407\n",
      "[34] loss: 1.407\n",
      "[35] loss: 1.382\n",
      "[36] loss: 1.388\n",
      "[37] loss: 1.372\n",
      "[38] loss: 1.375\n",
      "[39] loss: 1.357\n",
      "[40] loss: 1.353\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 2.046\n",
      "[2] loss: 1.898\n",
      "[3] loss: 1.834\n",
      "[4] loss: 1.788\n",
      "[5] loss: 1.761\n",
      "[6] loss: 1.733\n",
      "[7] loss: 1.698\n",
      "[8] loss: 1.679\n",
      "[9] loss: 1.652\n",
      "[10] loss: 1.631\n",
      "[11] loss: 1.612\n",
      "[12] loss: 1.590\n",
      "[13] loss: 1.588\n",
      "[14] loss: 1.564\n",
      "[15] loss: 1.542\n",
      "[16] loss: 1.537\n",
      "[17] loss: 1.525\n",
      "[18] loss: 1.522\n",
      "[19] loss: 1.497\n",
      "[20] loss: 1.484\n",
      "[21] loss: 1.479\n",
      "[22] loss: 1.478\n",
      "[23] loss: 1.465\n",
      "[24] loss: 1.447\n",
      "[25] loss: 1.439\n",
      "[26] loss: 1.425\n",
      "[27] loss: 1.420\n",
      "[28] loss: 1.419\n",
      "[29] loss: 1.396\n",
      "[30] loss: 1.388\n",
      "[31] loss: 1.387\n",
      "[32] loss: 1.377\n",
      "[33] loss: 1.366\n",
      "[34] loss: 1.355\n",
      "[35] loss: 1.349\n",
      "[36] loss: 1.348\n",
      "[37] loss: 1.339\n",
      "[38] loss: 1.321\n",
      "[39] loss: 1.330\n",
      "[40] loss: 1.304\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 2.044\n",
      "[2] loss: 1.885\n",
      "[3] loss: 1.816\n",
      "[4] loss: 1.772\n",
      "[5] loss: 1.746\n",
      "[6] loss: 1.707\n",
      "[7] loss: 1.674\n",
      "[8] loss: 1.650\n",
      "[9] loss: 1.623\n",
      "[10] loss: 1.595\n",
      "[11] loss: 1.567\n",
      "[12] loss: 1.545\n",
      "[13] loss: 1.532\n",
      "[14] loss: 1.516\n",
      "[15] loss: 1.497\n",
      "[16] loss: 1.482\n",
      "[17] loss: 1.472\n",
      "[18] loss: 1.467\n",
      "[19] loss: 1.456\n",
      "[20] loss: 1.440\n",
      "[21] loss: 1.416\n",
      "[22] loss: 1.412\n",
      "[23] loss: 1.400\n",
      "[24] loss: 1.391\n",
      "[25] loss: 1.383\n",
      "[26] loss: 1.378\n",
      "[27] loss: 1.365\n",
      "[28] loss: 1.367\n",
      "[29] loss: 1.346\n",
      "[30] loss: 1.340\n",
      "[31] loss: 1.336\n",
      "[32] loss: 1.320\n",
      "[33] loss: 1.317\n",
      "[34] loss: 1.305\n",
      "[35] loss: 1.295\n",
      "[36] loss: 1.291\n",
      "[37] loss: 1.291\n",
      "[38] loss: 1.279\n",
      "[39] loss: 1.282\n",
      "[40] loss: 1.263\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 2.037\n",
      "[2] loss: 1.887\n",
      "[3] loss: 1.809\n",
      "[4] loss: 1.751\n",
      "[5] loss: 1.717\n",
      "[6] loss: 1.677\n",
      "[7] loss: 1.645\n",
      "[8] loss: 1.621\n",
      "[9] loss: 1.587\n",
      "[10] loss: 1.558\n",
      "[11] loss: 1.531\n",
      "[12] loss: 1.504\n",
      "[13] loss: 1.491\n",
      "[14] loss: 1.457\n",
      "[15] loss: 1.435\n",
      "[16] loss: 1.422\n",
      "[17] loss: 1.396\n",
      "[18] loss: 1.384\n",
      "[19] loss: 1.360\n",
      "[20] loss: 1.335\n",
      "[21] loss: 1.318\n",
      "[22] loss: 1.300\n",
      "[23] loss: 1.289\n",
      "[24] loss: 1.270\n",
      "[25] loss: 1.261\n",
      "[26] loss: 1.244\n",
      "[27] loss: 1.239\n",
      "[28] loss: 1.230\n",
      "[29] loss: 1.217\n",
      "[30] loss: 1.211\n",
      "[31] loss: 1.209\n",
      "[32] loss: 1.202\n",
      "[33] loss: 1.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34] loss: 1.210\n",
      "[35] loss: 1.207\n",
      "[36] loss: 1.218\n",
      "[37] loss: 1.226\n",
      "[38] loss: 1.235\n",
      "[39] loss: 1.244\n",
      "[40] loss: 1.241\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 2.029\n",
      "[2] loss: 1.881\n",
      "[3] loss: 1.850\n",
      "[4] loss: 1.788\n",
      "[5] loss: 1.754\n",
      "[6] loss: 1.727\n",
      "[7] loss: 1.700\n",
      "[8] loss: 1.686\n",
      "[9] loss: 1.666\n",
      "[10] loss: 1.640\n",
      "[11] loss: 1.616\n",
      "[12] loss: 1.609\n",
      "[13] loss: 1.593\n",
      "[14] loss: 1.581\n",
      "[15] loss: 1.565\n",
      "[16] loss: 1.557\n",
      "[17] loss: 1.547\n",
      "[18] loss: 1.537\n",
      "[19] loss: 1.531\n",
      "[20] loss: 1.524\n",
      "[21] loss: 1.522\n",
      "[22] loss: 1.511\n",
      "[23] loss: 1.507\n",
      "[24] loss: 1.495\n",
      "[25] loss: 1.494\n",
      "[26] loss: 1.473\n",
      "[27] loss: 1.473\n",
      "[28] loss: 1.469\n",
      "[29] loss: 1.459\n",
      "[30] loss: 1.458\n",
      "[31] loss: 1.452\n",
      "[32] loss: 1.444\n",
      "[33] loss: 1.432\n",
      "[34] loss: 1.427\n",
      "[35] loss: 1.428\n",
      "[36] loss: 1.420\n",
      "[37] loss: 1.415\n",
      "[38] loss: 1.408\n",
      "[39] loss: 1.400\n",
      "[40] loss: 1.391\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 2.059\n",
      "[2] loss: 1.915\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.808\n",
      "[5] loss: 1.757\n",
      "[6] loss: 1.728\n",
      "[7] loss: 1.698\n",
      "[8] loss: 1.683\n",
      "[9] loss: 1.648\n",
      "[10] loss: 1.638\n",
      "[11] loss: 1.618\n",
      "[12] loss: 1.602\n",
      "[13] loss: 1.585\n",
      "[14] loss: 1.576\n",
      "[15] loss: 1.547\n",
      "[16] loss: 1.531\n",
      "[17] loss: 1.518\n",
      "[18] loss: 1.509\n",
      "[19] loss: 1.497\n",
      "[20] loss: 1.483\n",
      "[21] loss: 1.467\n",
      "[22] loss: 1.472\n",
      "[23] loss: 1.443\n",
      "[24] loss: 1.441\n",
      "[25] loss: 1.434\n",
      "[26] loss: 1.423\n",
      "[27] loss: 1.418\n",
      "[28] loss: 1.408\n",
      "[29] loss: 1.398\n",
      "[30] loss: 1.387\n",
      "[31] loss: 1.379\n",
      "[32] loss: 1.380\n",
      "[33] loss: 1.371\n",
      "[34] loss: 1.350\n",
      "[35] loss: 1.349\n",
      "[36] loss: 1.350\n",
      "[37] loss: 1.340\n",
      "[38] loss: 1.332\n",
      "[39] loss: 1.333\n",
      "[40] loss: 1.318\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 2.071\n",
      "[2] loss: 1.907\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.786\n",
      "[5] loss: 1.739\n",
      "[6] loss: 1.712\n",
      "[7] loss: 1.679\n",
      "[8] loss: 1.664\n",
      "[9] loss: 1.624\n",
      "[10] loss: 1.615\n",
      "[11] loss: 1.590\n",
      "[12] loss: 1.573\n",
      "[13] loss: 1.552\n",
      "[14] loss: 1.531\n",
      "[15] loss: 1.512\n",
      "[16] loss: 1.505\n",
      "[17] loss: 1.493\n",
      "[18] loss: 1.470\n",
      "[19] loss: 1.460\n",
      "[20] loss: 1.456\n",
      "[21] loss: 1.446\n",
      "[22] loss: 1.438\n",
      "[23] loss: 1.420\n",
      "[24] loss: 1.425\n",
      "[25] loss: 1.399\n",
      "[26] loss: 1.403\n",
      "[27] loss: 1.391\n",
      "[28] loss: 1.373\n",
      "[29] loss: 1.370\n",
      "[30] loss: 1.365\n",
      "[31] loss: 1.353\n",
      "[32] loss: 1.350\n",
      "[33] loss: 1.355\n",
      "[34] loss: 1.331\n",
      "[35] loss: 1.316\n",
      "[36] loss: 1.317\n",
      "[37] loss: 1.306\n",
      "[38] loss: 1.305\n",
      "[39] loss: 1.302\n",
      "[40] loss: 1.289\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 2.068\n",
      "[2] loss: 1.921\n",
      "[3] loss: 1.828\n",
      "[4] loss: 1.761\n",
      "[5] loss: 1.708\n",
      "[6] loss: 1.677\n",
      "[7] loss: 1.633\n",
      "[8] loss: 1.595\n",
      "[9] loss: 1.567\n",
      "[10] loss: 1.544\n",
      "[11] loss: 1.522\n",
      "[12] loss: 1.497\n",
      "[13] loss: 1.472\n",
      "[14] loss: 1.442\n",
      "[15] loss: 1.424\n",
      "[16] loss: 1.407\n",
      "[17] loss: 1.384\n",
      "[18] loss: 1.357\n",
      "[19] loss: 1.344\n",
      "[20] loss: 1.328\n",
      "[21] loss: 1.303\n",
      "[22] loss: 1.294\n",
      "[23] loss: 1.274\n",
      "[24] loss: 1.256\n",
      "[25] loss: 1.246\n",
      "[26] loss: 1.244\n",
      "[27] loss: 1.218\n",
      "[28] loss: 1.215\n",
      "[29] loss: 1.201\n",
      "[30] loss: 1.200\n",
      "[31] loss: 1.211\n",
      "[32] loss: 1.199\n",
      "[33] loss: 1.188\n",
      "[34] loss: 1.191\n",
      "[35] loss: 1.198\n",
      "[36] loss: 1.201\n",
      "[37] loss: 1.190\n",
      "[38] loss: 1.216\n",
      "[39] loss: 1.213\n",
      "[40] loss: 1.216\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 2.059\n",
      "[2] loss: 1.922\n",
      "[3] loss: 1.857\n",
      "[4] loss: 1.819\n",
      "[5] loss: 1.772\n",
      "[6] loss: 1.753\n",
      "[7] loss: 1.730\n",
      "[8] loss: 1.703\n",
      "[9] loss: 1.684\n",
      "[10] loss: 1.669\n",
      "[11] loss: 1.652\n",
      "[12] loss: 1.635\n",
      "[13] loss: 1.618\n",
      "[14] loss: 1.602\n",
      "[15] loss: 1.584\n",
      "[16] loss: 1.583\n",
      "[17] loss: 1.564\n",
      "[18] loss: 1.554\n",
      "[19] loss: 1.548\n",
      "[20] loss: 1.536\n",
      "[21] loss: 1.530\n",
      "[22] loss: 1.530\n",
      "[23] loss: 1.517\n",
      "[24] loss: 1.509\n",
      "[25] loss: 1.506\n",
      "[26] loss: 1.499\n",
      "[27] loss: 1.495\n",
      "[28] loss: 1.477\n",
      "[29] loss: 1.465\n",
      "[30] loss: 1.471\n",
      "[31] loss: 1.456\n",
      "[32] loss: 1.455\n",
      "[33] loss: 1.457\n",
      "[34] loss: 1.440\n",
      "[35] loss: 1.430\n",
      "[36] loss: 1.434\n",
      "[37] loss: 1.416\n",
      "[38] loss: 1.422\n",
      "[39] loss: 1.414\n",
      "[40] loss: 1.402\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 2.075\n",
      "[2] loss: 1.905\n",
      "[3] loss: 1.836\n",
      "[4] loss: 1.788\n",
      "[5] loss: 1.765\n",
      "[6] loss: 1.740\n",
      "[7] loss: 1.711\n",
      "[8] loss: 1.696\n",
      "[9] loss: 1.661\n",
      "[10] loss: 1.649\n",
      "[11] loss: 1.626\n",
      "[12] loss: 1.614\n",
      "[13] loss: 1.587\n",
      "[14] loss: 1.568\n",
      "[15] loss: 1.558\n",
      "[16] loss: 1.527\n",
      "[17] loss: 1.519\n",
      "[18] loss: 1.506\n",
      "[19] loss: 1.500\n",
      "[20] loss: 1.483\n",
      "[21] loss: 1.469\n",
      "[22] loss: 1.465\n",
      "[23] loss: 1.460\n",
      "[24] loss: 1.435\n",
      "[25] loss: 1.432\n",
      "[26] loss: 1.414\n",
      "[27] loss: 1.414\n",
      "[28] loss: 1.401\n",
      "[29] loss: 1.391\n",
      "[30] loss: 1.382\n",
      "[31] loss: 1.378\n",
      "[32] loss: 1.369\n",
      "[33] loss: 1.359\n",
      "[34] loss: 1.350\n",
      "[35] loss: 1.339\n",
      "[36] loss: 1.324\n",
      "[37] loss: 1.323\n",
      "[38] loss: 1.312\n",
      "[39] loss: 1.311\n",
      "[40] loss: 1.298\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 8\n",
      "[1] loss: 2.106\n",
      "[2] loss: 1.924\n",
      "[3] loss: 1.849\n",
      "[4] loss: 1.802\n",
      "[5] loss: 1.765\n",
      "[6] loss: 1.731\n",
      "[7] loss: 1.691\n",
      "[8] loss: 1.671\n",
      "[9] loss: 1.634\n",
      "[10] loss: 1.613\n",
      "[11] loss: 1.581\n",
      "[12] loss: 1.565\n",
      "[13] loss: 1.539\n",
      "[14] loss: 1.520\n",
      "[15] loss: 1.511\n",
      "[16] loss: 1.482\n",
      "[17] loss: 1.467\n",
      "[18] loss: 1.455\n",
      "[19] loss: 1.436\n",
      "[20] loss: 1.424\n",
      "[21] loss: 1.406\n",
      "[22] loss: 1.405\n",
      "[23] loss: 1.389\n",
      "[24] loss: 1.375\n",
      "[25] loss: 1.364\n",
      "[26] loss: 1.344\n",
      "[27] loss: 1.355\n",
      "[28] loss: 1.342\n",
      "[29] loss: 1.322\n",
      "[30] loss: 1.322\n",
      "[31] loss: 1.319\n",
      "[32] loss: 1.305\n",
      "[33] loss: 1.303\n",
      "[34] loss: 1.291\n",
      "[35] loss: 1.286\n",
      "[36] loss: 1.263\n",
      "[37] loss: 1.274\n",
      "[38] loss: 1.263\n",
      "[39] loss: 1.258\n",
      "[40] loss: 1.254\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 2.071\n",
      "[2] loss: 1.898\n",
      "[3] loss: 1.810\n",
      "[4] loss: 1.748\n",
      "[5] loss: 1.713\n",
      "[6] loss: 1.661\n",
      "[7] loss: 1.639\n",
      "[8] loss: 1.619\n",
      "[9] loss: 1.578\n",
      "[10] loss: 1.557\n",
      "[11] loss: 1.543\n",
      "[12] loss: 1.524\n",
      "[13] loss: 1.490\n",
      "[14] loss: 1.465\n",
      "[15] loss: 1.445\n",
      "[16] loss: 1.419\n",
      "[17] loss: 1.409\n",
      "[18] loss: 1.386\n",
      "[19] loss: 1.360\n",
      "[20] loss: 1.339\n",
      "[21] loss: 1.325\n",
      "[22] loss: 1.312\n",
      "[23] loss: 1.299\n",
      "[24] loss: 1.279\n",
      "[25] loss: 1.264\n",
      "[26] loss: 1.254\n",
      "[27] loss: 1.245\n",
      "[28] loss: 1.231\n",
      "[29] loss: 1.226\n",
      "[30] loss: 1.215\n",
      "[31] loss: 1.209\n",
      "[32] loss: 1.206\n",
      "[33] loss: 1.207\n",
      "[34] loss: 1.206\n",
      "[35] loss: 1.205\n",
      "[36] loss: 1.228\n",
      "[37] loss: 1.236\n",
      "[38] loss: 1.234\n",
      "[39] loss: 1.242\n",
      "[40] loss: 1.246\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 2.053\n",
      "[2] loss: 1.897\n",
      "[3] loss: 1.846\n",
      "[4] loss: 1.798\n",
      "[5] loss: 1.761\n",
      "[6] loss: 1.733\n",
      "[7] loss: 1.696\n",
      "[8] loss: 1.674\n",
      "[9] loss: 1.653\n",
      "[10] loss: 1.632\n",
      "[11] loss: 1.610\n",
      "[12] loss: 1.609\n",
      "[13] loss: 1.577\n",
      "[14] loss: 1.562\n",
      "[15] loss: 1.557\n",
      "[16] loss: 1.533\n",
      "[17] loss: 1.520\n",
      "[18] loss: 1.508\n",
      "[19] loss: 1.502\n",
      "[20] loss: 1.497\n",
      "[21] loss: 1.495\n",
      "[22] loss: 1.487\n",
      "[23] loss: 1.483\n",
      "[24] loss: 1.469\n",
      "[25] loss: 1.445\n",
      "[26] loss: 1.458\n",
      "[27] loss: 1.441\n",
      "[28] loss: 1.439\n",
      "[29] loss: 1.425\n",
      "[30] loss: 1.427\n",
      "[31] loss: 1.419\n",
      "[32] loss: 1.415\n",
      "[33] loss: 1.399\n",
      "[34] loss: 1.395\n",
      "[35] loss: 1.382\n",
      "[36] loss: 1.381\n",
      "[37] loss: 1.368\n",
      "[38] loss: 1.353\n",
      "[39] loss: 1.361\n",
      "[40] loss: 1.348\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 2.135\n",
      "[2] loss: 1.948\n",
      "[3] loss: 1.887\n",
      "[4] loss: 1.845\n",
      "[5] loss: 1.820\n",
      "[6] loss: 1.786\n",
      "[7] loss: 1.766\n",
      "[8] loss: 1.742\n",
      "[9] loss: 1.727\n",
      "[10] loss: 1.702\n",
      "[11] loss: 1.694\n",
      "[12] loss: 1.658\n",
      "[13] loss: 1.652\n",
      "[14] loss: 1.640\n",
      "[15] loss: 1.627\n",
      "[16] loss: 1.599\n",
      "[17] loss: 1.596\n",
      "[18] loss: 1.584\n",
      "[19] loss: 1.574\n",
      "[20] loss: 1.564\n",
      "[21] loss: 1.557\n",
      "[22] loss: 1.543\n",
      "[23] loss: 1.529\n",
      "[24] loss: 1.530\n",
      "[25] loss: 1.512\n",
      "[26] loss: 1.506\n",
      "[27] loss: 1.497\n",
      "[28] loss: 1.476\n",
      "[29] loss: 1.467\n",
      "[30] loss: 1.469\n",
      "[31] loss: 1.463\n",
      "[32] loss: 1.462\n",
      "[33] loss: 1.440\n",
      "[34] loss: 1.433\n",
      "[35] loss: 1.427\n",
      "[36] loss: 1.414\n",
      "[37] loss: 1.406\n",
      "[38] loss: 1.403\n",
      "[39] loss: 1.386\n",
      "[40] loss: 1.379\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.257\n",
      "[3] loss: 2.080\n",
      "[4] loss: 2.046\n",
      "[5] loss: 2.020\n",
      "[6] loss: 1.980\n",
      "[7] loss: 1.926\n",
      "[8] loss: 1.860\n",
      "[9] loss: 1.820\n",
      "[10] loss: 1.797\n",
      "[11] loss: 1.773\n",
      "[12] loss: 1.739\n",
      "[13] loss: 1.734\n",
      "[14] loss: 1.715\n",
      "[15] loss: 1.693\n",
      "[16] loss: 1.680\n",
      "[17] loss: 1.662\n",
      "[18] loss: 1.650\n",
      "[19] loss: 1.638\n",
      "[20] loss: 1.629\n",
      "[21] loss: 1.616\n",
      "[22] loss: 1.612\n",
      "[23] loss: 1.591\n",
      "[24] loss: 1.575\n",
      "[25] loss: 1.572\n",
      "[26] loss: 1.562\n",
      "[27] loss: 1.552\n",
      "[28] loss: 1.533\n",
      "[29] loss: 1.526\n",
      "[30] loss: 1.518\n",
      "[31] loss: 1.510\n",
      "[32] loss: 1.500\n",
      "[33] loss: 1.484\n",
      "[34] loss: 1.487\n",
      "[35] loss: 1.468\n",
      "[36] loss: 1.458\n",
      "[37] loss: 1.451\n",
      "[38] loss: 1.447\n",
      "[39] loss: 1.436\n",
      "[40] loss: 1.426\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.312\n",
      "[3] loss: 2.312\n",
      "[4] loss: 2.312\n",
      "[5] loss: 2.312\n",
      "[6] loss: 2.312\n",
      "[7] loss: 2.312\n",
      "[8] loss: 2.312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] loss: 2.312\n",
      "[10] loss: 2.314\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.312\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.312\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.206\n",
      "[22] loss: 2.066\n",
      "[23] loss: 2.014\n",
      "[24] loss: 1.919\n",
      "[25] loss: 1.877\n",
      "[26] loss: 1.845\n",
      "[27] loss: 1.817\n",
      "[28] loss: 1.794\n",
      "[29] loss: 1.776\n",
      "[30] loss: 1.752\n",
      "[31] loss: 1.716\n",
      "[32] loss: 1.691\n",
      "[33] loss: 1.665\n",
      "[34] loss: 1.649\n",
      "[35] loss: 1.628\n",
      "[36] loss: 1.608\n",
      "[37] loss: 1.597\n",
      "[38] loss: 1.576\n",
      "[39] loss: 1.562\n",
      "[40] loss: 1.555\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.312\n",
      "[3] loss: 2.312\n",
      "[4] loss: 2.312\n",
      "[5] loss: 2.312\n",
      "[6] loss: 2.312\n",
      "[7] loss: 2.312\n",
      "[8] loss: 2.312\n",
      "[9] loss: 2.312\n",
      "[10] loss: 2.312\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.312\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.312\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.312\n",
      "[22] loss: 2.312\n",
      "[23] loss: 2.312\n",
      "[24] loss: 2.312\n",
      "[25] loss: 2.312\n",
      "[26] loss: 2.312\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.312\n",
      "[29] loss: 2.312\n",
      "[30] loss: 2.312\n",
      "[31] loss: 2.312\n",
      "[32] loss: 2.312\n",
      "[33] loss: 2.312\n",
      "[34] loss: 2.312\n",
      "[35] loss: 2.312\n",
      "[36] loss: 2.312\n",
      "[37] loss: 2.312\n",
      "[38] loss: 2.312\n",
      "[39] loss: 2.312\n",
      "[40] loss: 2.312\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 2.013\n",
      "[2] loss: 1.872\n",
      "[3] loss: 1.805\n",
      "[4] loss: 1.761\n",
      "[5] loss: 1.731\n",
      "[6] loss: 1.703\n",
      "[7] loss: 1.669\n",
      "[8] loss: 1.648\n",
      "[9] loss: 1.618\n",
      "[10] loss: 1.603\n",
      "[11] loss: 1.589\n",
      "[12] loss: 1.559\n",
      "[13] loss: 1.553\n",
      "[14] loss: 1.544\n",
      "[15] loss: 1.525\n",
      "[16] loss: 1.505\n",
      "[17] loss: 1.502\n",
      "[18] loss: 1.491\n",
      "[19] loss: 1.468\n",
      "[20] loss: 1.455\n",
      "[21] loss: 1.450\n",
      "[22] loss: 1.437\n",
      "[23] loss: 1.424\n",
      "[24] loss: 1.411\n",
      "[25] loss: 1.403\n",
      "[26] loss: 1.397\n",
      "[27] loss: 1.388\n",
      "[28] loss: 1.371\n",
      "[29] loss: 1.358\n",
      "[30] loss: 1.356\n",
      "[31] loss: 1.353\n",
      "[32] loss: 1.331\n",
      "[33] loss: 1.325\n",
      "[34] loss: 1.321\n",
      "[35] loss: 1.317\n",
      "[36] loss: 1.293\n",
      "[37] loss: 1.297\n",
      "[38] loss: 1.284\n",
      "[39] loss: 1.280\n",
      "[40] loss: 1.268\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 2.017\n",
      "[2] loss: 1.884\n",
      "[3] loss: 1.812\n",
      "[4] loss: 1.768\n",
      "[5] loss: 1.726\n",
      "[6] loss: 1.684\n",
      "[7] loss: 1.657\n",
      "[8] loss: 1.629\n",
      "[9] loss: 1.605\n",
      "[10] loss: 1.574\n",
      "[11] loss: 1.556\n",
      "[12] loss: 1.525\n",
      "[13] loss: 1.526\n",
      "[14] loss: 1.510\n",
      "[15] loss: 1.487\n",
      "[16] loss: 1.466\n",
      "[17] loss: 1.463\n",
      "[18] loss: 1.457\n",
      "[19] loss: 1.436\n",
      "[20] loss: 1.425\n",
      "[21] loss: 1.420\n",
      "[22] loss: 1.411\n",
      "[23] loss: 1.390\n",
      "[24] loss: 1.385\n",
      "[25] loss: 1.375\n",
      "[26] loss: 1.368\n",
      "[27] loss: 1.364\n",
      "[28] loss: 1.357\n",
      "[29] loss: 1.345\n",
      "[30] loss: 1.332\n",
      "[31] loss: 1.336\n",
      "[32] loss: 1.321\n",
      "[33] loss: 1.318\n",
      "[34] loss: 1.305\n",
      "[35] loss: 1.299\n",
      "[36] loss: 1.288\n",
      "[37] loss: 1.286\n",
      "[38] loss: 1.275\n",
      "[39] loss: 1.275\n",
      "[40] loss: 1.256\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 2.017\n",
      "[2] loss: 1.868\n",
      "[3] loss: 1.802\n",
      "[4] loss: 1.741\n",
      "[5] loss: 1.697\n",
      "[6] loss: 1.657\n",
      "[7] loss: 1.643\n",
      "[8] loss: 1.611\n",
      "[9] loss: 1.577\n",
      "[10] loss: 1.562\n",
      "[11] loss: 1.528\n",
      "[12] loss: 1.513\n",
      "[13] loss: 1.485\n",
      "[14] loss: 1.470\n",
      "[15] loss: 1.438\n",
      "[16] loss: 1.415\n",
      "[17] loss: 1.382\n",
      "[18] loss: 1.366\n",
      "[19] loss: 1.346\n",
      "[20] loss: 1.327\n",
      "[21] loss: 1.318\n",
      "[22] loss: 1.297\n",
      "[23] loss: 1.273\n",
      "[24] loss: 1.262\n",
      "[25] loss: 1.252\n",
      "[26] loss: 1.243\n",
      "[27] loss: 1.235\n",
      "[28] loss: 1.228\n",
      "[29] loss: 1.210\n",
      "[30] loss: 1.217\n",
      "[31] loss: 1.218\n",
      "[32] loss: 1.213\n",
      "[33] loss: 1.218\n",
      "[34] loss: 1.215\n",
      "[35] loss: 1.220\n",
      "[36] loss: 1.231\n",
      "[37] loss: 1.236\n",
      "[38] loss: 1.232\n",
      "[39] loss: 1.247\n",
      "[40] loss: 1.260\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 2.009\n",
      "[2] loss: 1.866\n",
      "[3] loss: 1.805\n",
      "[4] loss: 1.772\n",
      "[5] loss: 1.753\n",
      "[6] loss: 1.710\n",
      "[7] loss: 1.682\n",
      "[8] loss: 1.665\n",
      "[9] loss: 1.646\n",
      "[10] loss: 1.620\n",
      "[11] loss: 1.604\n",
      "[12] loss: 1.575\n",
      "[13] loss: 1.569\n",
      "[14] loss: 1.560\n",
      "[15] loss: 1.541\n",
      "[16] loss: 1.524\n",
      "[17] loss: 1.513\n",
      "[18] loss: 1.502\n",
      "[19] loss: 1.484\n",
      "[20] loss: 1.488\n",
      "[21] loss: 1.476\n",
      "[22] loss: 1.467\n",
      "[23] loss: 1.456\n",
      "[24] loss: 1.450\n",
      "[25] loss: 1.437\n",
      "[26] loss: 1.436\n",
      "[27] loss: 1.423\n",
      "[28] loss: 1.430\n",
      "[29] loss: 1.403\n",
      "[30] loss: 1.411\n",
      "[31] loss: 1.389\n",
      "[32] loss: 1.394\n",
      "[33] loss: 1.383\n",
      "[34] loss: 1.388\n",
      "[35] loss: 1.364\n",
      "[36] loss: 1.352\n",
      "[37] loss: 1.359\n",
      "[38] loss: 1.343\n",
      "[39] loss: 1.331\n",
      "[40] loss: 1.340\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIW6iIhIQCjURUREAkKhLiIiEhD/D65NePCrOfRtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92784c2ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 150\n",
    "epochs = 40\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.0001\n",
    "syn_lr = 0.001\n",
    "\n",
    "run_cifar_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
