{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))\n",
    "\n",
    "# for i, data in enumerate(trainloader, 0):\n",
    "#     print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return  o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * ((Euv - Eu*Ev))\n",
    "        self.beta = (1-eta)*self.beta + eta * (Ev)\n",
    "        \n",
    "        self.eta = eta * 0.996\n",
    "        \n",
    "        \n",
    "class inc_BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(inc_BN, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return  o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * ((Euu - Eu**2))\n",
    "        self.beta = (1-eta)*self.beta + eta * (Eu)\n",
    "        \n",
    "        self.eta = eta * 0.996\n",
    "        \n",
    "class og_IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(og_IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = self.alpha*x + self.beta\n",
    "        \n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return  o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (1/self.alpha -2*Euv)\n",
    "        self.beta = (1-eta)*self.beta + eta * -2*(Ev)\n",
    "        \n",
    "        self.eta = eta * 0.996\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_deep_model(network, optimization, seed):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(80):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep Standard Net\n",
      "[1,   100] loss: 1.770\n",
      "[1,   200] loss: 0.847\n",
      "[1,   300] loss: 0.530\n",
      "[2,   100] loss: 0.419\n",
      "[2,   200] loss: 0.366\n",
      "[2,   300] loss: 0.344\n",
      "[3,   100] loss: 0.288\n",
      "[3,   200] loss: 0.252\n",
      "[3,   300] loss: 0.233\n",
      "[4,   100] loss: 0.194\n",
      "[4,   200] loss: 0.188\n",
      "[4,   300] loss: 0.190\n",
      "[5,   100] loss: 0.158\n",
      "[5,   200] loss: 0.152\n",
      "[5,   300] loss: 0.157\n",
      "[6,   100] loss: 0.129\n",
      "[6,   200] loss: 0.144\n",
      "[6,   300] loss: 0.131\n",
      "[7,   100] loss: 0.116\n",
      "[7,   200] loss: 0.115\n",
      "[7,   300] loss: 0.126\n",
      "[8,   100] loss: 0.105\n",
      "[8,   200] loss: 0.104\n",
      "[8,   300] loss: 0.105\n",
      "[9,   100] loss: 0.094\n",
      "[9,   200] loss: 0.102\n",
      "[9,   300] loss: 0.093\n",
      "[10,   100] loss: 0.083\n",
      "[10,   200] loss: 0.085\n",
      "[10,   300] loss: 0.097\n",
      "[11,   100] loss: 0.085\n",
      "[11,   200] loss: 0.077\n",
      "[11,   300] loss: 0.083\n",
      "[12,   100] loss: 0.075\n",
      "[12,   200] loss: 0.078\n",
      "[12,   300] loss: 0.070\n",
      "[13,   100] loss: 0.066\n",
      "[13,   200] loss: 0.069\n",
      "[13,   300] loss: 0.069\n",
      "[14,   100] loss: 0.066\n",
      "[14,   200] loss: 0.065\n",
      "[14,   300] loss: 0.065\n",
      "[15,   100] loss: 0.054\n",
      "[15,   200] loss: 0.059\n",
      "[15,   300] loss: 0.070\n",
      "[16,   100] loss: 0.054\n",
      "[16,   200] loss: 0.059\n",
      "[16,   300] loss: 0.063\n",
      "[17,   100] loss: 0.052\n",
      "[17,   200] loss: 0.057\n",
      "[17,   300] loss: 0.058\n",
      "[18,   100] loss: 0.044\n",
      "[18,   200] loss: 0.051\n",
      "[18,   300] loss: 0.054\n",
      "[19,   100] loss: 0.045\n",
      "[19,   200] loss: 0.046\n",
      "[19,   300] loss: 0.048\n",
      "[20,   100] loss: 0.045\n",
      "[20,   200] loss: 0.051\n",
      "[20,   300] loss: 0.053\n",
      "[21,   100] loss: 0.040\n",
      "[21,   200] loss: 0.045\n",
      "[21,   300] loss: 0.043\n",
      "[22,   100] loss: 0.039\n",
      "[22,   200] loss: 0.046\n",
      "[22,   300] loss: 0.046\n",
      "[23,   100] loss: 0.037\n",
      "[23,   200] loss: 0.037\n",
      "[23,   300] loss: 0.038\n",
      "[24,   100] loss: 0.036\n",
      "[24,   200] loss: 0.031\n",
      "[24,   300] loss: 0.045\n",
      "[25,   100] loss: 0.035\n",
      "[25,   200] loss: 0.036\n",
      "[25,   300] loss: 0.041\n",
      "[26,   100] loss: 0.034\n",
      "[26,   200] loss: 0.028\n",
      "[26,   300] loss: 0.038\n",
      "[27,   100] loss: 0.030\n",
      "[27,   200] loss: 0.036\n",
      "[27,   300] loss: 0.034\n",
      "[28,   100] loss: 0.034\n",
      "[28,   200] loss: 0.035\n",
      "[28,   300] loss: 0.038\n",
      "[29,   100] loss: 0.036\n",
      "[29,   200] loss: 0.030\n",
      "[29,   300] loss: 0.035\n",
      "[30,   100] loss: 0.029\n",
      "[30,   200] loss: 0.031\n",
      "[30,   300] loss: 0.030\n",
      "[31,   100] loss: 0.030\n",
      "[31,   200] loss: 0.019\n",
      "[31,   300] loss: 0.031\n",
      "[32,   100] loss: 0.028\n",
      "[32,   200] loss: 0.030\n",
      "[32,   300] loss: 0.036\n",
      "[33,   100] loss: 0.027\n",
      "[33,   200] loss: 0.020\n",
      "[33,   300] loss: 0.028\n",
      "[34,   100] loss: 0.023\n",
      "[34,   200] loss: 0.024\n",
      "[34,   300] loss: 0.027\n",
      "[35,   100] loss: 0.024\n",
      "[35,   200] loss: 0.023\n",
      "[35,   300] loss: 0.031\n",
      "[36,   100] loss: 0.026\n",
      "[36,   200] loss: 0.019\n",
      "[36,   300] loss: 0.030\n",
      "[37,   100] loss: 0.025\n",
      "[37,   200] loss: 0.026\n",
      "[37,   300] loss: 0.029\n",
      "[38,   100] loss: 0.027\n",
      "[38,   200] loss: 0.028\n",
      "[38,   300] loss: 0.028\n",
      "[39,   100] loss: 0.023\n",
      "[39,   200] loss: 0.017\n",
      "[39,   300] loss: 0.023\n",
      "[40,   100] loss: 0.021\n",
      "[40,   200] loss: 0.025\n",
      "[40,   300] loss: 0.019\n",
      "[41,   100] loss: 0.016\n",
      "[41,   200] loss: 0.024\n",
      "[41,   300] loss: 0.019\n",
      "[42,   100] loss: 0.021\n",
      "[42,   200] loss: 0.019\n",
      "[42,   300] loss: 0.024\n",
      "[43,   100] loss: 0.023\n",
      "[43,   200] loss: 0.029\n",
      "[43,   300] loss: 0.019\n",
      "[44,   100] loss: 0.016\n",
      "[44,   200] loss: 0.022\n",
      "[44,   300] loss: 0.025\n",
      "[45,   100] loss: 0.018\n",
      "[45,   200] loss: 0.018\n",
      "[45,   300] loss: 0.021\n",
      "[46,   100] loss: 0.017\n",
      "[46,   200] loss: 0.016\n",
      "[46,   300] loss: 0.018\n",
      "[47,   100] loss: 0.020\n",
      "[47,   200] loss: 0.019\n",
      "[47,   300] loss: 0.022\n",
      "[48,   100] loss: 0.013\n",
      "[48,   200] loss: 0.015\n",
      "[48,   300] loss: 0.024\n",
      "[49,   100] loss: 0.015\n",
      "[49,   200] loss: 0.019\n",
      "[49,   300] loss: 0.022\n",
      "[50,   100] loss: 0.018\n",
      "[50,   200] loss: 0.017\n",
      "[50,   300] loss: 0.023\n",
      "[51,   100] loss: 0.017\n",
      "[51,   200] loss: 0.014\n",
      "[51,   300] loss: 0.013\n",
      "[52,   100] loss: 0.013\n",
      "[52,   200] loss: 0.018\n",
      "[52,   300] loss: 0.024\n",
      "[53,   100] loss: 0.020\n",
      "[53,   200] loss: 0.016\n",
      "[53,   300] loss: 0.022\n",
      "[54,   100] loss: 0.019\n",
      "[54,   200] loss: 0.016\n",
      "[54,   300] loss: 0.022\n",
      "[55,   100] loss: 0.013\n",
      "[55,   200] loss: 0.019\n",
      "[55,   300] loss: 0.017\n",
      "[56,   100] loss: 0.011\n",
      "[56,   200] loss: 0.010\n",
      "[56,   300] loss: 0.013\n",
      "[57,   100] loss: 0.017\n",
      "[57,   200] loss: 0.019\n",
      "[57,   300] loss: 0.018\n",
      "[58,   100] loss: 0.022\n",
      "[58,   200] loss: 0.016\n",
      "[58,   300] loss: 0.017\n",
      "[59,   100] loss: 0.016\n",
      "[59,   200] loss: 0.015\n",
      "[59,   300] loss: 0.009\n",
      "[60,   100] loss: 0.014\n",
      "[60,   200] loss: 0.014\n",
      "[60,   300] loss: 0.015\n",
      "[61,   100] loss: 0.017\n",
      "[61,   200] loss: 0.010\n",
      "[61,   300] loss: 0.015\n",
      "[62,   100] loss: 0.014\n",
      "[62,   200] loss: 0.013\n",
      "[62,   300] loss: 0.020\n",
      "[63,   100] loss: 0.013\n",
      "[63,   200] loss: 0.019\n",
      "[63,   300] loss: 0.020\n",
      "[64,   100] loss: 0.013\n",
      "[64,   200] loss: 0.009\n",
      "[64,   300] loss: 0.013\n",
      "[65,   100] loss: 0.012\n",
      "[65,   200] loss: 0.016\n",
      "[65,   300] loss: 0.016\n",
      "[66,   100] loss: 0.010\n",
      "[66,   200] loss: 0.012\n",
      "[66,   300] loss: 0.013\n",
      "[67,   100] loss: 0.017\n",
      "[67,   200] loss: 0.010\n",
      "[67,   300] loss: 0.015\n",
      "[68,   100] loss: 0.012\n",
      "[68,   200] loss: 0.016\n",
      "[68,   300] loss: 0.016\n",
      "[69,   100] loss: 0.012\n",
      "[69,   200] loss: 0.011\n",
      "[69,   300] loss: 0.010\n",
      "[70,   100] loss: 0.008\n",
      "[70,   200] loss: 0.011\n",
      "[70,   300] loss: 0.016\n",
      "[71,   100] loss: 0.022\n",
      "[71,   200] loss: 0.012\n",
      "[71,   300] loss: 0.020\n",
      "[72,   100] loss: 0.013\n",
      "[72,   200] loss: 0.010\n",
      "[72,   300] loss: 0.009\n",
      "[73,   100] loss: 0.008\n",
      "[73,   200] loss: 0.013\n",
      "[73,   300] loss: 0.014\n",
      "[74,   100] loss: 0.011\n",
      "[74,   200] loss: 0.014\n",
      "[74,   300] loss: 0.017\n",
      "[75,   100] loss: 0.011\n",
      "[75,   200] loss: 0.011\n",
      "[75,   300] loss: 0.010\n",
      "[76,   100] loss: 0.007\n",
      "[76,   200] loss: 0.010\n",
      "[76,   300] loss: 0.012\n",
      "[77,   100] loss: 0.019\n",
      "[77,   200] loss: 0.012\n",
      "[77,   300] loss: 0.015\n",
      "[78,   100] loss: 0.012\n",
      "[78,   200] loss: 0.017\n",
      "[78,   300] loss: 0.014\n",
      "[79,   100] loss: 0.008\n",
      "[79,   200] loss: 0.009\n",
      "[79,   300] loss: 0.012\n",
      "[80,   100] loss: 0.010\n",
      "[80,   200] loss: 0.009\n",
      "[80,   300] loss: 0.013\n",
      "Finished training!\n",
      "\n",
      "IP with covariance and median\n",
      "[1,   100] loss: 0.941\n",
      "[1,   200] loss: 0.332\n",
      "[1,   300] loss: 0.265\n",
      "[2,   100] loss: 0.204\n",
      "[2,   200] loss: 0.185\n",
      "[2,   300] loss: 0.180\n",
      "[3,   100] loss: 0.148\n",
      "[3,   200] loss: 0.146\n",
      "[3,   300] loss: 0.140\n",
      "[4,   100] loss: 0.115\n",
      "[4,   200] loss: 0.124\n",
      "[4,   300] loss: 0.129\n",
      "[5,   100] loss: 0.108\n",
      "[5,   200] loss: 0.102\n",
      "[5,   300] loss: 0.109\n",
      "[6,   100] loss: 0.090\n",
      "[6,   200] loss: 0.107\n",
      "[6,   300] loss: 0.101\n",
      "[7,   100] loss: 0.081\n",
      "[7,   200] loss: 0.085\n",
      "[7,   300] loss: 0.097\n",
      "[8,   100] loss: 0.076\n",
      "[8,   200] loss: 0.082\n",
      "[8,   300] loss: 0.082\n",
      "[9,   100] loss: 0.064\n",
      "[9,   200] loss: 0.072\n",
      "[9,   300] loss: 0.085\n",
      "[10,   100] loss: 0.061\n",
      "[10,   200] loss: 0.065\n",
      "[10,   300] loss: 0.077\n",
      "[11,   100] loss: 0.065\n",
      "[11,   200] loss: 0.060\n",
      "[11,   300] loss: 0.070\n",
      "[12,   100] loss: 0.051\n",
      "[12,   200] loss: 0.059\n",
      "[12,   300] loss: 0.063\n",
      "[13,   100] loss: 0.053\n",
      "[13,   200] loss: 0.056\n",
      "[13,   300] loss: 0.061\n",
      "[14,   100] loss: 0.054\n",
      "[14,   200] loss: 0.050\n",
      "[14,   300] loss: 0.053\n",
      "[15,   100] loss: 0.052\n",
      "[15,   200] loss: 0.061\n",
      "[15,   300] loss: 0.057\n",
      "[16,   100] loss: 0.047\n",
      "[16,   200] loss: 0.046\n",
      "[16,   300] loss: 0.052\n",
      "[17,   100] loss: 0.045\n",
      "[17,   200] loss: 0.046\n",
      "[17,   300] loss: 0.060\n",
      "[18,   100] loss: 0.040\n",
      "[18,   200] loss: 0.050\n",
      "[18,   300] loss: 0.056\n",
      "[19,   100] loss: 0.045\n",
      "[19,   200] loss: 0.043\n",
      "[19,   300] loss: 0.041\n",
      "[20,   100] loss: 0.033\n",
      "[20,   200] loss: 0.044\n",
      "[20,   300] loss: 0.044\n",
      "[21,   100] loss: 0.030\n",
      "[21,   200] loss: 0.041\n",
      "[21,   300] loss: 0.047\n",
      "[22,   100] loss: 0.038\n",
      "[22,   200] loss: 0.042\n",
      "[22,   300] loss: 0.044\n",
      "[23,   100] loss: 0.035\n",
      "[23,   200] loss: 0.038\n",
      "[23,   300] loss: 0.038\n",
      "[24,   100] loss: 0.032\n",
      "[24,   200] loss: 0.038\n",
      "[24,   300] loss: 0.041\n",
      "[25,   100] loss: 0.028\n",
      "[25,   200] loss: 0.035\n",
      "[25,   300] loss: 0.038\n",
      "[26,   100] loss: 0.035\n",
      "[26,   200] loss: 0.034\n",
      "[26,   300] loss: 0.035\n",
      "[27,   100] loss: 0.029\n",
      "[27,   200] loss: 0.033\n",
      "[27,   300] loss: 0.032\n",
      "[28,   100] loss: 0.036\n",
      "[28,   200] loss: 0.032\n",
      "[28,   300] loss: 0.043\n",
      "[29,   100] loss: 0.034\n",
      "[29,   200] loss: 0.026\n",
      "[29,   300] loss: 0.039\n",
      "[30,   100] loss: 0.028\n",
      "[30,   200] loss: 0.027\n",
      "[30,   300] loss: 0.034\n",
      "[31,   100] loss: 0.027\n",
      "[31,   200] loss: 0.027\n",
      "[31,   300] loss: 0.030\n",
      "[32,   100] loss: 0.021\n",
      "[32,   200] loss: 0.027\n",
      "[32,   300] loss: 0.035\n",
      "[33,   100] loss: 0.031\n",
      "[33,   200] loss: 0.025\n",
      "[33,   300] loss: 0.029\n",
      "[34,   100] loss: 0.020\n",
      "[34,   200] loss: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34,   300] loss: 0.035\n",
      "[35,   100] loss: 0.027\n",
      "[35,   200] loss: 0.024\n",
      "[35,   300] loss: 0.028\n",
      "[36,   100] loss: 0.021\n",
      "[36,   200] loss: 0.020\n",
      "[36,   300] loss: 0.036\n",
      "[37,   100] loss: 0.027\n",
      "[37,   200] loss: 0.026\n",
      "[37,   300] loss: 0.036\n",
      "[38,   100] loss: 0.028\n",
      "[38,   200] loss: 0.031\n",
      "[38,   300] loss: 0.028\n",
      "[39,   100] loss: 0.019\n",
      "[39,   200] loss: 0.020\n",
      "[39,   300] loss: 0.027\n",
      "[40,   100] loss: 0.022\n",
      "[40,   200] loss: 0.032\n",
      "[40,   300] loss: 0.031\n",
      "[41,   100] loss: 0.017\n",
      "[41,   200] loss: 0.022\n",
      "[41,   300] loss: 0.022\n",
      "[42,   100] loss: 0.026\n",
      "[42,   200] loss: 0.026\n",
      "[42,   300] loss: 0.017\n",
      "[43,   100] loss: 0.016\n",
      "[43,   200] loss: 0.019\n",
      "[43,   300] loss: 0.032\n",
      "[44,   100] loss: 0.017\n",
      "[44,   200] loss: 0.020\n",
      "[44,   300] loss: 0.021\n",
      "[45,   100] loss: 0.017\n",
      "[45,   200] loss: 0.015\n",
      "[45,   300] loss: 0.022\n",
      "[46,   100] loss: 0.024\n",
      "[46,   200] loss: 0.023\n",
      "[46,   300] loss: 0.019\n",
      "[47,   100] loss: 0.023\n",
      "[47,   200] loss: 0.020\n",
      "[47,   300] loss: 0.024\n",
      "[48,   100] loss: 0.024\n",
      "[48,   200] loss: 0.020\n",
      "[48,   300] loss: 0.023\n",
      "[49,   100] loss: 0.015\n",
      "[49,   200] loss: 0.022\n",
      "[49,   300] loss: 0.015\n",
      "[50,   100] loss: 0.016\n",
      "[50,   200] loss: 0.017\n",
      "[50,   300] loss: 0.018\n",
      "[51,   100] loss: 0.022\n",
      "[51,   200] loss: 0.021\n",
      "[51,   300] loss: 0.015\n",
      "[52,   100] loss: 0.008\n",
      "[52,   200] loss: 0.014\n",
      "[52,   300] loss: 0.016\n",
      "[53,   100] loss: 0.011\n",
      "[53,   200] loss: 0.028\n",
      "[53,   300] loss: 0.022\n",
      "[54,   100] loss: 0.017\n",
      "[54,   200] loss: 0.016\n",
      "[54,   300] loss: 0.018\n",
      "[55,   100] loss: 0.021\n",
      "[55,   200] loss: 0.017\n",
      "[55,   300] loss: 0.017\n",
      "[56,   100] loss: 0.015\n",
      "[56,   200] loss: 0.019\n",
      "[56,   300] loss: 0.022\n",
      "[57,   100] loss: 0.021\n",
      "[57,   200] loss: 0.022\n",
      "[57,   300] loss: 0.016\n",
      "[58,   100] loss: 0.024\n",
      "[58,   200] loss: 0.028\n",
      "[58,   300] loss: 0.021\n",
      "[59,   100] loss: 0.010\n",
      "[59,   200] loss: 0.011\n",
      "[59,   300] loss: 0.010\n",
      "[60,   100] loss: 0.009\n",
      "[60,   200] loss: 0.011\n",
      "[60,   300] loss: 0.015\n",
      "[61,   100] loss: 0.011\n",
      "[61,   200] loss: 0.018\n",
      "[61,   300] loss: 0.017\n",
      "[62,   100] loss: 0.016\n",
      "[62,   200] loss: 0.016\n",
      "[62,   300] loss: 0.014\n",
      "[63,   100] loss: 0.012\n",
      "[63,   200] loss: 0.011\n",
      "[63,   300] loss: 0.017\n",
      "[64,   100] loss: 0.021\n",
      "[64,   200] loss: 0.015\n",
      "[64,   300] loss: 0.019\n",
      "[65,   100] loss: 0.011\n",
      "[65,   200] loss: 0.018\n",
      "[65,   300] loss: 0.017\n",
      "[66,   100] loss: 0.013\n",
      "[66,   200] loss: 0.016\n",
      "[66,   300] loss: 0.015\n",
      "[67,   100] loss: 0.010\n",
      "[67,   200] loss: 0.015\n",
      "[67,   300] loss: 0.020\n",
      "[68,   100] loss: 0.012\n",
      "[68,   200] loss: 0.018\n",
      "[68,   300] loss: 0.014\n",
      "[69,   100] loss: 0.011\n",
      "[69,   200] loss: 0.011\n",
      "[69,   300] loss: 0.013\n",
      "[70,   100] loss: 0.009\n",
      "[70,   200] loss: 0.013\n",
      "[70,   300] loss: 0.014\n",
      "[71,   100] loss: 0.014\n",
      "[71,   200] loss: 0.020\n",
      "[71,   300] loss: 0.020\n",
      "[72,   100] loss: 0.014\n",
      "[72,   200] loss: 0.011\n",
      "[72,   300] loss: 0.013\n",
      "[73,   100] loss: 0.011\n",
      "[73,   200] loss: 0.010\n",
      "[73,   300] loss: 0.015\n",
      "[74,   100] loss: 0.014\n",
      "[74,   200] loss: 0.010\n",
      "[74,   300] loss: 0.009\n",
      "[75,   100] loss: 0.015\n",
      "[75,   200] loss: 0.014\n",
      "[75,   300] loss: 0.015\n",
      "[76,   100] loss: 0.016\n",
      "[76,   200] loss: 0.013\n",
      "[76,   300] loss: 0.019\n",
      "[77,   100] loss: 0.016\n",
      "[77,   200] loss: 0.011\n",
      "[77,   300] loss: 0.009\n",
      "[78,   100] loss: 0.010\n",
      "[78,   200] loss: 0.010\n",
      "[78,   300] loss: 0.017\n",
      "[79,   100] loss: 0.009\n",
      "[79,   200] loss: 0.010\n",
      "[79,   300] loss: 0.010\n",
      "[80,   100] loss: 0.007\n",
      "[80,   200] loss: 0.008\n",
      "[80,   300] loss: 0.011\n",
      "Finished training!\n",
      "\n",
      "OG IP\n",
      "[1,   100] loss: 1.922\n",
      "[1,   200] loss: 1.430\n",
      "[1,   300] loss: 1.269\n",
      "[2,   100] loss: 1.193\n",
      "[2,   200] loss: 1.114\n",
      "[2,   300] loss: 1.048\n",
      "[3,   100] loss: 0.976\n",
      "[3,   200] loss: 0.932\n",
      "[3,   300] loss: 0.867\n",
      "[4,   100] loss: 0.821\n",
      "[4,   200] loss: 0.775\n",
      "[4,   300] loss: 0.740\n",
      "[5,   100] loss: 0.699\n",
      "[5,   200] loss: 0.656\n",
      "[5,   300] loss: 0.620\n",
      "[6,   100] loss: 0.550\n",
      "[6,   200] loss: 0.537\n",
      "[6,   300] loss: 0.505\n",
      "[7,   100] loss: 0.458\n",
      "[7,   200] loss: 0.442\n",
      "[7,   300] loss: 0.431\n",
      "[8,   100] loss: 0.388\n",
      "[8,   200] loss: 0.375\n",
      "[8,   300] loss: 0.379\n",
      "[9,   100] loss: 0.342\n",
      "[9,   200] loss: 0.339\n",
      "[9,   300] loss: 0.310\n",
      "[10,   100] loss: 0.308\n",
      "[10,   200] loss: 0.301\n",
      "[10,   300] loss: 0.290\n",
      "[11,   100] loss: 0.283\n",
      "[11,   200] loss: 0.273\n",
      "[11,   300] loss: 0.266\n",
      "[12,   100] loss: 0.260\n",
      "[12,   200] loss: 0.254\n",
      "[12,   300] loss: 0.238\n",
      "[13,   100] loss: 0.234\n",
      "[13,   200] loss: 0.249\n",
      "[13,   300] loss: 0.235\n",
      "[14,   100] loss: 0.227\n",
      "[14,   200] loss: 0.225\n",
      "[14,   300] loss: 0.214\n",
      "[15,   100] loss: 0.204\n",
      "[15,   200] loss: 0.207\n",
      "[15,   300] loss: 0.210\n",
      "[16,   100] loss: 0.187\n",
      "[16,   200] loss: 0.198\n",
      "[16,   300] loss: 0.204\n",
      "[17,   100] loss: 0.186\n",
      "[17,   200] loss: 0.187\n",
      "[17,   300] loss: 0.182\n",
      "[18,   100] loss: 0.172\n",
      "[18,   200] loss: 0.175\n",
      "[18,   300] loss: 0.179\n",
      "[19,   100] loss: 0.171\n",
      "[19,   200] loss: 0.165\n",
      "[19,   300] loss: 0.164\n",
      "[20,   100] loss: 0.155\n",
      "[20,   200] loss: 0.161\n",
      "[20,   300] loss: 0.156\n",
      "[21,   100] loss: 0.149\n",
      "[21,   200] loss: 0.158\n",
      "[21,   300] loss: 0.147\n",
      "[22,   100] loss: 0.143\n",
      "[22,   200] loss: 0.143\n",
      "[22,   300] loss: 0.142\n",
      "[23,   100] loss: 0.141\n",
      "[23,   200] loss: 0.132\n",
      "[23,   300] loss: 0.124\n",
      "[24,   100] loss: 0.135\n",
      "[24,   200] loss: 0.130\n",
      "[24,   300] loss: 0.136\n",
      "[25,   100] loss: 0.115\n",
      "[25,   200] loss: 0.131\n",
      "[25,   300] loss: 0.120\n",
      "[26,   100] loss: 0.114\n",
      "[26,   200] loss: 0.113\n",
      "[26,   300] loss: 0.125\n",
      "[27,   100] loss: 0.110\n",
      "[27,   200] loss: 0.116\n",
      "[27,   300] loss: 0.111\n",
      "[28,   100] loss: 0.107\n",
      "[28,   200] loss: 0.109\n",
      "[28,   300] loss: 0.108\n",
      "[29,   100] loss: 0.096\n",
      "[29,   200] loss: 0.102\n",
      "[29,   300] loss: 0.104\n",
      "[30,   100] loss: 0.092\n",
      "[30,   200] loss: 0.100\n",
      "[30,   300] loss: 0.098\n",
      "[31,   100] loss: 0.091\n",
      "[31,   200] loss: 0.091\n",
      "[31,   300] loss: 0.099\n",
      "[32,   100] loss: 0.085\n",
      "[32,   200] loss: 0.089\n",
      "[32,   300] loss: 0.090\n",
      "[33,   100] loss: 0.082\n",
      "[33,   200] loss: 0.088\n",
      "[33,   300] loss: 0.085\n",
      "[34,   100] loss: 0.074\n",
      "[34,   200] loss: 0.080\n",
      "[34,   300] loss: 0.083\n",
      "[35,   100] loss: 0.078\n",
      "[35,   200] loss: 0.077\n",
      "[35,   300] loss: 0.078\n",
      "[36,   100] loss: 0.069\n",
      "[36,   200] loss: 0.070\n",
      "[36,   300] loss: 0.078\n",
      "[37,   100] loss: 0.069\n",
      "[37,   200] loss: 0.066\n",
      "[37,   300] loss: 0.081\n",
      "[38,   100] loss: 0.066\n",
      "[38,   200] loss: 0.063\n",
      "[38,   300] loss: 0.071\n",
      "[39,   100] loss: 0.073\n",
      "[39,   200] loss: 0.058\n",
      "[39,   300] loss: 0.067\n",
      "[40,   100] loss: 0.064\n",
      "[40,   200] loss: 0.065\n",
      "[40,   300] loss: 0.060\n",
      "[41,   100] loss: 0.062\n",
      "[41,   200] loss: 0.062\n",
      "[41,   300] loss: 0.060\n",
      "[42,   100] loss: 0.057\n",
      "[42,   200] loss: 0.061\n",
      "[42,   300] loss: 0.055\n",
      "[43,   100] loss: 0.051\n",
      "[43,   200] loss: 0.059\n",
      "[43,   300] loss: 0.062\n",
      "[44,   100] loss: 0.050\n",
      "[44,   200] loss: 0.050\n",
      "[44,   300] loss: 0.061\n",
      "[45,   100] loss: 0.050\n",
      "[45,   200] loss: 0.046\n",
      "[45,   300] loss: 0.052\n",
      "[46,   100] loss: 0.042\n",
      "[46,   200] loss: 0.049\n",
      "[46,   300] loss: 0.056\n",
      "[47,   100] loss: 0.045\n",
      "[47,   200] loss: 0.046\n",
      "[47,   300] loss: 0.048\n",
      "[48,   100] loss: 0.045\n",
      "[48,   200] loss: 0.041\n",
      "[48,   300] loss: 0.059\n",
      "[49,   100] loss: 0.039\n",
      "[49,   200] loss: 0.043\n",
      "[49,   300] loss: 0.045\n",
      "[50,   100] loss: 0.037\n",
      "[50,   200] loss: 0.040\n",
      "[50,   300] loss: 0.048\n",
      "[51,   100] loss: 0.041\n",
      "[51,   200] loss: 0.040\n",
      "[51,   300] loss: 0.040\n",
      "[52,   100] loss: 0.036\n",
      "[52,   200] loss: 0.038\n",
      "[52,   300] loss: 0.045\n",
      "[53,   100] loss: 0.038\n",
      "[53,   200] loss: 0.040\n",
      "[53,   300] loss: 0.041\n",
      "[54,   100] loss: 0.040\n",
      "[54,   200] loss: 0.033\n",
      "[54,   300] loss: 0.042\n",
      "[55,   100] loss: 0.033\n",
      "[55,   200] loss: 0.030\n",
      "[55,   300] loss: 0.038\n",
      "[56,   100] loss: 0.028\n",
      "[56,   200] loss: 0.040\n",
      "[56,   300] loss: 0.038\n",
      "[57,   100] loss: 0.034\n",
      "[57,   200] loss: 0.034\n",
      "[57,   300] loss: 0.034\n",
      "[58,   100] loss: 0.024\n",
      "[58,   200] loss: 0.028\n",
      "[58,   300] loss: 0.034\n",
      "[59,   100] loss: 0.035\n",
      "[59,   200] loss: 0.030\n",
      "[59,   300] loss: 0.033\n",
      "[60,   100] loss: 0.032\n",
      "[60,   200] loss: 0.031\n",
      "[60,   300] loss: 0.036\n",
      "[61,   100] loss: 0.031\n",
      "[61,   200] loss: 0.032\n",
      "[61,   300] loss: 0.031\n",
      "[62,   100] loss: 0.027\n",
      "[62,   200] loss: 0.032\n",
      "[62,   300] loss: 0.029\n",
      "[63,   100] loss: 0.024\n",
      "[63,   200] loss: 0.028\n",
      "[63,   300] loss: 0.035\n",
      "[64,   100] loss: 0.025\n",
      "[64,   200] loss: 0.028\n",
      "[64,   300] loss: 0.031\n",
      "[65,   100] loss: 0.024\n",
      "[65,   200] loss: 0.023\n",
      "[65,   300] loss: 0.029\n",
      "[66,   100] loss: 0.025\n",
      "[66,   200] loss: 0.025\n",
      "[66,   300] loss: 0.026\n",
      "[67,   100] loss: 0.026\n",
      "[67,   200] loss: 0.023\n",
      "[67,   300] loss: 0.026\n",
      "[68,   100] loss: 0.024\n",
      "[68,   200] loss: 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68,   300] loss: 0.030\n",
      "[69,   100] loss: 0.022\n",
      "[69,   200] loss: 0.019\n",
      "[69,   300] loss: 0.032\n",
      "[70,   100] loss: 0.020\n",
      "[70,   200] loss: 0.027\n",
      "[70,   300] loss: 0.027\n",
      "[71,   100] loss: 0.024\n",
      "[71,   200] loss: 0.020\n",
      "[71,   300] loss: 0.023\n",
      "[72,   100] loss: 0.022\n",
      "[72,   200] loss: 0.022\n",
      "[72,   300] loss: 0.026\n",
      "[73,   100] loss: 0.020\n",
      "[73,   200] loss: 0.022\n",
      "[73,   300] loss: 0.022\n",
      "[74,   100] loss: 0.022\n",
      "[74,   200] loss: 0.028\n",
      "[74,   300] loss: 0.021\n",
      "[75,   100] loss: 0.015\n",
      "[75,   200] loss: 0.019\n",
      "[75,   300] loss: 0.027\n",
      "[76,   100] loss: 0.025\n",
      "[76,   200] loss: 0.020\n",
      "[76,   300] loss: 0.020\n",
      "[77,   100] loss: 0.017\n",
      "[77,   200] loss: 0.017\n",
      "[77,   300] loss: 0.023\n",
      "[78,   100] loss: 0.019\n",
      "[78,   200] loss: 0.014\n",
      "[78,   300] loss: 0.023\n",
      "[79,   100] loss: 0.025\n",
      "[79,   200] loss: 0.023\n",
      "[79,   300] loss: 0.020\n",
      "[80,   100] loss: 0.018\n",
      "[80,   200] loss: 0.029\n",
      "[80,   300] loss: 0.021\n",
      "Finished training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 795\n",
    "\n",
    "INPUTSIZE = 28*28\n",
    "NBLAYERS = 9\n",
    "LAYERSIZE = 50\n",
    "\n",
    "int_lr = 0.2\n",
    "syn_lr = 0.001\n",
    "\n",
    "# #Train IP Model\n",
    "# torch.manual_seed(seed)\n",
    "# IPnet = Net()\n",
    "# IPnet = IPnet.to(device)\n",
    "\n",
    "# optimizer1 = optim.Adam(IPnet.parameters(), lr=0.001)\n",
    "# print(\"Training IP Net\")\n",
    "# train_model(IPnet, optimizer1, seed, True)\n",
    "\n",
    "# #Train Standard Model\n",
    "# torch.manual_seed(seed)\n",
    "# net = Net()\n",
    "# net = net.to(device)\n",
    "\n",
    "# optimizer2 = optim.Adam(net.parameters(), lr=0.001)\n",
    "# print(\"Training Standard Net\")\n",
    "# train_model(net, optimizer2, seed, False)\n",
    "\n",
    "\n",
    "\n",
    "#Train Deep Standard Model\n",
    "torch.manual_seed(seed)\n",
    "DSnet = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "DSnet = DSnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(DSnet.parameters(), lr=syn_lr)\n",
    "print(\"Training Deep Standard Net\")\n",
    "standard_losses = train_deep_model(DSnet, optimizer, seed)\n",
    "\n",
    "\n",
    "\n",
    "#Train Deep IP Model\n",
    "torch.manual_seed(seed)\n",
    "DIPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "DIPnet = DIPnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(DIPnet.parameters(), lr=syn_lr)\n",
    "print(\"IP with covariance and median\")\n",
    "ip_losses = train_deep_model(DIPnet, optimizer, seed)\n",
    "\n",
    "\n",
    "\n",
    "#Train Deep OG IP\n",
    "torch.manual_seed(seed)\n",
    "DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "DOGnet = DOGnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "print(\"OG IP\")\n",
    "og_losses = train_deep_model(DOGnet, optimizer, seed)\n",
    "\n",
    "\n",
    "\n",
    "#Train Deep Batch Norm Model\n",
    "# torch.manual_seed(seed)\n",
    "# Varnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "# Varnet = Varnet.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(Varnet.parameters(), lr=syn_lr)\n",
    "# print(\"IP with variance and mean\")\n",
    "# var_losses = train_deep_model(Varnet, optimizer, seed)\n",
    "\n",
    "# for param in IPnet.parameters():\n",
    "#     print(param.data)\n",
    "    \n",
    "# for param in IPnet.parameters():\n",
    "#     print(param.data)\n",
    "\n",
    "# c_net = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAHwCAYAAABjb6hNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARBEARh0CAiVxAEQRAEQRAEQRg0/D/ut1/dXf7xwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37d5d8b630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.ylim([-0.1, 3])\n",
    "plt.title(\"Training losses\")\n",
    "plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "# plt.plot(var_losses[0], var_losses[1], label=\"Var\")\n",
    "plt.plot(og_losses[0], og_losses[1], label=\"OG\")\n",
    "plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the IP network on the 10000 test images: 96.3700 %\n",
      "Accuracy of the Var network on the 10000 test images: 96.2300 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        y9  = DIPnet(images)\n",
    "        _, predicted = torch.max(y9.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the IP network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        y9  = Varnet(images)\n",
    "        _, predicted = torch.max(y9.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the Var network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# val1, ind1 = DIPnet.fc1.weight.max(0)\n",
    "# max_weight = val1.max(0)\n",
    "# print(max_weight)\n",
    "\n",
    "# val1, ind1 = Dnet.fc1.weight.max(0)\n",
    "# max_weight = val1.max(0)\n",
    "# print(max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
