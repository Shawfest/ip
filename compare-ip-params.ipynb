{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "#         self.gain = nn.Parameter(torch.ones(layersize))\n",
    "#         self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*(Euv))\n",
    "        self.beta = (1-eta)*self.beta + eta * (4*Ev*Euv)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "        \n",
    "\n",
    "class IPish(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IPish, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (Euv)\n",
    "        self.beta = (1-eta)*self.beta + eta * (Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "        \n",
    "class inc_BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(inc_BN, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (torch.sqrt((Euu - Eu**2)))\n",
    "        self.beta = (1-eta)*self.beta + eta * (Eu)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "        \n",
    "class og_IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(og_IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = self.alpha*x + self.beta\n",
    "        \n",
    "        return nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = self.alpha + eta * (1/self.alpha -2*Euv)\n",
    "        self.beta = self.beta + eta * -2*(Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o\n",
    "    \n",
    "class CDNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(CDNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(3*32*32, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = DNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = DNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cifar_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    #Train Incremental BN\n",
    "    torch.manual_seed(seed)\n",
    "    BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "    BNnet = BNnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Incremental BN. Run %d\" % (1))\n",
    "    bn_losses = train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "          \n",
    "    #Train Deep OG IP\n",
    "    torch.manual_seed(seed)\n",
    "    DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "    DOGnet = DOGnet.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training Infomax Net. Run %d\" % (1))\n",
    "    og_losses = train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Standard Net. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Incremental BN\n",
    "        torch.manual_seed(seed)\n",
    "        BNnet = CDNet(LAYERSIZE, inc_BN, eta=int_lr)\n",
    "        BNnet = BNnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(BNnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Incremental BN. Run %d\" % (i+2))\n",
    "        bn_losses += train_deep_model(BNnet, optimizer, seed, epochs)\n",
    "              \n",
    "        #Train Deep OG IP\n",
    "        torch.manual_seed(seed)\n",
    "        DOGnet = CDNet(LAYERSIZE, og_IP, eta=int_lr)\n",
    "        DOGnet = DOGnet.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(DOGnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training Infomax Net. Run %d\" % (i+2))\n",
    "        og_losses += train_deep_model(DOGnet, optimizer, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = CDNet(LAYERSIZE, NN, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Standard Net. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    bn_losses = bn_losses/test_runs\n",
    "    og_losses = og_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Standard\")\n",
    "    plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(og_losses[0], og_losses[1], label=\"Infomax\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.609\n",
      "[2] loss: 0.283\n",
      "[3] loss: 0.207\n",
      "[4] loss: 0.165\n",
      "[5] loss: 0.142\n",
      "[6] loss: 0.123\n",
      "[7] loss: 0.110\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.092\n",
      "[10] loss: 0.084\n",
      "[11] loss: 0.078\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.056\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.049\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.048\n",
      "[20] loss: 0.042\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 0.797\n",
      "[2] loss: 0.383\n",
      "[3] loss: 0.275\n",
      "[4] loss: 0.219\n",
      "[5] loss: 0.192\n",
      "[6] loss: 0.165\n",
      "[7] loss: 0.142\n",
      "[8] loss: 0.128\n",
      "[9] loss: 0.115\n",
      "[10] loss: 0.108\n",
      "[11] loss: 0.100\n",
      "[12] loss: 0.091\n",
      "[13] loss: 0.084\n",
      "[14] loss: 0.080\n",
      "[15] loss: 0.072\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.065\n",
      "[18] loss: 0.062\n",
      "[19] loss: 0.059\n",
      "[20] loss: 0.054\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 2.274\n",
      "[2] loss: 2.310\n",
      "[3] loss: 2.310\n",
      "[4] loss: 2.310\n",
      "[5] loss: 2.310\n",
      "[6] loss: 2.310\n",
      "[7] loss: 2.310\n",
      "[8] loss: 2.310\n",
      "[9] loss: 2.310\n",
      "[10] loss: 2.310\n",
      "[11] loss: 2.310\n",
      "[12] loss: 2.310\n",
      "[13] loss: 2.310\n",
      "[14] loss: 2.310\n",
      "[15] loss: 2.310\n",
      "[16] loss: 2.310\n",
      "[17] loss: 2.310\n",
      "[18] loss: 2.310\n",
      "[19] loss: 2.310\n",
      "[20] loss: 2.310\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 1.607\n",
      "[2] loss: 0.993\n",
      "[3] loss: 0.547\n",
      "[4] loss: 0.407\n",
      "[5] loss: 0.216\n",
      "[6] loss: 0.186\n",
      "[7] loss: 0.159\n",
      "[8] loss: 0.148\n",
      "[9] loss: 0.140\n",
      "[10] loss: 0.136\n",
      "[11] loss: 0.134\n",
      "[12] loss: 0.122\n",
      "[13] loss: 0.127\n",
      "[14] loss: 0.133\n",
      "[15] loss: 0.120\n",
      "[16] loss: 0.113\n",
      "[17] loss: 0.118\n",
      "[18] loss: 0.115\n",
      "[19] loss: 0.112\n",
      "[20] loss: 0.111\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.779\n",
      "[2] loss: 0.339\n",
      "[3] loss: 0.243\n",
      "[4] loss: 0.189\n",
      "[5] loss: 0.161\n",
      "[6] loss: 0.136\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.114\n",
      "[9] loss: 0.101\n",
      "[10] loss: 0.094\n",
      "[11] loss: 0.083\n",
      "[12] loss: 0.080\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.064\n",
      "[16] loss: 0.060\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.053\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 0.874\n",
      "[2] loss: 0.430\n",
      "[3] loss: 0.309\n",
      "[4] loss: 0.239\n",
      "[5] loss: 0.200\n",
      "[6] loss: 0.174\n",
      "[7] loss: 0.155\n",
      "[8] loss: 0.134\n",
      "[9] loss: 0.123\n",
      "[10] loss: 0.112\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.094\n",
      "[13] loss: 0.091\n",
      "[14] loss: 0.085\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.075\n",
      "[17] loss: 0.070\n",
      "[18] loss: 0.065\n",
      "[19] loss: 0.061\n",
      "[20] loss: 0.058\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 1.336\n",
      "[2] loss: 0.963\n",
      "[3] loss: 0.970\n",
      "[4] loss: 1.318\n",
      "[5] loss: 1.885\n",
      "[6] loss: 1.797\n",
      "[7] loss: 1.495\n",
      "[8] loss: 1.716\n",
      "[9] loss: 1.876\n",
      "[10] loss: 1.951\n",
      "[11] loss: 2.251\n",
      "[12] loss: 2.282\n",
      "[13] loss: 2.235\n",
      "[14] loss: 2.241\n",
      "[15] loss: 2.164\n",
      "[16] loss: 2.239\n",
      "[17] loss: 2.261\n",
      "[18] loss: 2.316\n",
      "[19] loss: 2.321\n",
      "[20] loss: 2.316\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 1.166\n",
      "[2] loss: 0.390\n",
      "[3] loss: 0.262\n",
      "[4] loss: 0.208\n",
      "[5] loss: 0.182\n",
      "[6] loss: 0.165\n",
      "[7] loss: 0.154\n",
      "[8] loss: 0.142\n",
      "[9] loss: 0.136\n",
      "[10] loss: 0.130\n",
      "[11] loss: 0.129\n",
      "[12] loss: 0.116\n",
      "[13] loss: 0.119\n",
      "[14] loss: 0.118\n",
      "[15] loss: 0.113\n",
      "[16] loss: 0.109\n",
      "[17] loss: 0.111\n",
      "[18] loss: 0.109\n",
      "[19] loss: 0.117\n",
      "[20] loss: 0.112\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.611\n",
      "[2] loss: 0.275\n",
      "[3] loss: 0.207\n",
      "[4] loss: 0.169\n",
      "[5] loss: 0.139\n",
      "[6] loss: 0.122\n",
      "[7] loss: 0.113\n",
      "[8] loss: 0.096\n",
      "[9] loss: 0.091\n",
      "[10] loss: 0.081\n",
      "[11] loss: 0.074\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.066\n",
      "[14] loss: 0.059\n",
      "[15] loss: 0.057\n",
      "[16] loss: 0.053\n",
      "[17] loss: 0.051\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.046\n",
      "[20] loss: 0.043\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 3\n",
      "[1] loss: 0.760\n",
      "[2] loss: 0.366\n",
      "[3] loss: 0.294\n",
      "[4] loss: 0.241\n",
      "[5] loss: 0.207\n",
      "[6] loss: 0.188\n",
      "[7] loss: 0.165\n",
      "[8] loss: 0.147\n",
      "[9] loss: 0.132\n",
      "[10] loss: 0.121\n",
      "[11] loss: 0.107\n",
      "[12] loss: 0.101\n",
      "[13] loss: 0.091\n",
      "[14] loss: 0.083\n",
      "[15] loss: 0.080\n",
      "[16] loss: 0.073\n",
      "[17] loss: 0.070\n",
      "[18] loss: 0.064\n",
      "[19] loss: 0.060\n",
      "[20] loss: 0.055\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 1.330\n",
      "[2] loss: 1.016\n",
      "[3] loss: 1.151\n",
      "[4] loss: 1.190\n",
      "[5] loss: 1.461\n",
      "[6] loss: 2.124\n",
      "[7] loss: 2.137\n",
      "[8] loss: 2.245\n",
      "[9] loss: 2.285\n",
      "[10] loss: 2.308\n",
      "[11] loss: 2.301\n",
      "[12] loss: 2.310\n",
      "[13] loss: 2.310\n",
      "[14] loss: 2.309\n",
      "[15] loss: 2.310\n",
      "[16] loss: 2.283\n",
      "[17] loss: 2.175\n",
      "[18] loss: 1.992\n",
      "[19] loss: 2.130\n",
      "[20] loss: 2.181\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 0.947\n",
      "[2] loss: 0.402\n",
      "[3] loss: 0.273\n",
      "[4] loss: 0.245\n",
      "[5] loss: 0.196\n",
      "[6] loss: 0.181\n",
      "[7] loss: 0.167\n",
      "[8] loss: 0.155\n",
      "[9] loss: 0.149\n",
      "[10] loss: 0.153\n",
      "[11] loss: 0.150\n",
      "[12] loss: 0.140\n",
      "[13] loss: 0.147\n",
      "[14] loss: 0.149\n",
      "[15] loss: 0.129\n",
      "[16] loss: 0.127\n",
      "[17] loss: 0.141\n",
      "[18] loss: 0.137\n",
      "[19] loss: 0.134\n",
      "[20] loss: 0.125\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.675\n",
      "[2] loss: 0.284\n",
      "[3] loss: 0.202\n",
      "[4] loss: 0.161\n",
      "[5] loss: 0.140\n",
      "[6] loss: 0.124\n",
      "[7] loss: 0.108\n",
      "[8] loss: 0.100\n",
      "[9] loss: 0.093\n",
      "[10] loss: 0.086\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.069\n",
      "[14] loss: 0.062\n",
      "[15] loss: 0.054\n",
      "[16] loss: 0.054\n",
      "[17] loss: 0.046\n",
      "[18] loss: 0.048\n",
      "[19] loss: 0.047\n",
      "[20] loss: 0.042\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 0.783\n",
      "[2] loss: 0.379\n",
      "[3] loss: 0.280\n",
      "[4] loss: 0.222\n",
      "[5] loss: 0.184\n",
      "[6] loss: 0.160\n",
      "[7] loss: 0.142\n",
      "[8] loss: 0.124\n",
      "[9] loss: 0.115\n",
      "[10] loss: 0.102\n",
      "[11] loss: 0.097\n",
      "[12] loss: 0.087\n",
      "[13] loss: 0.083\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.071\n",
      "[16] loss: 0.067\n",
      "[17] loss: 0.064\n",
      "[18] loss: 0.058\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.053\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 1.362\n",
      "[2] loss: 0.902\n",
      "[3] loss: 0.802\n",
      "[4] loss: 1.114\n",
      "[5] loss: 1.980\n",
      "[6] loss: 2.031\n",
      "[7] loss: 1.714\n",
      "[8] loss: 1.779\n",
      "[9] loss: 1.873\n",
      "[10] loss: 1.955\n",
      "[11] loss: 2.212\n",
      "[12] loss: 2.184\n",
      "[13] loss: 2.101\n",
      "[14] loss: 2.072\n",
      "[15] loss: 2.250\n",
      "[16] loss: 2.121\n",
      "[17] loss: 2.230\n",
      "[18] loss: 2.211\n",
      "[19] loss: 2.084\n",
      "[20] loss: 2.066\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 0.876\n",
      "[2] loss: 0.410\n",
      "[3] loss: 0.292\n",
      "[4] loss: 0.232\n",
      "[5] loss: 0.211\n",
      "[6] loss: 0.196\n",
      "[7] loss: 0.182\n",
      "[8] loss: 0.172\n",
      "[9] loss: 0.158\n",
      "[10] loss: 0.155\n",
      "[11] loss: 0.146\n",
      "[12] loss: 0.140\n",
      "[13] loss: 0.144\n",
      "[14] loss: 0.154\n",
      "[15] loss: 0.140\n",
      "[16] loss: 0.133\n",
      "[17] loss: 0.140\n",
      "[18] loss: 0.142\n",
      "[19] loss: 0.135\n",
      "[20] loss: 0.133\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 0.684\n",
      "[2] loss: 0.322\n",
      "[3] loss: 0.228\n",
      "[4] loss: 0.178\n",
      "[5] loss: 0.150\n",
      "[6] loss: 0.129\n",
      "[7] loss: 0.112\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.091\n",
      "[10] loss: 0.084\n",
      "[11] loss: 0.078\n",
      "[12] loss: 0.071\n",
      "[13] loss: 0.067\n",
      "[14] loss: 0.061\n",
      "[15] loss: 0.055\n",
      "[16] loss: 0.055\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.049\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.043\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 0.740\n",
      "[2] loss: 0.382\n",
      "[3] loss: 0.281\n",
      "[4] loss: 0.228\n",
      "[5] loss: 0.194\n",
      "[6] loss: 0.168\n",
      "[7] loss: 0.147\n",
      "[8] loss: 0.131\n",
      "[9] loss: 0.118\n",
      "[10] loss: 0.108\n",
      "[11] loss: 0.100\n",
      "[12] loss: 0.095\n",
      "[13] loss: 0.085\n",
      "[14] loss: 0.081\n",
      "[15] loss: 0.072\n",
      "[16] loss: 0.071\n",
      "[17] loss: 0.066\n",
      "[18] loss: 0.062\n",
      "[19] loss: 0.060\n",
      "[20] loss: 0.057\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 1.124\n",
      "[2] loss: 0.877\n",
      "[3] loss: 0.938\n",
      "[4] loss: 1.834\n",
      "[5] loss: 1.972\n",
      "[6] loss: 2.309\n",
      "[7] loss: 2.177\n",
      "[8] loss: 2.181\n",
      "[9] loss: 2.173\n",
      "[10] loss: 2.043\n",
      "[11] loss: 1.887\n",
      "[12] loss: 2.224\n",
      "[13] loss: 2.253\n",
      "[14] loss: 2.123\n",
      "[15] loss: 2.034\n",
      "[16] loss: 2.064\n",
      "[17] loss: 2.268\n",
      "[18] loss: 2.216\n",
      "[19] loss: 2.263\n",
      "[20] loss: 2.285\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 0.654\n",
      "[2] loss: 0.308\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.198\n",
      "[5] loss: 0.186\n",
      "[6] loss: 0.166\n",
      "[7] loss: 0.152\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.137\n",
      "[10] loss: 0.135\n",
      "[11] loss: 0.134\n",
      "[12] loss: 0.135\n",
      "[13] loss: 0.132\n",
      "[14] loss: 0.138\n",
      "[15] loss: 0.123\n",
      "[16] loss: 0.128\n",
      "[17] loss: 0.122\n",
      "[18] loss: 0.127\n",
      "[19] loss: 0.128\n",
      "[20] loss: 0.118\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 0.587\n",
      "[2] loss: 0.263\n",
      "[3] loss: 0.192\n",
      "[4] loss: 0.160\n",
      "[5] loss: 0.135\n",
      "[6] loss: 0.121\n",
      "[7] loss: 0.105\n",
      "[8] loss: 0.094\n",
      "[9] loss: 0.089\n",
      "[10] loss: 0.082\n",
      "[11] loss: 0.073\n",
      "[12] loss: 0.068\n",
      "[13] loss: 0.063\n",
      "[14] loss: 0.060\n",
      "[15] loss: 0.057\n",
      "[16] loss: 0.054\n",
      "[17] loss: 0.051\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.043\n",
      "[20] loss: 0.040\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 0.656\n",
      "[2] loss: 0.334\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.191\n",
      "[5] loss: 0.158\n",
      "[6] loss: 0.141\n",
      "[7] loss: 0.122\n",
      "[8] loss: 0.109\n",
      "[9] loss: 0.100\n",
      "[10] loss: 0.092\n",
      "[11] loss: 0.085\n",
      "[12] loss: 0.078\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] loss: 0.067\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.047\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 0.874\n",
      "[2] loss: 0.642\n",
      "[3] loss: 0.586\n",
      "[4] loss: 0.487\n",
      "[5] loss: 0.949\n",
      "[6] loss: 1.399\n",
      "[7] loss: 1.727\n",
      "[8] loss: 1.642\n",
      "[9] loss: 1.909\n",
      "[10] loss: 1.993\n",
      "[11] loss: 2.203\n",
      "[12] loss: 1.923\n",
      "[13] loss: 1.980\n",
      "[14] loss: 1.920\n",
      "[15] loss: 1.778\n",
      "[16] loss: 2.133\n",
      "[17] loss: 2.232\n",
      "[18] loss: 2.248\n",
      "[19] loss: 2.281\n",
      "[20] loss: 2.307\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 0.819\n",
      "[2] loss: 0.375\n",
      "[3] loss: 0.261\n",
      "[4] loss: 0.221\n",
      "[5] loss: 0.188\n",
      "[6] loss: 0.173\n",
      "[7] loss: 0.168\n",
      "[8] loss: 0.159\n",
      "[9] loss: 0.149\n",
      "[10] loss: 0.138\n",
      "[11] loss: 0.148\n",
      "[12] loss: 0.130\n",
      "[13] loss: 0.133\n",
      "[14] loss: 0.127\n",
      "[15] loss: 0.132\n",
      "[16] loss: 0.120\n",
      "[17] loss: 0.124\n",
      "[18] loss: 0.122\n",
      "[19] loss: 0.118\n",
      "[20] loss: 0.113\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 0.957\n",
      "[2] loss: 0.354\n",
      "[3] loss: 0.243\n",
      "[4] loss: 0.195\n",
      "[5] loss: 0.162\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.127\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.097\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.079\n",
      "[13] loss: 0.074\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.065\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.050\n",
      "[19] loss: 0.053\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 0.765\n",
      "[2] loss: 0.358\n",
      "[3] loss: 0.291\n",
      "[4] loss: 0.244\n",
      "[5] loss: 0.206\n",
      "[6] loss: 0.180\n",
      "[7] loss: 0.153\n",
      "[8] loss: 0.137\n",
      "[9] loss: 0.124\n",
      "[10] loss: 0.114\n",
      "[11] loss: 0.104\n",
      "[12] loss: 0.095\n",
      "[13] loss: 0.090\n",
      "[14] loss: 0.084\n",
      "[15] loss: 0.078\n",
      "[16] loss: 0.075\n",
      "[17] loss: 0.069\n",
      "[18] loss: 0.063\n",
      "[19] loss: 0.058\n",
      "[20] loss: 0.056\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 1.638\n",
      "[2] loss: 1.285\n",
      "[3] loss: 1.594\n",
      "[4] loss: 1.798\n",
      "[5] loss: 1.902\n",
      "[6] loss: 1.678\n",
      "[7] loss: 1.848\n",
      "[8] loss: 2.053\n",
      "[9] loss: 2.012\n",
      "[10] loss: 1.755\n",
      "[11] loss: 1.754\n",
      "[12] loss: 1.947\n",
      "[13] loss: 1.719\n",
      "[14] loss: 1.969\n",
      "[15] loss: 1.943\n",
      "[16] loss: 1.843\n",
      "[17] loss: 1.900\n",
      "[18] loss: 2.342\n",
      "[19] loss: 2.345\n",
      "[20] loss: 2.319\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 1.033\n",
      "[2] loss: 0.411\n",
      "[3] loss: 0.252\n",
      "[4] loss: 0.204\n",
      "[5] loss: 0.193\n",
      "[6] loss: 0.164\n",
      "[7] loss: 0.152\n",
      "[8] loss: 0.148\n",
      "[9] loss: 0.141\n",
      "[10] loss: 0.133\n",
      "[11] loss: 0.138\n",
      "[12] loss: 0.127\n",
      "[13] loss: 0.131\n",
      "[14] loss: 0.129\n",
      "[15] loss: 0.126\n",
      "[16] loss: 0.122\n",
      "[17] loss: 0.112\n",
      "[18] loss: 0.125\n",
      "[19] loss: 0.129\n",
      "[20] loss: 0.135\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 0.594\n",
      "[2] loss: 0.280\n",
      "[3] loss: 0.206\n",
      "[4] loss: 0.165\n",
      "[5] loss: 0.138\n",
      "[6] loss: 0.121\n",
      "[7] loss: 0.109\n",
      "[8] loss: 0.099\n",
      "[9] loss: 0.088\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.074\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.062\n",
      "[14] loss: 0.057\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.052\n",
      "[17] loss: 0.048\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.043\n",
      "[20] loss: 0.042\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 8\n",
      "[1] loss: 0.666\n",
      "[2] loss: 0.334\n",
      "[3] loss: 0.261\n",
      "[4] loss: 0.212\n",
      "[5] loss: 0.176\n",
      "[6] loss: 0.155\n",
      "[7] loss: 0.135\n",
      "[8] loss: 0.121\n",
      "[9] loss: 0.111\n",
      "[10] loss: 0.096\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.080\n",
      "[13] loss: 0.076\n",
      "[14] loss: 0.068\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.056\n",
      "[19] loss: 0.050\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 0.923\n",
      "[2] loss: 0.579\n",
      "[3] loss: 0.540\n",
      "[4] loss: 0.545\n",
      "[5] loss: 0.849\n",
      "[6] loss: 1.235\n",
      "[7] loss: 1.303\n",
      "[8] loss: 1.840\n",
      "[9] loss: 1.780\n",
      "[10] loss: 1.432\n",
      "[11] loss: 1.894\n",
      "[12] loss: 2.003\n",
      "[13] loss: 1.920\n",
      "[14] loss: 1.871\n",
      "[15] loss: 2.134\n",
      "[16] loss: 2.034\n",
      "[17] loss: 1.848\n",
      "[18] loss: 2.138\n",
      "[19] loss: 2.152\n",
      "[20] loss: 2.285\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 0.654\n",
      "[2] loss: 0.306\n",
      "[3] loss: 0.229\n",
      "[4] loss: 0.189\n",
      "[5] loss: 0.175\n",
      "[6] loss: 0.152\n",
      "[7] loss: 0.145\n",
      "[8] loss: 0.143\n",
      "[9] loss: 0.133\n",
      "[10] loss: 0.128\n",
      "[11] loss: 0.126\n",
      "[12] loss: 0.121\n",
      "[13] loss: 0.123\n",
      "[14] loss: 0.116\n",
      "[15] loss: 0.114\n",
      "[16] loss: 0.124\n",
      "[17] loss: 0.121\n",
      "[18] loss: 0.110\n",
      "[19] loss: 0.111\n",
      "[20] loss: 0.106\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 0.593\n",
      "[2] loss: 0.267\n",
      "[3] loss: 0.194\n",
      "[4] loss: 0.153\n",
      "[5] loss: 0.134\n",
      "[6] loss: 0.115\n",
      "[7] loss: 0.105\n",
      "[8] loss: 0.097\n",
      "[9] loss: 0.086\n",
      "[10] loss: 0.079\n",
      "[11] loss: 0.070\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.064\n",
      "[14] loss: 0.056\n",
      "[15] loss: 0.057\n",
      "[16] loss: 0.050\n",
      "[17] loss: 0.048\n",
      "[18] loss: 0.045\n",
      "[19] loss: 0.041\n",
      "[20] loss: 0.040\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 0.630\n",
      "[2] loss: 0.337\n",
      "[3] loss: 0.247\n",
      "[4] loss: 0.200\n",
      "[5] loss: 0.169\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.132\n",
      "[8] loss: 0.116\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.100\n",
      "[11] loss: 0.089\n",
      "[12] loss: 0.084\n",
      "[13] loss: 0.076\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.064\n",
      "[17] loss: 0.059\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 0.900\n",
      "[2] loss: 0.739\n",
      "[3] loss: 0.606\n",
      "[4] loss: 0.616\n",
      "[5] loss: 0.641\n",
      "[6] loss: 1.550\n",
      "[7] loss: 1.611\n",
      "[8] loss: 1.330\n",
      "[9] loss: 1.830\n",
      "[10] loss: 2.079\n",
      "[11] loss: 1.821\n",
      "[12] loss: 1.739\n",
      "[13] loss: 2.177\n",
      "[14] loss: 2.323\n",
      "[15] loss: 2.314\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.311\n",
      "[18] loss: 2.311\n",
      "[19] loss: 2.310\n",
      "[20] loss: 2.311\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 0.606\n",
      "[2] loss: 0.306\n",
      "[3] loss: 0.258\n",
      "[4] loss: 0.234\n",
      "[5] loss: 0.221\n",
      "[6] loss: 0.212\n",
      "[7] loss: 0.195\n",
      "[8] loss: 0.180\n",
      "[9] loss: 0.167\n",
      "[10] loss: 0.161\n",
      "[11] loss: 0.150\n",
      "[12] loss: 0.153\n",
      "[13] loss: 0.145\n",
      "[14] loss: 0.156\n",
      "[15] loss: 0.144\n",
      "[16] loss: 0.131\n",
      "[17] loss: 0.136\n",
      "[18] loss: 0.140\n",
      "[19] loss: 0.131\n",
      "[20] loss: 0.134\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 0.907\n",
      "[2] loss: 0.316\n",
      "[3] loss: 0.227\n",
      "[4] loss: 0.179\n",
      "[5] loss: 0.153\n",
      "[6] loss: 0.132\n",
      "[7] loss: 0.120\n",
      "[8] loss: 0.105\n",
      "[9] loss: 0.092\n",
      "[10] loss: 0.087\n",
      "[11] loss: 0.077\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.061\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.049\n",
      "[19] loss: 0.044\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 1.036\n",
      "[2] loss: 0.464\n",
      "[3] loss: 0.322\n",
      "[4] loss: 0.258\n",
      "[5] loss: 0.215\n",
      "[6] loss: 0.184\n",
      "[7] loss: 0.158\n",
      "[8] loss: 0.140\n",
      "[9] loss: 0.125\n",
      "[10] loss: 0.112\n",
      "[11] loss: 0.103\n",
      "[12] loss: 0.094\n",
      "[13] loss: 0.088\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.075\n",
      "[16] loss: 0.070\n",
      "[17] loss: 0.065\n",
      "[18] loss: 0.063\n",
      "[19] loss: 0.058\n",
      "[20] loss: 0.053\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 1.506\n",
      "[2] loss: 1.386\n",
      "[3] loss: 1.522\n",
      "[4] loss: 1.741\n",
      "[5] loss: 1.940\n",
      "[6] loss: 1.932\n",
      "[7] loss: 1.937\n",
      "[8] loss: 1.878\n",
      "[9] loss: 1.798\n",
      "[10] loss: 1.909\n",
      "[11] loss: 1.776\n",
      "[12] loss: 1.554\n",
      "[13] loss: 1.666\n",
      "[14] loss: 1.769\n",
      "[15] loss: 2.117\n",
      "[16] loss: 2.238\n",
      "[17] loss: 2.122\n",
      "[18] loss: 2.267\n",
      "[19] loss: 2.300\n",
      "[20] loss: 2.323\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 0.920\n",
      "[2] loss: 0.407\n",
      "[3] loss: 0.272\n",
      "[4] loss: 0.212\n",
      "[5] loss: 0.178\n",
      "[6] loss: 0.164\n",
      "[7] loss: 0.152\n",
      "[8] loss: 0.140\n",
      "[9] loss: 0.128\n",
      "[10] loss: 0.129\n",
      "[11] loss: 0.128\n",
      "[12] loss: 0.118\n",
      "[13] loss: 0.112\n",
      "[14] loss: 0.114\n",
      "[15] loss: 0.118\n",
      "[16] loss: 0.112\n",
      "[17] loss: 0.121\n",
      "[18] loss: 0.126\n",
      "[19] loss: 0.110\n",
      "[20] loss: 0.121\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 11\n",
      "[1] loss: 0.787\n",
      "[2] loss: 0.279\n",
      "[3] loss: 0.212\n",
      "[4] loss: 0.174\n",
      "[5] loss: 0.147\n",
      "[6] loss: 0.128\n",
      "[7] loss: 0.114\n",
      "[8] loss: 0.098\n",
      "[9] loss: 0.093\n",
      "[10] loss: 0.086\n",
      "[11] loss: 0.080\n",
      "[12] loss: 0.075\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.065\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.055\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.046\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 11\n",
      "[1] loss: 0.664\n",
      "[2] loss: 0.330\n",
      "[3] loss: 0.264\n",
      "[4] loss: 0.211\n",
      "[5] loss: 0.176\n",
      "[6] loss: 0.149\n",
      "[7] loss: 0.136\n",
      "[8] loss: 0.118\n",
      "[9] loss: 0.107\n",
      "[10] loss: 0.099\n",
      "[11] loss: 0.088\n",
      "[12] loss: 0.085\n",
      "[13] loss: 0.078\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.068\n",
      "[16] loss: 0.063\n",
      "[17] loss: 0.061\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.056\n",
      "[20] loss: 0.048\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 11\n",
      "[1] loss: 1.082\n",
      "[2] loss: 0.822\n",
      "[3] loss: 0.929\n",
      "[4] loss: 1.728\n",
      "[5] loss: 1.815\n",
      "[6] loss: 1.753\n",
      "[7] loss: 1.952\n",
      "[8] loss: 1.979\n",
      "[9] loss: 1.790\n",
      "[10] loss: 2.030\n",
      "[11] loss: 2.024\n",
      "[12] loss: 2.077\n",
      "[13] loss: 2.209\n",
      "[14] loss: 2.247\n",
      "[15] loss: 2.314\n",
      "[16] loss: 2.313\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.310\n",
      "[19] loss: 2.311\n",
      "[20] loss: 2.311\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 11\n",
      "[1] loss: 0.859\n",
      "[2] loss: 0.309\n",
      "[3] loss: 0.211\n",
      "[4] loss: 0.178\n",
      "[5] loss: 0.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] loss: 0.138\n",
      "[7] loss: 0.129\n",
      "[8] loss: 0.130\n",
      "[9] loss: 0.120\n",
      "[10] loss: 0.108\n",
      "[11] loss: 0.114\n",
      "[12] loss: 0.120\n",
      "[13] loss: 0.116\n",
      "[14] loss: 0.110\n",
      "[15] loss: 0.107\n",
      "[16] loss: 0.105\n",
      "[17] loss: 0.108\n",
      "[18] loss: 0.103\n",
      "[19] loss: 0.102\n",
      "[20] loss: 0.103\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 12\n",
      "[1] loss: 0.727\n",
      "[2] loss: 0.306\n",
      "[3] loss: 0.213\n",
      "[4] loss: 0.170\n",
      "[5] loss: 0.142\n",
      "[6] loss: 0.124\n",
      "[7] loss: 0.112\n",
      "[8] loss: 0.101\n",
      "[9] loss: 0.094\n",
      "[10] loss: 0.086\n",
      "[11] loss: 0.079\n",
      "[12] loss: 0.073\n",
      "[13] loss: 0.071\n",
      "[14] loss: 0.063\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.056\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.048\n",
      "[19] loss: 0.045\n",
      "[20] loss: 0.046\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 12\n",
      "[1] loss: 0.705\n",
      "[2] loss: 0.362\n",
      "[3] loss: 0.253\n",
      "[4] loss: 0.194\n",
      "[5] loss: 0.161\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.112\n",
      "[9] loss: 0.102\n",
      "[10] loss: 0.093\n",
      "[11] loss: 0.086\n",
      "[12] loss: 0.078\n",
      "[13] loss: 0.073\n",
      "[14] loss: 0.068\n",
      "[15] loss: 0.065\n",
      "[16] loss: 0.057\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.049\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 12\n",
      "[1] loss: 1.547\n",
      "[2] loss: 1.295\n",
      "[3] loss: 1.490\n",
      "[4] loss: 1.906\n",
      "[5] loss: 1.763\n",
      "[6] loss: 1.980\n",
      "[7] loss: 2.195\n",
      "[8] loss: 2.049\n",
      "[9] loss: 2.035\n",
      "[10] loss: 2.076\n",
      "[11] loss: 1.913\n",
      "[12] loss: 1.839\n",
      "[13] loss: 1.852\n",
      "[14] loss: 2.005\n",
      "[15] loss: 1.814\n",
      "[16] loss: 2.000\n",
      "[17] loss: 1.806\n",
      "[18] loss: 1.926\n",
      "[19] loss: 1.828\n",
      "[20] loss: 2.057\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 12\n",
      "[1] loss: 0.960\n",
      "[2] loss: 0.411\n",
      "[3] loss: 0.267\n",
      "[4] loss: 0.210\n",
      "[5] loss: 0.197\n",
      "[6] loss: 0.175\n",
      "[7] loss: 0.156\n",
      "[8] loss: 0.145\n",
      "[9] loss: 0.144\n",
      "[10] loss: 0.142\n",
      "[11] loss: 0.142\n",
      "[12] loss: 0.134\n",
      "[13] loss: 0.129\n",
      "[14] loss: 0.128\n",
      "[15] loss: 0.131\n",
      "[16] loss: 0.137\n",
      "[17] loss: 0.138\n",
      "[18] loss: 0.126\n",
      "[19] loss: 0.121\n",
      "[20] loss: 0.120\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 13\n",
      "[1] loss: 0.539\n",
      "[2] loss: 0.248\n",
      "[3] loss: 0.191\n",
      "[4] loss: 0.155\n",
      "[5] loss: 0.131\n",
      "[6] loss: 0.113\n",
      "[7] loss: 0.101\n",
      "[8] loss: 0.095\n",
      "[9] loss: 0.084\n",
      "[10] loss: 0.082\n",
      "[11] loss: 0.073\n",
      "[12] loss: 0.067\n",
      "[13] loss: 0.065\n",
      "[14] loss: 0.056\n",
      "[15] loss: 0.055\n",
      "[16] loss: 0.053\n",
      "[17] loss: 0.048\n",
      "[18] loss: 0.047\n",
      "[19] loss: 0.045\n",
      "[20] loss: 0.040\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 13\n",
      "[1] loss: 0.598\n",
      "[2] loss: 0.313\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.196\n",
      "[5] loss: 0.168\n",
      "[6] loss: 0.144\n",
      "[7] loss: 0.126\n",
      "[8] loss: 0.113\n",
      "[9] loss: 0.104\n",
      "[10] loss: 0.098\n",
      "[11] loss: 0.087\n",
      "[12] loss: 0.083\n",
      "[13] loss: 0.074\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.067\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.057\n",
      "[18] loss: 0.055\n",
      "[19] loss: 0.049\n",
      "[20] loss: 0.046\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 13\n",
      "[1] loss: 1.021\n",
      "[2] loss: 0.648\n",
      "[3] loss: 0.587\n",
      "[4] loss: 0.579\n",
      "[5] loss: 0.773\n",
      "[6] loss: 1.548\n",
      "[7] loss: 1.538\n",
      "[8] loss: 1.645\n",
      "[9] loss: 1.449\n",
      "[10] loss: 1.574\n",
      "[11] loss: 1.641\n",
      "[12] loss: 1.546\n",
      "[13] loss: 1.425\n",
      "[14] loss: 1.492\n",
      "[15] loss: 2.234\n",
      "[16] loss: 2.343\n",
      "[17] loss: 2.318\n",
      "[18] loss: 2.313\n",
      "[19] loss: 2.311\n",
      "[20] loss: 2.312\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 13\n",
      "[1] loss: 0.627\n",
      "[2] loss: 0.257\n",
      "[3] loss: 0.201\n",
      "[4] loss: 0.162\n",
      "[5] loss: 0.146\n",
      "[6] loss: 0.138\n",
      "[7] loss: 0.127\n",
      "[8] loss: 0.129\n",
      "[9] loss: 0.131\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.106\n",
      "[12] loss: 0.113\n",
      "[13] loss: 0.117\n",
      "[14] loss: 0.105\n",
      "[15] loss: 0.121\n",
      "[16] loss: 0.108\n",
      "[17] loss: 0.116\n",
      "[18] loss: 0.109\n",
      "[19] loss: 0.099\n",
      "[20] loss: 0.110\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 14\n",
      "[1] loss: 0.657\n",
      "[2] loss: 0.311\n",
      "[3] loss: 0.234\n",
      "[4] loss: 0.185\n",
      "[5] loss: 0.161\n",
      "[6] loss: 0.136\n",
      "[7] loss: 0.121\n",
      "[8] loss: 0.107\n",
      "[9] loss: 0.096\n",
      "[10] loss: 0.091\n",
      "[11] loss: 0.084\n",
      "[12] loss: 0.075\n",
      "[13] loss: 0.071\n",
      "[14] loss: 0.064\n",
      "[15] loss: 0.059\n",
      "[16] loss: 0.054\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.049\n",
      "[19] loss: 0.047\n",
      "[20] loss: 0.045\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 14\n",
      "[1] loss: 0.704\n",
      "[2] loss: 0.371\n",
      "[3] loss: 0.271\n",
      "[4] loss: 0.210\n",
      "[5] loss: 0.174\n",
      "[6] loss: 0.145\n",
      "[7] loss: 0.131\n",
      "[8] loss: 0.115\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.094\n",
      "[11] loss: 0.086\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.074\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.065\n",
      "[16] loss: 0.059\n",
      "[17] loss: 0.056\n",
      "[18] loss: 0.051\n",
      "[19] loss: 0.051\n",
      "[20] loss: 0.050\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 14\n",
      "[1] loss: 1.100\n",
      "[2] loss: 0.767\n",
      "[3] loss: 0.562\n",
      "[4] loss: 0.501\n",
      "[5] loss: 0.705\n",
      "[6] loss: 1.472\n",
      "[7] loss: 1.683\n",
      "[8] loss: 1.791\n",
      "[9] loss: 1.761\n",
      "[10] loss: 1.814\n",
      "[11] loss: 1.792\n",
      "[12] loss: 1.836\n",
      "[13] loss: 1.840\n",
      "[14] loss: 1.623\n",
      "[15] loss: 1.834\n",
      "[16] loss: 2.154\n",
      "[17] loss: 2.179\n",
      "[18] loss: 2.291\n",
      "[19] loss: 2.247\n",
      "[20] loss: 2.285\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 14\n",
      "[1] loss: 0.786\n",
      "[2] loss: 0.318\n",
      "[3] loss: 0.269\n",
      "[4] loss: 0.245\n",
      "[5] loss: 0.204\n",
      "[6] loss: 0.177\n",
      "[7] loss: 0.167\n",
      "[8] loss: 0.166\n",
      "[9] loss: 0.153\n",
      "[10] loss: 0.164\n",
      "[11] loss: 0.139\n",
      "[12] loss: 0.139\n",
      "[13] loss: 0.133\n",
      "[14] loss: 0.124\n",
      "[15] loss: 0.129\n",
      "[16] loss: 0.133\n",
      "[17] loss: 0.132\n",
      "[18] loss: 0.127\n",
      "[19] loss: 0.144\n",
      "[20] loss: 0.130\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 15\n",
      "[1] loss: 0.723\n",
      "[2] loss: 0.324\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.185\n",
      "[5] loss: 0.157\n",
      "[6] loss: 0.138\n",
      "[7] loss: 0.122\n",
      "[8] loss: 0.109\n",
      "[9] loss: 0.100\n",
      "[10] loss: 0.093\n",
      "[11] loss: 0.086\n",
      "[12] loss: 0.080\n",
      "[13] loss: 0.072\n",
      "[14] loss: 0.069\n",
      "[15] loss: 0.066\n",
      "[16] loss: 0.060\n",
      "[17] loss: 0.059\n",
      "[18] loss: 0.054\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.047\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 15\n",
      "[1] loss: 0.788\n",
      "[2] loss: 0.386\n",
      "[3] loss: 0.281\n",
      "[4] loss: 0.228\n",
      "[5] loss: 0.198\n",
      "[6] loss: 0.175\n",
      "[7] loss: 0.156\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.124\n",
      "[10] loss: 0.113\n",
      "[11] loss: 0.109\n",
      "[12] loss: 0.100\n",
      "[13] loss: 0.095\n",
      "[14] loss: 0.086\n",
      "[15] loss: 0.081\n",
      "[16] loss: 0.074\n",
      "[17] loss: 0.071\n",
      "[18] loss: 0.067\n",
      "[19] loss: 0.064\n",
      "[20] loss: 0.057\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 15\n",
      "[1] loss: 1.485\n",
      "[2] loss: 0.972\n",
      "[3] loss: 1.179\n",
      "[4] loss: 1.281\n",
      "[5] loss: 1.763\n",
      "[6] loss: 1.692\n",
      "[7] loss: 1.774\n",
      "[8] loss: 1.815\n",
      "[9] loss: 1.715\n",
      "[10] loss: 1.735\n",
      "[11] loss: 1.732\n",
      "[12] loss: 1.994\n",
      "[13] loss: 1.918\n",
      "[14] loss: 1.814\n",
      "[15] loss: 2.268\n",
      "[16] loss: 2.327\n",
      "[17] loss: 2.289\n",
      "[18] loss: 2.287\n",
      "[19] loss: 2.322\n",
      "[20] loss: 2.314\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 15\n",
      "[1] loss: 0.857\n",
      "[2] loss: 0.307\n",
      "[3] loss: 0.230\n",
      "[4] loss: 0.197\n",
      "[5] loss: 0.182\n",
      "[6] loss: 0.177\n",
      "[7] loss: 0.156\n",
      "[8] loss: 0.150\n",
      "[9] loss: 0.152\n",
      "[10] loss: 0.147\n",
      "[11] loss: 0.140\n",
      "[12] loss: 0.146\n",
      "[13] loss: 0.137\n",
      "[14] loss: 0.128\n",
      "[15] loss: 0.125\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.120\n",
      "[18] loss: 0.127\n",
      "[19] loss: 0.129\n",
      "[20] loss: 0.116\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3bee607320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 15\n",
    "\n",
    "int_lr = 0.03\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 1.995\n",
      "[2] loss: 1.852\n",
      "[3] loss: 1.791\n",
      "[4] loss: 1.741\n",
      "[5] loss: 1.701\n",
      "[6] loss: 1.658\n",
      "[7] loss: 1.625\n",
      "[8] loss: 1.595\n",
      "[9] loss: 1.564\n",
      "[10] loss: 1.542\n",
      "[11] loss: 1.522\n",
      "[12] loss: 1.495\n",
      "[13] loss: 1.474\n",
      "[14] loss: 1.454\n",
      "[15] loss: 1.434\n",
      "[16] loss: 1.419\n",
      "[17] loss: 1.400\n",
      "[18] loss: 1.384\n",
      "[19] loss: 1.365\n",
      "[20] loss: 1.347\n",
      "[21] loss: 1.336\n",
      "[22] loss: 1.319\n",
      "[23] loss: 1.300\n",
      "[24] loss: 1.291\n",
      "[25] loss: 1.277\n",
      "[26] loss: 1.262\n",
      "[27] loss: 1.251\n",
      "[28] loss: 1.237\n",
      "[29] loss: 1.225\n",
      "[30] loss: 1.209\n",
      "[31] loss: 1.202\n",
      "[32] loss: 1.188\n",
      "[33] loss: 1.172\n",
      "[34] loss: 1.164\n",
      "[35] loss: 1.152\n",
      "[36] loss: 1.140\n",
      "[37] loss: 1.130\n",
      "[38] loss: 1.120\n",
      "[39] loss: 1.105\n",
      "[40] loss: 1.095\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 1\n",
      "[1] loss: 2.079\n",
      "[2] loss: 1.995\n",
      "[3] loss: 1.995\n",
      "[4] loss: 1.979\n",
      "[5] loss: 1.950\n",
      "[6] loss: 1.931\n",
      "[7] loss: 1.904\n",
      "[8] loss: 1.864\n",
      "[9] loss: 1.856\n",
      "[10] loss: 1.834\n",
      "[11] loss: 1.818\n",
      "[12] loss: 1.787\n",
      "[13] loss: 1.772\n",
      "[14] loss: 1.756\n",
      "[15] loss: 1.745\n",
      "[16] loss: 1.723\n",
      "[17] loss: 1.709\n",
      "[18] loss: 1.690\n",
      "[19] loss: 1.670\n",
      "[20] loss: 1.649\n",
      "[21] loss: 1.635\n",
      "[22] loss: 1.619\n",
      "[23] loss: 1.602\n",
      "[24] loss: 1.590\n",
      "[25] loss: 1.580\n",
      "[26] loss: 1.567\n",
      "[27] loss: 1.553\n",
      "[28] loss: 1.543\n",
      "[29] loss: 1.530\n",
      "[30] loss: 1.515\n",
      "[31] loss: 1.505\n",
      "[32] loss: 1.498\n",
      "[33] loss: 1.484\n",
      "[34] loss: 1.474\n",
      "[35] loss: 1.462\n",
      "[36] loss: 1.452\n",
      "[37] loss: 1.440\n",
      "[38] loss: 1.430\n",
      "[39] loss: 1.421\n",
      "[40] loss: 1.411\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 1\n",
      "[1] loss: 2.137\n",
      "[2] loss: 2.129\n",
      "[3] loss: 2.117\n",
      "[4] loss: 2.131\n",
      "[5] loss: 2.186\n",
      "[6] loss: 2.212\n",
      "[7] loss: 2.198\n",
      "[8] loss: 2.286\n",
      "[9] loss: 2.281\n",
      "[10] loss: 2.305\n",
      "[11] loss: 2.314\n",
      "[12] loss: 2.313\n",
      "[13] loss: 2.314\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.306\n",
      "[16] loss: 2.303\n",
      "[17] loss: 2.308\n",
      "[18] loss: 2.311\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.312\n",
      "[22] loss: 2.312\n",
      "[23] loss: 2.312\n",
      "[24] loss: 2.312\n",
      "[25] loss: 2.312\n",
      "[26] loss: 2.312\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.312\n",
      "[29] loss: 2.312\n",
      "[30] loss: 2.312\n",
      "[31] loss: 2.312\n",
      "[32] loss: 2.312\n",
      "[33] loss: 2.312\n",
      "[34] loss: 2.312\n",
      "[35] loss: 2.312\n",
      "[36] loss: 2.312\n",
      "[37] loss: 2.312\n",
      "[38] loss: 2.312\n",
      "[39] loss: 2.312\n",
      "[40] loss: 2.312\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 1\n",
      "[1] loss: 2.119\n",
      "[2] loss: 1.944\n",
      "[3] loss: 1.876\n",
      "[4] loss: 1.835\n",
      "[5] loss: 1.804\n",
      "[6] loss: 1.769\n",
      "[7] loss: 1.744\n",
      "[8] loss: 1.731\n",
      "[9] loss: 1.704\n",
      "[10] loss: 1.676\n",
      "[11] loss: 1.665\n",
      "[12] loss: 1.639\n",
      "[13] loss: 1.619\n",
      "[14] loss: 1.605\n",
      "[15] loss: 1.589\n",
      "[16] loss: 1.584\n",
      "[17] loss: 1.569\n",
      "[18] loss: 1.550\n",
      "[19] loss: 1.546\n",
      "[20] loss: 1.531\n",
      "[21] loss: 1.516\n",
      "[22] loss: 1.523\n",
      "[23] loss: 1.504\n",
      "[24] loss: 1.498\n",
      "[25] loss: 1.496\n",
      "[26] loss: 1.477\n",
      "[27] loss: 1.463\n",
      "[28] loss: 1.466\n",
      "[29] loss: 1.452\n",
      "[30] loss: 1.454\n",
      "[31] loss: 1.435\n",
      "[32] loss: 1.432\n",
      "[33] loss: 1.419\n",
      "[34] loss: 1.420\n",
      "[35] loss: 1.404\n",
      "[36] loss: 1.409\n",
      "[37] loss: 1.407\n",
      "[38] loss: 1.390\n",
      "[39] loss: 1.390\n",
      "[40] loss: 1.379\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 1.980\n",
      "[2] loss: 1.848\n",
      "[3] loss: 1.777\n",
      "[4] loss: 1.730\n",
      "[5] loss: 1.687\n",
      "[6] loss: 1.653\n",
      "[7] loss: 1.627\n",
      "[8] loss: 1.598\n",
      "[9] loss: 1.573\n",
      "[10] loss: 1.548\n",
      "[11] loss: 1.530\n",
      "[12] loss: 1.500\n",
      "[13] loss: 1.482\n",
      "[14] loss: 1.462\n",
      "[15] loss: 1.438\n",
      "[16] loss: 1.421\n",
      "[17] loss: 1.410\n",
      "[18] loss: 1.389\n",
      "[19] loss: 1.373\n",
      "[20] loss: 1.353\n",
      "[21] loss: 1.341\n",
      "[22] loss: 1.323\n",
      "[23] loss: 1.311\n",
      "[24] loss: 1.292\n",
      "[25] loss: 1.277\n",
      "[26] loss: 1.264\n",
      "[27] loss: 1.250\n",
      "[28] loss: 1.233\n",
      "[29] loss: 1.226\n",
      "[30] loss: 1.208\n",
      "[31] loss: 1.197\n",
      "[32] loss: 1.187\n",
      "[33] loss: 1.174\n",
      "[34] loss: 1.160\n",
      "[35] loss: 1.145\n",
      "[36] loss: 1.135\n",
      "[37] loss: 1.124\n",
      "[38] loss: 1.113\n",
      "[39] loss: 1.100\n",
      "[40] loss: 1.093\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 2\n",
      "[1] loss: 2.002\n",
      "[2] loss: 1.917\n",
      "[3] loss: 1.893\n",
      "[4] loss: 1.881\n",
      "[5] loss: 1.853\n",
      "[6] loss: 1.848\n",
      "[7] loss: 1.837\n",
      "[8] loss: 1.819\n",
      "[9] loss: 1.802\n",
      "[10] loss: 1.786\n",
      "[11] loss: 1.768\n",
      "[12] loss: 1.749\n",
      "[13] loss: 1.736\n",
      "[14] loss: 1.716\n",
      "[15] loss: 1.704\n",
      "[16] loss: 1.677\n",
      "[17] loss: 1.668\n",
      "[18] loss: 1.653\n",
      "[19] loss: 1.641\n",
      "[20] loss: 1.621\n",
      "[21] loss: 1.611\n",
      "[22] loss: 1.595\n",
      "[23] loss: 1.589\n",
      "[24] loss: 1.577\n",
      "[25] loss: 1.563\n",
      "[26] loss: 1.545\n",
      "[27] loss: 1.528\n",
      "[28] loss: 1.515\n",
      "[29] loss: 1.506\n",
      "[30] loss: 1.493\n",
      "[31] loss: 1.481\n",
      "[32] loss: 1.473\n",
      "[33] loss: 1.463\n",
      "[34] loss: 1.451\n",
      "[35] loss: 1.444\n",
      "[36] loss: 1.431\n",
      "[37] loss: 1.424\n",
      "[38] loss: 1.414\n",
      "[39] loss: 1.408\n",
      "[40] loss: 1.396\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 2\n",
      "[1] loss: 2.173\n",
      "[2] loss: 2.143\n",
      "[3] loss: 2.114\n",
      "[4] loss: 2.168\n",
      "[5] loss: 2.170\n",
      "[6] loss: 2.188\n",
      "[7] loss: 2.233\n",
      "[8] loss: 2.309\n",
      "[9] loss: 2.249\n",
      "[10] loss: 2.268\n",
      "[11] loss: 2.313\n",
      "[12] loss: 2.288\n",
      "[13] loss: 2.281\n",
      "[14] loss: 2.260\n",
      "[15] loss: 2.254\n",
      "[16] loss: 2.287\n",
      "[17] loss: 2.242\n",
      "[18] loss: 2.212\n",
      "[19] loss: 2.314\n",
      "[20] loss: 2.314\n",
      "[21] loss: 2.316\n",
      "[22] loss: 2.286\n",
      "[23] loss: 2.253\n",
      "[24] loss: 2.268\n",
      "[25] loss: 2.290\n",
      "[26] loss: 2.315\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.272\n",
      "[29] loss: 2.222\n",
      "[30] loss: 2.275\n",
      "[31] loss: 2.245\n",
      "[32] loss: 2.277\n",
      "[33] loss: 2.268\n",
      "[34] loss: 2.278\n",
      "[35] loss: 2.259\n",
      "[36] loss: 2.215\n",
      "[37] loss: 2.231\n",
      "[38] loss: 2.223\n",
      "[39] loss: 2.310\n",
      "[40] loss: 2.313\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 2\n",
      "[1] loss: 2.035\n",
      "[2] loss: 1.895\n",
      "[3] loss: 1.824\n",
      "[4] loss: 1.783\n",
      "[5] loss: 1.755\n",
      "[6] loss: 1.711\n",
      "[7] loss: 1.695\n",
      "[8] loss: 1.674\n",
      "[9] loss: 1.644\n",
      "[10] loss: 1.641\n",
      "[11] loss: 1.633\n",
      "[12] loss: 1.613\n",
      "[13] loss: 1.593\n",
      "[14] loss: 1.585\n",
      "[15] loss: 1.570\n",
      "[16] loss: 1.559\n",
      "[17] loss: 1.548\n",
      "[18] loss: 1.527\n",
      "[19] loss: 1.518\n",
      "[20] loss: 1.508\n",
      "[21] loss: 1.503\n",
      "[22] loss: 1.503\n",
      "[23] loss: 1.485\n",
      "[24] loss: 1.474\n",
      "[25] loss: 1.469\n",
      "[26] loss: 1.456\n",
      "[27] loss: 1.448\n",
      "[28] loss: 1.440\n",
      "[29] loss: 1.438\n",
      "[30] loss: 1.435\n",
      "[31] loss: 1.432\n",
      "[32] loss: 1.420\n",
      "[33] loss: 1.422\n",
      "[34] loss: 1.410\n",
      "[35] loss: 1.410\n",
      "[36] loss: 1.396\n",
      "[37] loss: 1.386\n",
      "[38] loss: 1.379\n",
      "[39] loss: 1.363\n",
      "[40] loss: 1.361\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 1.999\n",
      "[2] loss: 1.862\n",
      "[3] loss: 1.816\n",
      "[4] loss: 1.773\n",
      "[5] loss: 1.723\n",
      "[6] loss: 1.683\n",
      "[7] loss: 1.649\n",
      "[8] loss: 1.611\n",
      "[9] loss: 1.577\n",
      "[10] loss: 1.552\n",
      "[11] loss: 1.527\n",
      "[12] loss: 1.505\n",
      "[13] loss: 1.483\n",
      "[14] loss: 1.459\n",
      "[15] loss: 1.439\n",
      "[16] loss: 1.422\n",
      "[17] loss: 1.400\n",
      "[18] loss: 1.384\n",
      "[19] loss: 1.365\n",
      "[20] loss: 1.348\n",
      "[21] loss: 1.334\n",
      "[22] loss: 1.321\n",
      "[23] loss: 1.303\n",
      "[24] loss: 1.289\n",
      "[25] loss: 1.272\n",
      "[26] loss: 1.259\n",
      "[27] loss: 1.244\n",
      "[28] loss: 1.238\n",
      "[29] loss: 1.216\n",
      "[30] loss: 1.212\n",
      "[31] loss: 1.193\n",
      "[32] loss: 1.183\n",
      "[33] loss: 1.173\n",
      "[34] loss: 1.161\n",
      "[35] loss: 1.151\n",
      "[36] loss: 1.136\n",
      "[37] loss: 1.121\n",
      "[38] loss: 1.115\n",
      "[39] loss: 1.101\n",
      "[40] loss: 1.091\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 3\n",
      "[1] loss: 2.022\n",
      "[2] loss: 1.897\n",
      "[3] loss: 1.850\n",
      "[4] loss: 1.829\n",
      "[5] loss: 1.816\n",
      "[6] loss: 1.786\n",
      "[7] loss: 1.773\n",
      "[8] loss: 1.744\n",
      "[9] loss: 1.735\n",
      "[10] loss: 1.713\n",
      "[11] loss: 1.692\n",
      "[12] loss: 1.670\n",
      "[13] loss: 1.650\n",
      "[14] loss: 1.633\n",
      "[15] loss: 1.621\n",
      "[16] loss: 1.604\n",
      "[17] loss: 1.586\n",
      "[18] loss: 1.574\n",
      "[19] loss: 1.561\n",
      "[20] loss: 1.553\n",
      "[21] loss: 1.551\n",
      "[22] loss: 1.546\n",
      "[23] loss: 1.526\n",
      "[24] loss: 1.516\n",
      "[25] loss: 1.504\n",
      "[26] loss: 1.493\n",
      "[27] loss: 1.485\n",
      "[28] loss: 1.474\n",
      "[29] loss: 1.463\n",
      "[30] loss: 1.453\n",
      "[31] loss: 1.441\n",
      "[32] loss: 1.433\n",
      "[33] loss: 1.422\n",
      "[34] loss: 1.411\n",
      "[35] loss: 1.403\n",
      "[36] loss: 1.391\n",
      "[37] loss: 1.380\n",
      "[38] loss: 1.369\n",
      "[39] loss: 1.362\n",
      "[40] loss: 1.350\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 3\n",
      "[1] loss: 2.136\n",
      "[2] loss: 2.146\n",
      "[3] loss: 2.122\n",
      "[4] loss: 2.113\n",
      "[5] loss: 2.096\n",
      "[6] loss: 2.089\n",
      "[7] loss: 2.054\n",
      "[8] loss: 2.177\n",
      "[9] loss: 2.196\n",
      "[10] loss: 2.275\n",
      "[11] loss: 2.300\n",
      "[12] loss: 2.280\n",
      "[13] loss: 2.276\n",
      "[14] loss: 2.277\n",
      "[15] loss: 2.249\n",
      "[16] loss: 2.229\n",
      "[17] loss: 2.218\n",
      "[18] loss: 2.229\n",
      "[19] loss: 2.258\n",
      "[20] loss: 2.220\n",
      "[21] loss: 2.226\n",
      "[22] loss: 2.213\n",
      "[23] loss: 2.205\n",
      "[24] loss: 2.201\n",
      "[25] loss: 2.197\n",
      "[26] loss: 2.190\n",
      "[27] loss: 2.237\n",
      "[28] loss: 2.285\n",
      "[29] loss: 2.275\n",
      "[30] loss: 2.280\n",
      "[31] loss: 2.288\n",
      "[32] loss: 2.302\n",
      "[33] loss: 2.243\n",
      "[34] loss: 2.285\n",
      "[35] loss: 2.294\n",
      "[36] loss: 2.228\n",
      "[37] loss: 2.255\n",
      "[38] loss: 2.217\n",
      "[39] loss: 2.268\n",
      "[40] loss: 2.307\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 3\n",
      "[1] loss: 2.052\n",
      "[2] loss: 1.896\n",
      "[3] loss: 1.846\n",
      "[4] loss: 1.813\n",
      "[5] loss: 1.774\n",
      "[6] loss: 1.750\n",
      "[7] loss: 1.723\n",
      "[8] loss: 1.688\n",
      "[9] loss: 1.669\n",
      "[10] loss: 1.651\n",
      "[11] loss: 1.629\n",
      "[12] loss: 1.618\n",
      "[13] loss: 1.612\n",
      "[14] loss: 1.594\n",
      "[15] loss: 1.572\n",
      "[16] loss: 1.569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] loss: 1.558\n",
      "[18] loss: 1.543\n",
      "[19] loss: 1.532\n",
      "[20] loss: 1.521\n",
      "[21] loss: 1.514\n",
      "[22] loss: 1.509\n",
      "[23] loss: 1.503\n",
      "[24] loss: 1.485\n",
      "[25] loss: 1.489\n",
      "[26] loss: 1.473\n",
      "[27] loss: 1.473\n",
      "[28] loss: 1.459\n",
      "[29] loss: 1.461\n",
      "[30] loss: 1.457\n",
      "[31] loss: 1.443\n",
      "[32] loss: 1.439\n",
      "[33] loss: 1.440\n",
      "[34] loss: 1.427\n",
      "[35] loss: 1.417\n",
      "[36] loss: 1.420\n",
      "[37] loss: 1.418\n",
      "[38] loss: 1.405\n",
      "[39] loss: 1.384\n",
      "[40] loss: 1.390\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 2.008\n",
      "[2] loss: 1.874\n",
      "[3] loss: 1.814\n",
      "[4] loss: 1.764\n",
      "[5] loss: 1.720\n",
      "[6] loss: 1.681\n",
      "[7] loss: 1.645\n",
      "[8] loss: 1.612\n",
      "[9] loss: 1.583\n",
      "[10] loss: 1.553\n",
      "[11] loss: 1.538\n",
      "[12] loss: 1.510\n",
      "[13] loss: 1.489\n",
      "[14] loss: 1.470\n",
      "[15] loss: 1.450\n",
      "[16] loss: 1.430\n",
      "[17] loss: 1.414\n",
      "[18] loss: 1.391\n",
      "[19] loss: 1.373\n",
      "[20] loss: 1.363\n",
      "[21] loss: 1.341\n",
      "[22] loss: 1.322\n",
      "[23] loss: 1.315\n",
      "[24] loss: 1.295\n",
      "[25] loss: 1.282\n",
      "[26] loss: 1.269\n",
      "[27] loss: 1.256\n",
      "[28] loss: 1.243\n",
      "[29] loss: 1.233\n",
      "[30] loss: 1.221\n",
      "[31] loss: 1.204\n",
      "[32] loss: 1.192\n",
      "[33] loss: 1.177\n",
      "[34] loss: 1.165\n",
      "[35] loss: 1.149\n",
      "[36] loss: 1.141\n",
      "[37] loss: 1.128\n",
      "[38] loss: 1.113\n",
      "[39] loss: 1.107\n",
      "[40] loss: 1.094\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 4\n",
      "[1] loss: 2.024\n",
      "[2] loss: 1.923\n",
      "[3] loss: 1.897\n",
      "[4] loss: 1.880\n",
      "[5] loss: 1.863\n",
      "[6] loss: 1.838\n",
      "[7] loss: 1.815\n",
      "[8] loss: 1.802\n",
      "[9] loss: 1.788\n",
      "[10] loss: 1.778\n",
      "[11] loss: 1.767\n",
      "[12] loss: 1.754\n",
      "[13] loss: 1.746\n",
      "[14] loss: 1.734\n",
      "[15] loss: 1.719\n",
      "[16] loss: 1.700\n",
      "[17] loss: 1.687\n",
      "[18] loss: 1.669\n",
      "[19] loss: 1.654\n",
      "[20] loss: 1.649\n",
      "[21] loss: 1.635\n",
      "[22] loss: 1.619\n",
      "[23] loss: 1.606\n",
      "[24] loss: 1.591\n",
      "[25] loss: 1.573\n",
      "[26] loss: 1.559\n",
      "[27] loss: 1.542\n",
      "[28] loss: 1.532\n",
      "[29] loss: 1.527\n",
      "[30] loss: 1.512\n",
      "[31] loss: 1.496\n",
      "[32] loss: 1.487\n",
      "[33] loss: 1.476\n",
      "[34] loss: 1.466\n",
      "[35] loss: 1.459\n",
      "[36] loss: 1.450\n",
      "[37] loss: 1.437\n",
      "[38] loss: 1.430\n",
      "[39] loss: 1.423\n",
      "[40] loss: 1.411\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 4\n",
      "[1] loss: 2.148\n",
      "[2] loss: 2.150\n",
      "[3] loss: 2.122\n",
      "[4] loss: 2.126\n",
      "[5] loss: 2.188\n",
      "[6] loss: 2.250\n",
      "[7] loss: 2.182\n",
      "[8] loss: 2.175\n",
      "[9] loss: 2.253\n",
      "[10] loss: 2.272\n",
      "[11] loss: 2.313\n",
      "[12] loss: 2.309\n",
      "[13] loss: 2.314\n",
      "[14] loss: 2.315\n",
      "[15] loss: 2.253\n",
      "[16] loss: 2.314\n",
      "[17] loss: 2.303\n",
      "[18] loss: 2.316\n",
      "[19] loss: 2.313\n",
      "[20] loss: 2.316\n",
      "[21] loss: 2.315\n",
      "[22] loss: 2.313\n",
      "[23] loss: 2.262\n",
      "[24] loss: 2.283\n",
      "[25] loss: 2.237\n",
      "[26] loss: 2.215\n",
      "[27] loss: 2.210\n",
      "[28] loss: 2.227\n",
      "[29] loss: 2.308\n",
      "[30] loss: 2.224\n",
      "[31] loss: 2.236\n",
      "[32] loss: 2.245\n",
      "[33] loss: 2.287\n",
      "[34] loss: 2.222\n",
      "[35] loss: 2.303\n",
      "[36] loss: 2.272\n",
      "[37] loss: 2.301\n",
      "[38] loss: 2.264\n",
      "[39] loss: 2.261\n",
      "[40] loss: 2.228\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 4\n",
      "[1] loss: 2.055\n",
      "[2] loss: 1.921\n",
      "[3] loss: 1.847\n",
      "[4] loss: 1.807\n",
      "[5] loss: 1.776\n",
      "[6] loss: 1.757\n",
      "[7] loss: 1.735\n",
      "[8] loss: 1.707\n",
      "[9] loss: 1.680\n",
      "[10] loss: 1.666\n",
      "[11] loss: 1.661\n",
      "[12] loss: 1.631\n",
      "[13] loss: 1.634\n",
      "[14] loss: 1.617\n",
      "[15] loss: 1.583\n",
      "[16] loss: 1.585\n",
      "[17] loss: 1.577\n",
      "[18] loss: 1.557\n",
      "[19] loss: 1.547\n",
      "[20] loss: 1.548\n",
      "[21] loss: 1.531\n",
      "[22] loss: 1.539\n",
      "[23] loss: 1.524\n",
      "[24] loss: 1.513\n",
      "[25] loss: 1.505\n",
      "[26] loss: 1.501\n",
      "[27] loss: 1.489\n",
      "[28] loss: 1.485\n",
      "[29] loss: 1.474\n",
      "[30] loss: 1.474\n",
      "[31] loss: 1.469\n",
      "[32] loss: 1.454\n",
      "[33] loss: 1.464\n",
      "[34] loss: 1.449\n",
      "[35] loss: 1.458\n",
      "[36] loss: 1.441\n",
      "[37] loss: 1.417\n",
      "[38] loss: 1.416\n",
      "[39] loss: 1.428\n",
      "[40] loss: 1.413\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 5\n",
      "[1] loss: 1.988\n",
      "[2] loss: 1.852\n",
      "[3] loss: 1.796\n",
      "[4] loss: 1.742\n",
      "[5] loss: 1.698\n",
      "[6] loss: 1.661\n",
      "[7] loss: 1.632\n",
      "[8] loss: 1.601\n",
      "[9] loss: 1.579\n",
      "[10] loss: 1.554\n",
      "[11] loss: 1.532\n",
      "[12] loss: 1.508\n",
      "[13] loss: 1.493\n",
      "[14] loss: 1.474\n",
      "[15] loss: 1.460\n",
      "[16] loss: 1.440\n",
      "[17] loss: 1.427\n",
      "[18] loss: 1.410\n",
      "[19] loss: 1.392\n",
      "[20] loss: 1.376\n",
      "[21] loss: 1.362\n",
      "[22] loss: 1.349\n",
      "[23] loss: 1.333\n",
      "[24] loss: 1.320\n",
      "[25] loss: 1.303\n",
      "[26] loss: 1.293\n",
      "[27] loss: 1.276\n",
      "[28] loss: 1.261\n",
      "[29] loss: 1.246\n",
      "[30] loss: 1.243\n",
      "[31] loss: 1.218\n",
      "[32] loss: 1.207\n",
      "[33] loss: 1.199\n",
      "[34] loss: 1.181\n",
      "[35] loss: 1.171\n",
      "[36] loss: 1.163\n",
      "[37] loss: 1.145\n",
      "[38] loss: 1.136\n",
      "[39] loss: 1.127\n",
      "[40] loss: 1.115\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 5\n",
      "[1] loss: 2.027\n",
      "[2] loss: 1.917\n",
      "[3] loss: 1.902\n",
      "[4] loss: 1.886\n",
      "[5] loss: 1.869\n",
      "[6] loss: 1.854\n",
      "[7] loss: 1.844\n",
      "[8] loss: 1.820\n",
      "[9] loss: 1.812\n",
      "[10] loss: 1.795\n",
      "[11] loss: 1.778\n",
      "[12] loss: 1.763\n",
      "[13] loss: 1.743\n",
      "[14] loss: 1.733\n",
      "[15] loss: 1.711\n",
      "[16] loss: 1.694\n",
      "[17] loss: 1.671\n",
      "[18] loss: 1.658\n",
      "[19] loss: 1.633\n",
      "[20] loss: 1.615\n",
      "[21] loss: 1.605\n",
      "[22] loss: 1.582\n",
      "[23] loss: 1.572\n",
      "[24] loss: 1.555\n",
      "[25] loss: 1.544\n",
      "[26] loss: 1.532\n",
      "[27] loss: 1.519\n",
      "[28] loss: 1.507\n",
      "[29] loss: 1.496\n",
      "[30] loss: 1.485\n",
      "[31] loss: 1.472\n",
      "[32] loss: 1.461\n",
      "[33] loss: 1.454\n",
      "[34] loss: 1.445\n",
      "[35] loss: 1.433\n",
      "[36] loss: 1.423\n",
      "[37] loss: 1.410\n",
      "[38] loss: 1.401\n",
      "[39] loss: 1.394\n",
      "[40] loss: 1.381\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 5\n",
      "[1] loss: 2.105\n",
      "[2] loss: 2.058\n",
      "[3] loss: 2.067\n",
      "[4] loss: 2.038\n",
      "[5] loss: 2.046\n",
      "[6] loss: 2.094\n",
      "[7] loss: 2.174\n",
      "[8] loss: 2.150\n",
      "[9] loss: 2.145\n",
      "[10] loss: 2.231\n",
      "[11] loss: 2.217\n",
      "[12] loss: 2.198\n",
      "[13] loss: 2.268\n",
      "[14] loss: 2.274\n",
      "[15] loss: 2.243\n",
      "[16] loss: 2.240\n",
      "[17] loss: 2.221\n",
      "[18] loss: 2.243\n",
      "[19] loss: 2.218\n",
      "[20] loss: 2.260\n",
      "[21] loss: 2.250\n",
      "[22] loss: 2.286\n",
      "[23] loss: 2.307\n",
      "[24] loss: 2.233\n",
      "[25] loss: 2.231\n",
      "[26] loss: 2.310\n",
      "[27] loss: 2.270\n",
      "[28] loss: 2.267\n",
      "[29] loss: 2.218\n",
      "[30] loss: 2.230\n",
      "[31] loss: 2.251\n",
      "[32] loss: 2.235\n",
      "[33] loss: 2.261\n",
      "[34] loss: 2.312\n",
      "[35] loss: 2.253\n",
      "[36] loss: 2.302\n",
      "[37] loss: 2.315\n",
      "[38] loss: 2.316\n",
      "[39] loss: 2.315\n",
      "[40] loss: 2.315\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 5\n",
      "[1] loss: 2.031\n",
      "[2] loss: 1.910\n",
      "[3] loss: 1.841\n",
      "[4] loss: 1.790\n",
      "[5] loss: 1.757\n",
      "[6] loss: 1.743\n",
      "[7] loss: 1.696\n",
      "[8] loss: 1.674\n",
      "[9] loss: 1.653\n",
      "[10] loss: 1.636\n",
      "[11] loss: 1.616\n",
      "[12] loss: 1.603\n",
      "[13] loss: 1.588\n",
      "[14] loss: 1.577\n",
      "[15] loss: 1.563\n",
      "[16] loss: 1.544\n",
      "[17] loss: 1.546\n",
      "[18] loss: 1.535\n",
      "[19] loss: 1.534\n",
      "[20] loss: 1.510\n",
      "[21] loss: 1.501\n",
      "[22] loss: 1.494\n",
      "[23] loss: 1.483\n",
      "[24] loss: 1.477\n",
      "[25] loss: 1.473\n",
      "[26] loss: 1.460\n",
      "[27] loss: 1.444\n",
      "[28] loss: 1.452\n",
      "[29] loss: 1.433\n",
      "[30] loss: 1.429\n",
      "[31] loss: 1.417\n",
      "[32] loss: 1.415\n",
      "[33] loss: 1.406\n",
      "[34] loss: 1.401\n",
      "[35] loss: 1.390\n",
      "[36] loss: 1.392\n",
      "[37] loss: 1.373\n",
      "[38] loss: 1.367\n",
      "[39] loss: 1.366\n",
      "[40] loss: 1.362\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 6\n",
      "[1] loss: 2.012\n",
      "[2] loss: 1.866\n",
      "[3] loss: 1.811\n",
      "[4] loss: 1.766\n",
      "[5] loss: 1.719\n",
      "[6] loss: 1.677\n",
      "[7] loss: 1.639\n",
      "[8] loss: 1.612\n",
      "[9] loss: 1.583\n",
      "[10] loss: 1.561\n",
      "[11] loss: 1.536\n",
      "[12] loss: 1.514\n",
      "[13] loss: 1.499\n",
      "[14] loss: 1.475\n",
      "[15] loss: 1.463\n",
      "[16] loss: 1.438\n",
      "[17] loss: 1.424\n",
      "[18] loss: 1.409\n",
      "[19] loss: 1.391\n",
      "[20] loss: 1.373\n",
      "[21] loss: 1.359\n",
      "[22] loss: 1.346\n",
      "[23] loss: 1.326\n",
      "[24] loss: 1.309\n",
      "[25] loss: 1.292\n",
      "[26] loss: 1.279\n",
      "[27] loss: 1.271\n",
      "[28] loss: 1.253\n",
      "[29] loss: 1.237\n",
      "[30] loss: 1.226\n",
      "[31] loss: 1.211\n",
      "[32] loss: 1.199\n",
      "[33] loss: 1.189\n",
      "[34] loss: 1.172\n",
      "[35] loss: 1.162\n",
      "[36] loss: 1.152\n",
      "[37] loss: 1.139\n",
      "[38] loss: 1.122\n",
      "[39] loss: 1.116\n",
      "[40] loss: 1.101\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 6\n",
      "[1] loss: 2.042\n",
      "[2] loss: 1.914\n",
      "[3] loss: 1.900\n",
      "[4] loss: 1.876\n",
      "[5] loss: 1.859\n",
      "[6] loss: 1.844\n",
      "[7] loss: 1.822\n",
      "[8] loss: 1.790\n",
      "[9] loss: 1.774\n",
      "[10] loss: 1.752\n",
      "[11] loss: 1.737\n",
      "[12] loss: 1.720\n",
      "[13] loss: 1.700\n",
      "[14] loss: 1.683\n",
      "[15] loss: 1.662\n",
      "[16] loss: 1.647\n",
      "[17] loss: 1.627\n",
      "[18] loss: 1.611\n",
      "[19] loss: 1.591\n",
      "[20] loss: 1.576\n",
      "[21] loss: 1.565\n",
      "[22] loss: 1.545\n",
      "[23] loss: 1.533\n",
      "[24] loss: 1.518\n",
      "[25] loss: 1.503\n",
      "[26] loss: 1.494\n",
      "[27] loss: 1.482\n",
      "[28] loss: 1.471\n",
      "[29] loss: 1.464\n",
      "[30] loss: 1.450\n",
      "[31] loss: 1.436\n",
      "[32] loss: 1.425\n",
      "[33] loss: 1.419\n",
      "[34] loss: 1.406\n",
      "[35] loss: 1.392\n",
      "[36] loss: 1.385\n",
      "[37] loss: 1.372\n",
      "[38] loss: 1.359\n",
      "[39] loss: 1.347\n",
      "[40] loss: 1.338\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 6\n",
      "[1] loss: 2.151\n",
      "[2] loss: 2.148\n",
      "[3] loss: 2.162\n",
      "[4] loss: 2.134\n",
      "[5] loss: 2.170\n",
      "[6] loss: 2.203\n",
      "[7] loss: 2.286\n",
      "[8] loss: 2.274\n",
      "[9] loss: 2.251\n",
      "[10] loss: 2.287\n",
      "[11] loss: 2.308\n",
      "[12] loss: 2.280\n",
      "[13] loss: 2.264\n",
      "[14] loss: 2.227\n",
      "[15] loss: 2.227\n",
      "[16] loss: 2.238\n",
      "[17] loss: 2.228\n",
      "[18] loss: 2.268\n",
      "[19] loss: 2.245\n",
      "[20] loss: 2.219\n",
      "[21] loss: 2.245\n",
      "[22] loss: 2.258\n",
      "[23] loss: 2.258\n",
      "[24] loss: 2.314\n",
      "[25] loss: 2.282\n",
      "[26] loss: 2.263\n",
      "[27] loss: 2.263\n",
      "[28] loss: 2.263\n",
      "[29] loss: 2.266\n",
      "[30] loss: 2.278\n",
      "[31] loss: 2.239\n",
      "[32] loss: 2.233\n",
      "[33] loss: 2.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34] loss: 2.235\n",
      "[35] loss: 2.230\n",
      "[36] loss: 2.228\n",
      "[37] loss: 2.227\n",
      "[38] loss: 2.223\n",
      "[39] loss: 2.219\n",
      "[40] loss: 2.223\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 6\n",
      "[1] loss: 2.064\n",
      "[2] loss: 1.907\n",
      "[3] loss: 1.853\n",
      "[4] loss: 1.794\n",
      "[5] loss: 1.768\n",
      "[6] loss: 1.723\n",
      "[7] loss: 1.713\n",
      "[8] loss: 1.684\n",
      "[9] loss: 1.669\n",
      "[10] loss: 1.652\n",
      "[11] loss: 1.648\n",
      "[12] loss: 1.625\n",
      "[13] loss: 1.621\n",
      "[14] loss: 1.600\n",
      "[15] loss: 1.592\n",
      "[16] loss: 1.585\n",
      "[17] loss: 1.570\n",
      "[18] loss: 1.557\n",
      "[19] loss: 1.536\n",
      "[20] loss: 1.533\n",
      "[21] loss: 1.530\n",
      "[22] loss: 1.518\n",
      "[23] loss: 1.515\n",
      "[24] loss: 1.492\n",
      "[25] loss: 1.492\n",
      "[26] loss: 1.482\n",
      "[27] loss: 1.472\n",
      "[28] loss: 1.466\n",
      "[29] loss: 1.467\n",
      "[30] loss: 1.457\n",
      "[31] loss: 1.456\n",
      "[32] loss: 1.448\n",
      "[33] loss: 1.438\n",
      "[34] loss: 1.430\n",
      "[35] loss: 1.415\n",
      "[36] loss: 1.423\n",
      "[37] loss: 1.410\n",
      "[38] loss: 1.405\n",
      "[39] loss: 1.401\n",
      "[40] loss: 1.401\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 7\n",
      "[1] loss: 1.993\n",
      "[2] loss: 1.855\n",
      "[3] loss: 1.795\n",
      "[4] loss: 1.758\n",
      "[5] loss: 1.721\n",
      "[6] loss: 1.685\n",
      "[7] loss: 1.652\n",
      "[8] loss: 1.620\n",
      "[9] loss: 1.592\n",
      "[10] loss: 1.563\n",
      "[11] loss: 1.542\n",
      "[12] loss: 1.524\n",
      "[13] loss: 1.500\n",
      "[14] loss: 1.482\n",
      "[15] loss: 1.455\n",
      "[16] loss: 1.438\n",
      "[17] loss: 1.413\n",
      "[18] loss: 1.402\n",
      "[19] loss: 1.380\n",
      "[20] loss: 1.368\n",
      "[21] loss: 1.349\n",
      "[22] loss: 1.333\n",
      "[23] loss: 1.317\n",
      "[24] loss: 1.301\n",
      "[25] loss: 1.291\n",
      "[26] loss: 1.274\n",
      "[27] loss: 1.262\n",
      "[28] loss: 1.250\n",
      "[29] loss: 1.236\n",
      "[30] loss: 1.219\n",
      "[31] loss: 1.206\n",
      "[32] loss: 1.196\n",
      "[33] loss: 1.179\n",
      "[34] loss: 1.166\n",
      "[35] loss: 1.156\n",
      "[36] loss: 1.144\n",
      "[37] loss: 1.129\n",
      "[38] loss: 1.118\n",
      "[39] loss: 1.111\n",
      "[40] loss: 1.096\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 7\n",
      "[1] loss: 2.002\n",
      "[2] loss: 1.898\n",
      "[3] loss: 1.887\n",
      "[4] loss: 1.856\n",
      "[5] loss: 1.844\n",
      "[6] loss: 1.826\n",
      "[7] loss: 1.810\n",
      "[8] loss: 1.799\n",
      "[9] loss: 1.779\n",
      "[10] loss: 1.758\n",
      "[11] loss: 1.745\n",
      "[12] loss: 1.737\n",
      "[13] loss: 1.726\n",
      "[14] loss: 1.707\n",
      "[15] loss: 1.694\n",
      "[16] loss: 1.678\n",
      "[17] loss: 1.660\n",
      "[18] loss: 1.649\n",
      "[19] loss: 1.645\n",
      "[20] loss: 1.619\n",
      "[21] loss: 1.597\n",
      "[22] loss: 1.578\n",
      "[23] loss: 1.569\n",
      "[24] loss: 1.552\n",
      "[25] loss: 1.539\n",
      "[26] loss: 1.524\n",
      "[27] loss: 1.514\n",
      "[28] loss: 1.504\n",
      "[29] loss: 1.498\n",
      "[30] loss: 1.486\n",
      "[31] loss: 1.476\n",
      "[32] loss: 1.467\n",
      "[33] loss: 1.456\n",
      "[34] loss: 1.442\n",
      "[35] loss: 1.436\n",
      "[36] loss: 1.424\n",
      "[37] loss: 1.416\n",
      "[38] loss: 1.408\n",
      "[39] loss: 1.398\n",
      "[40] loss: 1.387\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 7\n",
      "[1] loss: 2.106\n",
      "[2] loss: 2.074\n",
      "[3] loss: 2.044\n",
      "[4] loss: 2.059\n",
      "[5] loss: 2.046\n",
      "[6] loss: 2.064\n",
      "[7] loss: 2.079\n",
      "[8] loss: 2.134\n",
      "[9] loss: 2.176\n",
      "[10] loss: 2.200\n",
      "[11] loss: 2.207\n",
      "[12] loss: 2.219\n",
      "[13] loss: 2.214\n",
      "[14] loss: 2.235\n",
      "[15] loss: 2.255\n",
      "[16] loss: 2.214\n",
      "[17] loss: 2.202\n",
      "[18] loss: 2.210\n",
      "[19] loss: 2.215\n",
      "[20] loss: 2.242\n",
      "[21] loss: 2.308\n",
      "[22] loss: 2.261\n",
      "[23] loss: 2.280\n",
      "[24] loss: 2.274\n",
      "[25] loss: 2.254\n",
      "[26] loss: 2.315\n",
      "[27] loss: 2.316\n",
      "[28] loss: 2.287\n",
      "[29] loss: 2.316\n",
      "[30] loss: 2.315\n",
      "[31] loss: 2.313\n",
      "[32] loss: 2.287\n",
      "[33] loss: 2.246\n",
      "[34] loss: 2.239\n",
      "[35] loss: 2.224\n",
      "[36] loss: 2.272\n",
      "[37] loss: 2.306\n",
      "[38] loss: 2.239\n",
      "[39] loss: 2.271\n",
      "[40] loss: 2.282\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 7\n",
      "[1] loss: 2.047\n",
      "[2] loss: 1.896\n",
      "[3] loss: 1.836\n",
      "[4] loss: 1.784\n",
      "[5] loss: 1.737\n",
      "[6] loss: 1.705\n",
      "[7] loss: 1.675\n",
      "[8] loss: 1.667\n",
      "[9] loss: 1.634\n",
      "[10] loss: 1.631\n",
      "[11] loss: 1.604\n",
      "[12] loss: 1.593\n",
      "[13] loss: 1.587\n",
      "[14] loss: 1.565\n",
      "[15] loss: 1.555\n",
      "[16] loss: 1.538\n",
      "[17] loss: 1.527\n",
      "[18] loss: 1.521\n",
      "[19] loss: 1.516\n",
      "[20] loss: 1.500\n",
      "[21] loss: 1.495\n",
      "[22] loss: 1.485\n",
      "[23] loss: 1.478\n",
      "[24] loss: 1.458\n",
      "[25] loss: 1.456\n",
      "[26] loss: 1.447\n",
      "[27] loss: 1.428\n",
      "[28] loss: 1.430\n",
      "[29] loss: 1.424\n",
      "[30] loss: 1.424\n",
      "[31] loss: 1.410\n",
      "[32] loss: 1.406\n",
      "[33] loss: 1.400\n",
      "[34] loss: 1.389\n",
      "[35] loss: 1.386\n",
      "[36] loss: 1.369\n",
      "[37] loss: 1.367\n",
      "[38] loss: 1.368\n",
      "[39] loss: 1.355\n",
      "[40] loss: 1.347\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 8\n",
      "[1] loss: 1.994\n",
      "[2] loss: 1.845\n",
      "[3] loss: 1.792\n",
      "[4] loss: 1.744\n",
      "[5] loss: 1.698\n",
      "[6] loss: 1.655\n",
      "[7] loss: 1.620\n",
      "[8] loss: 1.593\n",
      "[9] loss: 1.571\n",
      "[10] loss: 1.548\n",
      "[11] loss: 1.522\n",
      "[12] loss: 1.499\n",
      "[13] loss: 1.482\n",
      "[14] loss: 1.460\n",
      "[15] loss: 1.443\n",
      "[16] loss: 1.426\n",
      "[17] loss: 1.404\n",
      "[18] loss: 1.388\n",
      "[19] loss: 1.371\n",
      "[20] loss: 1.360\n",
      "[21] loss: 1.338\n",
      "[22] loss: 1.325\n",
      "[23] loss: 1.306\n",
      "[24] loss: 1.299\n",
      "[25] loss: 1.279\n",
      "[26] loss: 1.265\n",
      "[27] loss: 1.258\n",
      "[28] loss: 1.240\n",
      "[29] loss: 1.229\n",
      "[30] loss: 1.215\n",
      "[31] loss: 1.204\n",
      "[32] loss: 1.192\n",
      "[33] loss: 1.180\n",
      "[34] loss: 1.164\n",
      "[35] loss: 1.150\n",
      "[36] loss: 1.142\n",
      "[37] loss: 1.131\n",
      "[38] loss: 1.119\n",
      "[39] loss: 1.111\n",
      "[40] loss: 1.093\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 8\n",
      "[1] loss: 2.044\n",
      "[2] loss: 1.932\n",
      "[3] loss: 1.909\n",
      "[4] loss: 1.890\n",
      "[5] loss: 1.881\n",
      "[6] loss: 1.863\n",
      "[7] loss: 1.849\n",
      "[8] loss: 1.833\n",
      "[9] loss: 1.826\n",
      "[10] loss: 1.807\n",
      "[11] loss: 1.791\n",
      "[12] loss: 1.785\n",
      "[13] loss: 1.774\n",
      "[14] loss: 1.771\n",
      "[15] loss: 1.750\n",
      "[16] loss: 1.730\n",
      "[17] loss: 1.715\n",
      "[18] loss: 1.698\n",
      "[19] loss: 1.679\n",
      "[20] loss: 1.668\n",
      "[21] loss: 1.656\n",
      "[22] loss: 1.632\n",
      "[23] loss: 1.621\n",
      "[24] loss: 1.598\n",
      "[25] loss: 1.583\n",
      "[26] loss: 1.563\n",
      "[27] loss: 1.549\n",
      "[28] loss: 1.537\n",
      "[29] loss: 1.517\n",
      "[30] loss: 1.508\n",
      "[31] loss: 1.493\n",
      "[32] loss: 1.479\n",
      "[33] loss: 1.476\n",
      "[34] loss: 1.461\n",
      "[35] loss: 1.448\n",
      "[36] loss: 1.441\n",
      "[37] loss: 1.434\n",
      "[38] loss: 1.422\n",
      "[39] loss: 1.416\n",
      "[40] loss: 1.400\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 8\n",
      "[1] loss: 2.173\n",
      "[2] loss: 2.193\n",
      "[3] loss: 2.178\n",
      "[4] loss: 2.208\n",
      "[5] loss: 2.248\n",
      "[6] loss: 2.195\n",
      "[7] loss: 2.253\n",
      "[8] loss: 2.244\n",
      "[9] loss: 2.191\n",
      "[10] loss: 2.207\n",
      "[11] loss: 2.258\n",
      "[12] loss: 2.238\n",
      "[13] loss: 2.252\n",
      "[14] loss: 2.257\n",
      "[15] loss: 2.233\n",
      "[16] loss: 2.315\n",
      "[17] loss: 2.314\n",
      "[18] loss: 2.311\n",
      "[19] loss: 2.265\n",
      "[20] loss: 2.253\n",
      "[21] loss: 2.243\n",
      "[22] loss: 2.228\n",
      "[23] loss: 2.220\n",
      "[24] loss: 2.226\n",
      "[25] loss: 2.226\n",
      "[26] loss: 2.219\n",
      "[27] loss: 2.211\n",
      "[28] loss: 2.232\n",
      "[29] loss: 2.224\n",
      "[30] loss: 2.214\n",
      "[31] loss: 2.216\n",
      "[32] loss: 2.212\n",
      "[33] loss: 2.210\n",
      "[34] loss: 2.206\n",
      "[35] loss: 2.201\n",
      "[36] loss: 2.199\n",
      "[37] loss: 2.199\n",
      "[38] loss: 2.201\n",
      "[39] loss: 2.199\n",
      "[40] loss: 2.218\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 8\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.312\n",
      "[3] loss: 2.312\n",
      "[4] loss: 2.312\n",
      "[5] loss: 2.312\n",
      "[6] loss: 2.312\n",
      "[7] loss: 2.312\n",
      "[8] loss: 2.312\n",
      "[9] loss: 2.312\n",
      "[10] loss: 2.312\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.312\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.312\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.312\n",
      "[22] loss: 2.312\n",
      "[23] loss: 2.312\n",
      "[24] loss: 2.312\n",
      "[25] loss: 2.312\n",
      "[26] loss: 2.312\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.312\n",
      "[29] loss: 2.312\n",
      "[30] loss: 2.312\n",
      "[31] loss: 2.312\n",
      "[32] loss: 2.312\n",
      "[33] loss: 2.312\n",
      "[34] loss: 2.312\n",
      "[35] loss: 2.312\n",
      "[36] loss: 2.312\n",
      "[37] loss: 2.312\n",
      "[38] loss: 2.312\n",
      "[39] loss: 2.312\n",
      "[40] loss: 2.312\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 9\n",
      "[1] loss: 1.986\n",
      "[2] loss: 1.844\n",
      "[3] loss: 1.775\n",
      "[4] loss: 1.731\n",
      "[5] loss: 1.686\n",
      "[6] loss: 1.654\n",
      "[7] loss: 1.628\n",
      "[8] loss: 1.592\n",
      "[9] loss: 1.575\n",
      "[10] loss: 1.547\n",
      "[11] loss: 1.535\n",
      "[12] loss: 1.510\n",
      "[13] loss: 1.489\n",
      "[14] loss: 1.467\n",
      "[15] loss: 1.448\n",
      "[16] loss: 1.430\n",
      "[17] loss: 1.412\n",
      "[18] loss: 1.398\n",
      "[19] loss: 1.387\n",
      "[20] loss: 1.366\n",
      "[21] loss: 1.352\n",
      "[22] loss: 1.332\n",
      "[23] loss: 1.322\n",
      "[24] loss: 1.305\n",
      "[25] loss: 1.287\n",
      "[26] loss: 1.274\n",
      "[27] loss: 1.258\n",
      "[28] loss: 1.247\n",
      "[29] loss: 1.237\n",
      "[30] loss: 1.217\n",
      "[31] loss: 1.209\n",
      "[32] loss: 1.188\n",
      "[33] loss: 1.176\n",
      "[34] loss: 1.169\n",
      "[35] loss: 1.156\n",
      "[36] loss: 1.145\n",
      "[37] loss: 1.134\n",
      "[38] loss: 1.121\n",
      "[39] loss: 1.109\n",
      "[40] loss: 1.093\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 9\n",
      "[1] loss: 2.008\n",
      "[2] loss: 1.915\n",
      "[3] loss: 1.885\n",
      "[4] loss: 1.876\n",
      "[5] loss: 1.873\n",
      "[6] loss: 1.848\n",
      "[7] loss: 1.824\n",
      "[8] loss: 1.804\n",
      "[9] loss: 1.779\n",
      "[10] loss: 1.766\n",
      "[11] loss: 1.743\n",
      "[12] loss: 1.732\n",
      "[13] loss: 1.712\n",
      "[14] loss: 1.692\n",
      "[15] loss: 1.675\n",
      "[16] loss: 1.658\n",
      "[17] loss: 1.639\n",
      "[18] loss: 1.623\n",
      "[19] loss: 1.614\n",
      "[20] loss: 1.596\n",
      "[21] loss: 1.580\n",
      "[22] loss: 1.564\n",
      "[23] loss: 1.561\n",
      "[24] loss: 1.539\n",
      "[25] loss: 1.533\n",
      "[26] loss: 1.521\n",
      "[27] loss: 1.512\n",
      "[28] loss: 1.513\n",
      "[29] loss: 1.496\n",
      "[30] loss: 1.486\n",
      "[31] loss: 1.477\n",
      "[32] loss: 1.462\n",
      "[33] loss: 1.448\n",
      "[34] loss: 1.442\n",
      "[35] loss: 1.433\n",
      "[36] loss: 1.419\n",
      "[37] loss: 1.412\n",
      "[38] loss: 1.396\n",
      "[39] loss: 1.391\n",
      "[40] loss: 1.374\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 9\n",
      "[1] loss: 2.141\n",
      "[2] loss: 2.151\n",
      "[3] loss: 2.138\n",
      "[4] loss: 2.136\n",
      "[5] loss: 2.183\n",
      "[6] loss: 2.183\n",
      "[7] loss: 2.203\n",
      "[8] loss: 2.242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] loss: 2.259\n",
      "[10] loss: 2.289\n",
      "[11] loss: 2.294\n",
      "[12] loss: 2.287\n",
      "[13] loss: 2.307\n",
      "[14] loss: 2.251\n",
      "[15] loss: 2.237\n",
      "[16] loss: 2.236\n",
      "[17] loss: 2.269\n",
      "[18] loss: 2.301\n",
      "[19] loss: 2.265\n",
      "[20] loss: 2.263\n",
      "[21] loss: 2.292\n",
      "[22] loss: 2.313\n",
      "[23] loss: 2.302\n",
      "[24] loss: 2.222\n",
      "[25] loss: 2.272\n",
      "[26] loss: 2.277\n",
      "[27] loss: 2.256\n",
      "[28] loss: 2.310\n",
      "[29] loss: 2.305\n",
      "[30] loss: 2.314\n",
      "[31] loss: 2.287\n",
      "[32] loss: 2.252\n",
      "[33] loss: 2.270\n",
      "[34] loss: 2.282\n",
      "[35] loss: 2.313\n",
      "[36] loss: 2.271\n",
      "[37] loss: 2.255\n",
      "[38] loss: 2.252\n",
      "[39] loss: 2.247\n",
      "[40] loss: 2.306\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 9\n",
      "[1] loss: 2.073\n",
      "[2] loss: 1.917\n",
      "[3] loss: 1.854\n",
      "[4] loss: 1.813\n",
      "[5] loss: 1.782\n",
      "[6] loss: 1.759\n",
      "[7] loss: 1.736\n",
      "[8] loss: 1.706\n",
      "[9] loss: 1.703\n",
      "[10] loss: 1.674\n",
      "[11] loss: 1.656\n",
      "[12] loss: 1.633\n",
      "[13] loss: 1.619\n",
      "[14] loss: 1.613\n",
      "[15] loss: 1.584\n",
      "[16] loss: 1.589\n",
      "[17] loss: 1.570\n",
      "[18] loss: 1.561\n",
      "[19] loss: 1.553\n",
      "[20] loss: 1.543\n",
      "[21] loss: 1.537\n",
      "[22] loss: 1.529\n",
      "[23] loss: 1.518\n",
      "[24] loss: 1.515\n",
      "[25] loss: 1.504\n",
      "[26] loss: 1.501\n",
      "[27] loss: 1.484\n",
      "[28] loss: 1.482\n",
      "[29] loss: 1.478\n",
      "[30] loss: 1.465\n",
      "[31] loss: 1.458\n",
      "[32] loss: 1.461\n",
      "[33] loss: 1.444\n",
      "[34] loss: 1.448\n",
      "[35] loss: 1.438\n",
      "[36] loss: 1.431\n",
      "[37] loss: 1.416\n",
      "[38] loss: 1.406\n",
      "[39] loss: 1.405\n",
      "[40] loss: 1.396\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 10\n",
      "[1] loss: 1.990\n",
      "[2] loss: 1.853\n",
      "[3] loss: 1.794\n",
      "[4] loss: 1.737\n",
      "[5] loss: 1.699\n",
      "[6] loss: 1.657\n",
      "[7] loss: 1.624\n",
      "[8] loss: 1.601\n",
      "[9] loss: 1.571\n",
      "[10] loss: 1.547\n",
      "[11] loss: 1.527\n",
      "[12] loss: 1.502\n",
      "[13] loss: 1.481\n",
      "[14] loss: 1.462\n",
      "[15] loss: 1.440\n",
      "[16] loss: 1.414\n",
      "[17] loss: 1.395\n",
      "[18] loss: 1.381\n",
      "[19] loss: 1.358\n",
      "[20] loss: 1.349\n",
      "[21] loss: 1.327\n",
      "[22] loss: 1.321\n",
      "[23] loss: 1.303\n",
      "[24] loss: 1.292\n",
      "[25] loss: 1.280\n",
      "[26] loss: 1.261\n",
      "[27] loss: 1.247\n",
      "[28] loss: 1.231\n",
      "[29] loss: 1.223\n",
      "[30] loss: 1.212\n",
      "[31] loss: 1.196\n",
      "[32] loss: 1.190\n",
      "[33] loss: 1.176\n",
      "[34] loss: 1.162\n",
      "[35] loss: 1.152\n",
      "[36] loss: 1.139\n",
      "[37] loss: 1.130\n",
      "[38] loss: 1.118\n",
      "[39] loss: 1.105\n",
      "[40] loss: 1.091\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 10\n",
      "[1] loss: 2.015\n",
      "[2] loss: 1.906\n",
      "[3] loss: 1.876\n",
      "[4] loss: 1.861\n",
      "[5] loss: 1.831\n",
      "[6] loss: 1.813\n",
      "[7] loss: 1.799\n",
      "[8] loss: 1.778\n",
      "[9] loss: 1.770\n",
      "[10] loss: 1.758\n",
      "[11] loss: 1.752\n",
      "[12] loss: 1.732\n",
      "[13] loss: 1.721\n",
      "[14] loss: 1.703\n",
      "[15] loss: 1.690\n",
      "[16] loss: 1.670\n",
      "[17] loss: 1.655\n",
      "[18] loss: 1.642\n",
      "[19] loss: 1.624\n",
      "[20] loss: 1.609\n",
      "[21] loss: 1.593\n",
      "[22] loss: 1.583\n",
      "[23] loss: 1.565\n",
      "[24] loss: 1.545\n",
      "[25] loss: 1.535\n",
      "[26] loss: 1.522\n",
      "[27] loss: 1.507\n",
      "[28] loss: 1.497\n",
      "[29] loss: 1.490\n",
      "[30] loss: 1.476\n",
      "[31] loss: 1.467\n",
      "[32] loss: 1.457\n",
      "[33] loss: 1.447\n",
      "[34] loss: 1.438\n",
      "[35] loss: 1.427\n",
      "[36] loss: 1.417\n",
      "[37] loss: 1.407\n",
      "[38] loss: 1.400\n",
      "[39] loss: 1.389\n",
      "[40] loss: 1.379\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 10\n",
      "[1] loss: 2.116\n",
      "[2] loss: 2.087\n",
      "[3] loss: 2.091\n",
      "[4] loss: 2.075\n",
      "[5] loss: 2.065\n",
      "[6] loss: 2.081\n",
      "[7] loss: 2.091\n",
      "[8] loss: 2.091\n",
      "[9] loss: 2.168\n",
      "[10] loss: 2.153\n",
      "[11] loss: 2.208\n",
      "[12] loss: 2.238\n",
      "[13] loss: 2.266\n",
      "[14] loss: 2.258\n",
      "[15] loss: 2.250\n",
      "[16] loss: 2.230\n",
      "[17] loss: 2.221\n",
      "[18] loss: 2.207\n",
      "[19] loss: 2.217\n",
      "[20] loss: 2.212\n",
      "[21] loss: 2.208\n",
      "[22] loss: 2.212\n",
      "[23] loss: 2.237\n",
      "[24] loss: 2.223\n",
      "[25] loss: 2.212\n",
      "[26] loss: 2.243\n",
      "[27] loss: 2.239\n",
      "[28] loss: 2.222\n",
      "[29] loss: 2.208\n",
      "[30] loss: 2.231\n",
      "[31] loss: 2.222\n",
      "[32] loss: 2.262\n",
      "[33] loss: 2.208\n",
      "[34] loss: 2.261\n",
      "[35] loss: 2.263\n",
      "[36] loss: 2.218\n",
      "[37] loss: 2.218\n",
      "[38] loss: 2.224\n",
      "[39] loss: 2.201\n",
      "[40] loss: 2.313\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 10\n",
      "[1] loss: 2.023\n",
      "[2] loss: 1.899\n",
      "[3] loss: 1.839\n",
      "[4] loss: 1.796\n",
      "[5] loss: 1.779\n",
      "[6] loss: 1.737\n",
      "[7] loss: 1.703\n",
      "[8] loss: 1.693\n",
      "[9] loss: 1.662\n",
      "[10] loss: 1.645\n",
      "[11] loss: 1.627\n",
      "[12] loss: 1.611\n",
      "[13] loss: 1.602\n",
      "[14] loss: 1.589\n",
      "[15] loss: 1.579\n",
      "[16] loss: 1.565\n",
      "[17] loss: 1.562\n",
      "[18] loss: 1.549\n",
      "[19] loss: 1.531\n",
      "[20] loss: 1.524\n",
      "[21] loss: 1.510\n",
      "[22] loss: 1.510\n",
      "[23] loss: 1.503\n",
      "[24] loss: 1.492\n",
      "[25] loss: 1.487\n",
      "[26] loss: 1.477\n",
      "[27] loss: 1.461\n",
      "[28] loss: 1.464\n",
      "[29] loss: 1.465\n",
      "[30] loss: 1.444\n",
      "[31] loss: 1.434\n",
      "[32] loss: 1.426\n",
      "[33] loss: 1.434\n",
      "[34] loss: 1.429\n",
      "[35] loss: 1.413\n",
      "[36] loss: 1.402\n",
      "[37] loss: 1.388\n",
      "[38] loss: 1.396\n",
      "[39] loss: 1.384\n",
      "[40] loss: 1.382\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 11\n",
      "[1] loss: 2.004\n",
      "[2] loss: 1.879\n",
      "[3] loss: 1.811\n",
      "[4] loss: 1.765\n",
      "[5] loss: 1.724\n",
      "[6] loss: 1.689\n",
      "[7] loss: 1.657\n",
      "[8] loss: 1.626\n",
      "[9] loss: 1.603\n",
      "[10] loss: 1.574\n",
      "[11] loss: 1.548\n",
      "[12] loss: 1.525\n",
      "[13] loss: 1.506\n",
      "[14] loss: 1.485\n",
      "[15] loss: 1.465\n",
      "[16] loss: 1.450\n",
      "[17] loss: 1.427\n",
      "[18] loss: 1.411\n",
      "[19] loss: 1.395\n",
      "[20] loss: 1.375\n",
      "[21] loss: 1.364\n",
      "[22] loss: 1.341\n",
      "[23] loss: 1.331\n",
      "[24] loss: 1.314\n",
      "[25] loss: 1.304\n",
      "[26] loss: 1.288\n",
      "[27] loss: 1.272\n",
      "[28] loss: 1.262\n",
      "[29] loss: 1.249\n",
      "[30] loss: 1.236\n",
      "[31] loss: 1.222\n",
      "[32] loss: 1.208\n",
      "[33] loss: 1.197\n",
      "[34] loss: 1.186\n",
      "[35] loss: 1.167\n",
      "[36] loss: 1.157\n",
      "[37] loss: 1.146\n",
      "[38] loss: 1.130\n",
      "[39] loss: 1.125\n",
      "[40] loss: 1.107\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 11\n",
      "[1] loss: 2.004\n",
      "[2] loss: 1.916\n",
      "[3] loss: 1.900\n",
      "[4] loss: 1.877\n",
      "[5] loss: 1.858\n",
      "[6] loss: 1.841\n",
      "[7] loss: 1.809\n",
      "[8] loss: 1.799\n",
      "[9] loss: 1.782\n",
      "[10] loss: 1.757\n",
      "[11] loss: 1.744\n",
      "[12] loss: 1.725\n",
      "[13] loss: 1.711\n",
      "[14] loss: 1.699\n",
      "[15] loss: 1.683\n",
      "[16] loss: 1.669\n",
      "[17] loss: 1.659\n",
      "[18] loss: 1.639\n",
      "[19] loss: 1.633\n",
      "[20] loss: 1.614\n",
      "[21] loss: 1.599\n",
      "[22] loss: 1.583\n",
      "[23] loss: 1.569\n",
      "[24] loss: 1.557\n",
      "[25] loss: 1.541\n",
      "[26] loss: 1.533\n",
      "[27] loss: 1.518\n",
      "[28] loss: 1.507\n",
      "[29] loss: 1.501\n",
      "[30] loss: 1.487\n",
      "[31] loss: 1.472\n",
      "[32] loss: 1.464\n",
      "[33] loss: 1.453\n",
      "[34] loss: 1.441\n",
      "[35] loss: 1.428\n",
      "[36] loss: 1.423\n",
      "[37] loss: 1.413\n",
      "[38] loss: 1.401\n",
      "[39] loss: 1.392\n",
      "[40] loss: 1.380\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 11\n",
      "[1] loss: 2.160\n",
      "[2] loss: 2.162\n",
      "[3] loss: 2.130\n",
      "[4] loss: 2.147\n",
      "[5] loss: 2.230\n",
      "[6] loss: 2.159\n",
      "[7] loss: 2.242\n",
      "[8] loss: 2.210\n",
      "[9] loss: 2.227\n",
      "[10] loss: 2.259\n",
      "[11] loss: 2.209\n",
      "[12] loss: 2.274\n",
      "[13] loss: 2.279\n",
      "[14] loss: 2.231\n",
      "[15] loss: 2.266\n",
      "[16] loss: 2.307\n",
      "[17] loss: 2.315\n",
      "[18] loss: 2.315\n",
      "[19] loss: 2.306\n",
      "[20] loss: 2.307\n",
      "[21] loss: 2.316\n",
      "[22] loss: 2.308\n",
      "[23] loss: 2.309\n",
      "[24] loss: 2.290\n",
      "[25] loss: 2.311\n",
      "[26] loss: 2.287\n",
      "[27] loss: 2.222\n",
      "[28] loss: 2.231\n",
      "[29] loss: 2.208\n",
      "[30] loss: 2.255\n",
      "[31] loss: 2.253\n",
      "[32] loss: 2.216\n",
      "[33] loss: 2.277\n",
      "[34] loss: 2.275\n",
      "[35] loss: 2.279\n",
      "[36] loss: 2.298\n",
      "[37] loss: 2.314\n",
      "[38] loss: 2.315\n",
      "[39] loss: 2.314\n",
      "[40] loss: 2.314\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 11\n",
      "[1] loss: 2.043\n",
      "[2] loss: 1.885\n",
      "[3] loss: 1.822\n",
      "[4] loss: 1.778\n",
      "[5] loss: 1.746\n",
      "[6] loss: 1.716\n",
      "[7] loss: 1.692\n",
      "[8] loss: 1.670\n",
      "[9] loss: 1.648\n",
      "[10] loss: 1.622\n",
      "[11] loss: 1.610\n",
      "[12] loss: 1.582\n",
      "[13] loss: 1.576\n",
      "[14] loss: 1.564\n",
      "[15] loss: 1.545\n",
      "[16] loss: 1.535\n",
      "[17] loss: 1.529\n",
      "[18] loss: 1.514\n",
      "[19] loss: 1.501\n",
      "[20] loss: 1.504\n",
      "[21] loss: 1.481\n",
      "[22] loss: 1.481\n",
      "[23] loss: 1.462\n",
      "[24] loss: 1.461\n",
      "[25] loss: 1.447\n",
      "[26] loss: 1.449\n",
      "[27] loss: 1.443\n",
      "[28] loss: 1.440\n",
      "[29] loss: 1.431\n",
      "[30] loss: 1.421\n",
      "[31] loss: 1.411\n",
      "[32] loss: 1.407\n",
      "[33] loss: 1.405\n",
      "[34] loss: 1.399\n",
      "[35] loss: 1.390\n",
      "[36] loss: 1.380\n",
      "[37] loss: 1.383\n",
      "[38] loss: 1.361\n",
      "[39] loss: 1.355\n",
      "[40] loss: 1.353\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 12\n",
      "[1] loss: 2.016\n",
      "[2] loss: 1.868\n",
      "[3] loss: 1.798\n",
      "[4] loss: 1.751\n",
      "[5] loss: 1.707\n",
      "[6] loss: 1.671\n",
      "[7] loss: 1.638\n",
      "[8] loss: 1.610\n",
      "[9] loss: 1.581\n",
      "[10] loss: 1.556\n",
      "[11] loss: 1.536\n",
      "[12] loss: 1.512\n",
      "[13] loss: 1.491\n",
      "[14] loss: 1.469\n",
      "[15] loss: 1.447\n",
      "[16] loss: 1.428\n",
      "[17] loss: 1.406\n",
      "[18] loss: 1.387\n",
      "[19] loss: 1.374\n",
      "[20] loss: 1.357\n",
      "[21] loss: 1.340\n",
      "[22] loss: 1.324\n",
      "[23] loss: 1.307\n",
      "[24] loss: 1.290\n",
      "[25] loss: 1.283\n",
      "[26] loss: 1.263\n",
      "[27] loss: 1.252\n",
      "[28] loss: 1.238\n",
      "[29] loss: 1.223\n",
      "[30] loss: 1.209\n",
      "[31] loss: 1.196\n",
      "[32] loss: 1.185\n",
      "[33] loss: 1.171\n",
      "[34] loss: 1.157\n",
      "[35] loss: 1.146\n",
      "[36] loss: 1.133\n",
      "[37] loss: 1.120\n",
      "[38] loss: 1.103\n",
      "[39] loss: 1.100\n",
      "[40] loss: 1.083\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 12\n",
      "[1] loss: 2.043\n",
      "[2] loss: 1.932\n",
      "[3] loss: 1.907\n",
      "[4] loss: 1.904\n",
      "[5] loss: 1.885\n",
      "[6] loss: 1.867\n",
      "[7] loss: 1.858\n",
      "[8] loss: 1.853\n",
      "[9] loss: 1.841\n",
      "[10] loss: 1.832\n",
      "[11] loss: 1.819\n",
      "[12] loss: 1.810\n",
      "[13] loss: 1.787\n",
      "[14] loss: 1.775\n",
      "[15] loss: 1.765\n",
      "[16] loss: 1.749\n",
      "[17] loss: 1.733\n",
      "[18] loss: 1.716\n",
      "[19] loss: 1.696\n",
      "[20] loss: 1.681\n",
      "[21] loss: 1.673\n",
      "[22] loss: 1.651\n",
      "[23] loss: 1.632\n",
      "[24] loss: 1.614\n",
      "[25] loss: 1.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26] loss: 1.584\n",
      "[27] loss: 1.565\n",
      "[28] loss: 1.553\n",
      "[29] loss: 1.531\n",
      "[30] loss: 1.520\n",
      "[31] loss: 1.506\n",
      "[32] loss: 1.497\n",
      "[33] loss: 1.481\n",
      "[34] loss: 1.472\n",
      "[35] loss: 1.461\n",
      "[36] loss: 1.456\n",
      "[37] loss: 1.441\n",
      "[38] loss: 1.431\n",
      "[39] loss: 1.423\n",
      "[40] loss: 1.416\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 12\n",
      "[1] loss: 2.142\n",
      "[2] loss: 2.120\n",
      "[3] loss: 2.131\n",
      "[4] loss: 2.157\n",
      "[5] loss: 2.153\n",
      "[6] loss: 2.209\n",
      "[7] loss: 2.214\n",
      "[8] loss: 2.179\n",
      "[9] loss: 2.214\n",
      "[10] loss: 2.188\n",
      "[11] loss: 2.240\n",
      "[12] loss: 2.234\n",
      "[13] loss: 2.260\n",
      "[14] loss: 2.283\n",
      "[15] loss: 2.315\n",
      "[16] loss: 2.315\n",
      "[17] loss: 2.315\n",
      "[18] loss: 2.313\n",
      "[19] loss: 2.305\n",
      "[20] loss: 2.226\n",
      "[21] loss: 2.218\n",
      "[22] loss: 2.204\n",
      "[23] loss: 2.294\n",
      "[24] loss: 2.288\n",
      "[25] loss: 2.258\n",
      "[26] loss: 2.282\n",
      "[27] loss: 2.206\n",
      "[28] loss: 2.219\n",
      "[29] loss: 2.232\n",
      "[30] loss: 2.198\n",
      "[31] loss: 2.257\n",
      "[32] loss: 2.298\n",
      "[33] loss: 2.233\n",
      "[34] loss: 2.206\n",
      "[35] loss: 2.242\n",
      "[36] loss: 2.271\n",
      "[37] loss: 2.247\n",
      "[38] loss: 2.225\n",
      "[39] loss: 2.251\n",
      "[40] loss: 2.265\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 12\n",
      "[1] loss: 2.312\n",
      "[2] loss: 2.312\n",
      "[3] loss: 2.312\n",
      "[4] loss: 2.312\n",
      "[5] loss: 2.312\n",
      "[6] loss: 2.312\n",
      "[7] loss: 2.312\n",
      "[8] loss: 2.312\n",
      "[9] loss: 2.312\n",
      "[10] loss: 2.312\n",
      "[11] loss: 2.312\n",
      "[12] loss: 2.312\n",
      "[13] loss: 2.312\n",
      "[14] loss: 2.312\n",
      "[15] loss: 2.312\n",
      "[16] loss: 2.312\n",
      "[17] loss: 2.312\n",
      "[18] loss: 2.312\n",
      "[19] loss: 2.312\n",
      "[20] loss: 2.312\n",
      "[21] loss: 2.312\n",
      "[22] loss: 2.312\n",
      "[23] loss: 2.312\n",
      "[24] loss: 2.312\n",
      "[25] loss: 2.312\n",
      "[26] loss: 2.312\n",
      "[27] loss: 2.312\n",
      "[28] loss: 2.312\n",
      "[29] loss: 2.312\n",
      "[30] loss: 2.312\n",
      "[31] loss: 2.312\n",
      "[32] loss: 2.312\n",
      "[33] loss: 2.312\n",
      "[34] loss: 2.312\n",
      "[35] loss: 2.312\n",
      "[36] loss: 2.312\n",
      "[37] loss: 2.312\n",
      "[38] loss: 2.312\n",
      "[39] loss: 2.312\n",
      "[40] loss: 2.312\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 13\n",
      "[1] loss: 1.983\n",
      "[2] loss: 1.849\n",
      "[3] loss: 1.791\n",
      "[4] loss: 1.740\n",
      "[5] loss: 1.683\n",
      "[6] loss: 1.643\n",
      "[7] loss: 1.616\n",
      "[8] loss: 1.586\n",
      "[9] loss: 1.566\n",
      "[10] loss: 1.537\n",
      "[11] loss: 1.519\n",
      "[12] loss: 1.494\n",
      "[13] loss: 1.478\n",
      "[14] loss: 1.454\n",
      "[15] loss: 1.435\n",
      "[16] loss: 1.418\n",
      "[17] loss: 1.404\n",
      "[18] loss: 1.385\n",
      "[19] loss: 1.371\n",
      "[20] loss: 1.356\n",
      "[21] loss: 1.334\n",
      "[22] loss: 1.322\n",
      "[23] loss: 1.301\n",
      "[24] loss: 1.287\n",
      "[25] loss: 1.274\n",
      "[26] loss: 1.257\n",
      "[27] loss: 1.250\n",
      "[28] loss: 1.232\n",
      "[29] loss: 1.226\n",
      "[30] loss: 1.205\n",
      "[31] loss: 1.202\n",
      "[32] loss: 1.177\n",
      "[33] loss: 1.170\n",
      "[34] loss: 1.161\n",
      "[35] loss: 1.149\n",
      "[36] loss: 1.130\n",
      "[37] loss: 1.120\n",
      "[38] loss: 1.110\n",
      "[39] loss: 1.095\n",
      "[40] loss: 1.084\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 13\n",
      "[1] loss: 2.011\n",
      "[2] loss: 1.909\n",
      "[3] loss: 1.879\n",
      "[4] loss: 1.857\n",
      "[5] loss: 1.849\n",
      "[6] loss: 1.823\n",
      "[7] loss: 1.808\n",
      "[8] loss: 1.793\n",
      "[9] loss: 1.784\n",
      "[10] loss: 1.769\n",
      "[11] loss: 1.756\n",
      "[12] loss: 1.751\n",
      "[13] loss: 1.741\n",
      "[14] loss: 1.730\n",
      "[15] loss: 1.725\n",
      "[16] loss: 1.709\n",
      "[17] loss: 1.692\n",
      "[18] loss: 1.670\n",
      "[19] loss: 1.650\n",
      "[20] loss: 1.638\n",
      "[21] loss: 1.625\n",
      "[22] loss: 1.606\n",
      "[23] loss: 1.591\n",
      "[24] loss: 1.574\n",
      "[25] loss: 1.569\n",
      "[26] loss: 1.553\n",
      "[27] loss: 1.533\n",
      "[28] loss: 1.523\n",
      "[29] loss: 1.516\n",
      "[30] loss: 1.502\n",
      "[31] loss: 1.493\n",
      "[32] loss: 1.483\n",
      "[33] loss: 1.472\n",
      "[34] loss: 1.465\n",
      "[35] loss: 1.455\n",
      "[36] loss: 1.444\n",
      "[37] loss: 1.438\n",
      "[38] loss: 1.430\n",
      "[39] loss: 1.420\n",
      "[40] loss: 1.411\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 13\n",
      "[1] loss: 2.090\n",
      "[2] loss: 2.046\n",
      "[3] loss: 2.071\n",
      "[4] loss: 2.089\n",
      "[5] loss: 2.015\n",
      "[6] loss: 2.114\n",
      "[7] loss: 2.115\n",
      "[8] loss: 2.129\n",
      "[9] loss: 2.187\n",
      "[10] loss: 2.243\n",
      "[11] loss: 2.213\n",
      "[12] loss: 2.271\n",
      "[13] loss: 2.252\n",
      "[14] loss: 2.224\n",
      "[15] loss: 2.177\n",
      "[16] loss: 2.250\n",
      "[17] loss: 2.210\n",
      "[18] loss: 2.223\n",
      "[19] loss: 2.216\n",
      "[20] loss: 2.233\n",
      "[21] loss: 2.231\n",
      "[22] loss: 2.245\n",
      "[23] loss: 2.268\n",
      "[24] loss: 2.242\n",
      "[25] loss: 2.216\n",
      "[26] loss: 2.208\n",
      "[27] loss: 2.242\n",
      "[28] loss: 2.265\n",
      "[29] loss: 2.281\n",
      "[30] loss: 2.267\n",
      "[31] loss: 2.289\n",
      "[32] loss: 2.286\n",
      "[33] loss: 2.216\n",
      "[34] loss: 2.218\n",
      "[35] loss: 2.217\n",
      "[36] loss: 2.238\n",
      "[37] loss: 2.228\n",
      "[38] loss: 2.229\n",
      "[39] loss: 2.253\n",
      "[40] loss: 2.222\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 13\n",
      "[1] loss: 2.039\n",
      "[2] loss: 1.904\n",
      "[3] loss: 1.825\n",
      "[4] loss: 1.795\n",
      "[5] loss: 1.773\n",
      "[6] loss: 1.733\n",
      "[7] loss: 1.700\n",
      "[8] loss: 1.685\n",
      "[9] loss: 1.665\n",
      "[10] loss: 1.640\n",
      "[11] loss: 1.622\n",
      "[12] loss: 1.618\n",
      "[13] loss: 1.603\n",
      "[14] loss: 1.582\n",
      "[15] loss: 1.566\n",
      "[16] loss: 1.562\n",
      "[17] loss: 1.551\n",
      "[18] loss: 1.536\n",
      "[19] loss: 1.529\n",
      "[20] loss: 1.522\n",
      "[21] loss: 1.508\n",
      "[22] loss: 1.508\n",
      "[23] loss: 1.493\n",
      "[24] loss: 1.496\n",
      "[25] loss: 1.479\n",
      "[26] loss: 1.473\n",
      "[27] loss: 1.455\n",
      "[28] loss: 1.453\n",
      "[29] loss: 1.448\n",
      "[30] loss: 1.437\n",
      "[31] loss: 1.430\n",
      "[32] loss: 1.424\n",
      "[33] loss: 1.410\n",
      "[34] loss: 1.412\n",
      "[35] loss: 1.401\n",
      "[36] loss: 1.382\n",
      "[37] loss: 1.383\n",
      "[38] loss: 1.394\n",
      "[39] loss: 1.388\n",
      "[40] loss: 1.387\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 14\n",
      "[1] loss: 1.986\n",
      "[2] loss: 1.845\n",
      "[3] loss: 1.789\n",
      "[4] loss: 1.733\n",
      "[5] loss: 1.690\n",
      "[6] loss: 1.649\n",
      "[7] loss: 1.626\n",
      "[8] loss: 1.599\n",
      "[9] loss: 1.572\n",
      "[10] loss: 1.552\n",
      "[11] loss: 1.529\n",
      "[12] loss: 1.513\n",
      "[13] loss: 1.494\n",
      "[14] loss: 1.469\n",
      "[15] loss: 1.449\n",
      "[16] loss: 1.436\n",
      "[17] loss: 1.422\n",
      "[18] loss: 1.400\n",
      "[19] loss: 1.383\n",
      "[20] loss: 1.363\n",
      "[21] loss: 1.348\n",
      "[22] loss: 1.332\n",
      "[23] loss: 1.322\n",
      "[24] loss: 1.306\n",
      "[25] loss: 1.286\n",
      "[26] loss: 1.277\n",
      "[27] loss: 1.262\n",
      "[28] loss: 1.247\n",
      "[29] loss: 1.235\n",
      "[30] loss: 1.221\n",
      "[31] loss: 1.206\n",
      "[32] loss: 1.203\n",
      "[33] loss: 1.182\n",
      "[34] loss: 1.174\n",
      "[35] loss: 1.156\n",
      "[36] loss: 1.149\n",
      "[37] loss: 1.142\n",
      "[38] loss: 1.124\n",
      "[39] loss: 1.114\n",
      "[40] loss: 1.100\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 14\n",
      "[1] loss: 2.006\n",
      "[2] loss: 1.884\n",
      "[3] loss: 1.857\n",
      "[4] loss: 1.834\n",
      "[5] loss: 1.816\n",
      "[6] loss: 1.785\n",
      "[7] loss: 1.766\n",
      "[8] loss: 1.743\n",
      "[9] loss: 1.718\n",
      "[10] loss: 1.685\n",
      "[11] loss: 1.662\n",
      "[12] loss: 1.641\n",
      "[13] loss: 1.626\n",
      "[14] loss: 1.606\n",
      "[15] loss: 1.584\n",
      "[16] loss: 1.579\n",
      "[17] loss: 1.559\n",
      "[18] loss: 1.541\n",
      "[19] loss: 1.527\n",
      "[20] loss: 1.513\n",
      "[21] loss: 1.501\n",
      "[22] loss: 1.482\n",
      "[23] loss: 1.476\n",
      "[24] loss: 1.464\n",
      "[25] loss: 1.452\n",
      "[26] loss: 1.441\n",
      "[27] loss: 1.432\n",
      "[28] loss: 1.418\n",
      "[29] loss: 1.405\n",
      "[30] loss: 1.394\n",
      "[31] loss: 1.385\n",
      "[32] loss: 1.374\n",
      "[33] loss: 1.366\n",
      "[34] loss: 1.352\n",
      "[35] loss: 1.341\n",
      "[36] loss: 1.331\n",
      "[37] loss: 1.323\n",
      "[38] loss: 1.305\n",
      "[39] loss: 1.298\n",
      "[40] loss: 1.281\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 14\n",
      "[1] loss: 2.129\n",
      "[2] loss: 2.109\n",
      "[3] loss: 2.099\n",
      "[4] loss: 2.134\n",
      "[5] loss: 2.103\n",
      "[6] loss: 2.135\n",
      "[7] loss: 2.133\n",
      "[8] loss: 2.187\n",
      "[9] loss: 2.219\n",
      "[10] loss: 2.239\n",
      "[11] loss: 2.248\n",
      "[12] loss: 2.193\n",
      "[13] loss: 2.157\n",
      "[14] loss: 2.158\n",
      "[15] loss: 2.214\n",
      "[16] loss: 2.212\n",
      "[17] loss: 2.134\n",
      "[18] loss: 2.128\n",
      "[19] loss: 2.150\n",
      "[20] loss: 2.272\n",
      "[21] loss: 2.235\n",
      "[22] loss: 2.231\n",
      "[23] loss: 2.225\n",
      "[24] loss: 2.217\n",
      "[25] loss: 2.207\n",
      "[26] loss: 2.208\n",
      "[27] loss: 2.205\n",
      "[28] loss: 2.200\n",
      "[29] loss: 2.216\n",
      "[30] loss: 2.263\n",
      "[31] loss: 2.221\n",
      "[32] loss: 2.239\n",
      "[33] loss: 2.214\n",
      "[34] loss: 2.203\n",
      "[35] loss: 2.199\n",
      "[36] loss: 2.195\n",
      "[37] loss: 2.224\n",
      "[38] loss: 2.293\n",
      "[39] loss: 2.314\n",
      "[40] loss: 2.318\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 14\n",
      "[1] loss: 2.045\n",
      "[2] loss: 1.895\n",
      "[3] loss: 1.831\n",
      "[4] loss: 1.813\n",
      "[5] loss: 1.766\n",
      "[6] loss: 1.724\n",
      "[7] loss: 1.705\n",
      "[8] loss: 1.678\n",
      "[9] loss: 1.661\n",
      "[10] loss: 1.648\n",
      "[11] loss: 1.621\n",
      "[12] loss: 1.608\n",
      "[13] loss: 1.600\n",
      "[14] loss: 1.582\n",
      "[15] loss: 1.550\n",
      "[16] loss: 1.558\n",
      "[17] loss: 1.542\n",
      "[18] loss: 1.525\n",
      "[19] loss: 1.524\n",
      "[20] loss: 1.522\n",
      "[21] loss: 1.503\n",
      "[22] loss: 1.502\n",
      "[23] loss: 1.488\n",
      "[24] loss: 1.484\n",
      "[25] loss: 1.474\n",
      "[26] loss: 1.465\n",
      "[27] loss: 1.463\n",
      "[28] loss: 1.454\n",
      "[29] loss: 1.444\n",
      "[30] loss: 1.439\n",
      "[31] loss: 1.429\n",
      "[32] loss: 1.425\n",
      "[33] loss: 1.421\n",
      "[34] loss: 1.414\n",
      "[35] loss: 1.407\n",
      "[36] loss: 1.391\n",
      "[37] loss: 1.396\n",
      "[38] loss: 1.371\n",
      "[39] loss: 1.370\n",
      "[40] loss: 1.370\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 15\n",
      "[1] loss: 2.003\n",
      "[2] loss: 1.863\n",
      "[3] loss: 1.817\n",
      "[4] loss: 1.765\n",
      "[5] loss: 1.724\n",
      "[6] loss: 1.683\n",
      "[7] loss: 1.646\n",
      "[8] loss: 1.623\n",
      "[9] loss: 1.589\n",
      "[10] loss: 1.570\n",
      "[11] loss: 1.543\n",
      "[12] loss: 1.535\n",
      "[13] loss: 1.511\n",
      "[14] loss: 1.488\n",
      "[15] loss: 1.472\n",
      "[16] loss: 1.450\n",
      "[17] loss: 1.436\n",
      "[18] loss: 1.418\n",
      "[19] loss: 1.403\n",
      "[20] loss: 1.386\n",
      "[21] loss: 1.368\n",
      "[22] loss: 1.354\n",
      "[23] loss: 1.342\n",
      "[24] loss: 1.328\n",
      "[25] loss: 1.315\n",
      "[26] loss: 1.299\n",
      "[27] loss: 1.276\n",
      "[28] loss: 1.266\n",
      "[29] loss: 1.250\n",
      "[30] loss: 1.233\n",
      "[31] loss: 1.229\n",
      "[32] loss: 1.210\n",
      "[33] loss: 1.194\n",
      "[34] loss: 1.183\n",
      "[35] loss: 1.174\n",
      "[36] loss: 1.162\n",
      "[37] loss: 1.150\n",
      "[38] loss: 1.130\n",
      "[39] loss: 1.125\n",
      "[40] loss: 1.112\n",
      "Finished training!\n",
      "\n",
      "Training Incremental BN. Run 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.024\n",
      "[2] loss: 1.904\n",
      "[3] loss: 1.872\n",
      "[4] loss: 1.848\n",
      "[5] loss: 1.843\n",
      "[6] loss: 1.815\n",
      "[7] loss: 1.789\n",
      "[8] loss: 1.773\n",
      "[9] loss: 1.750\n",
      "[10] loss: 1.733\n",
      "[11] loss: 1.706\n",
      "[12] loss: 1.690\n",
      "[13] loss: 1.663\n",
      "[14] loss: 1.642\n",
      "[15] loss: 1.628\n",
      "[16] loss: 1.611\n",
      "[17] loss: 1.593\n",
      "[18] loss: 1.573\n",
      "[19] loss: 1.561\n",
      "[20] loss: 1.552\n",
      "[21] loss: 1.540\n",
      "[22] loss: 1.527\n",
      "[23] loss: 1.518\n",
      "[24] loss: 1.502\n",
      "[25] loss: 1.490\n",
      "[26] loss: 1.477\n",
      "[27] loss: 1.462\n",
      "[28] loss: 1.452\n",
      "[29] loss: 1.445\n",
      "[30] loss: 1.433\n",
      "[31] loss: 1.423\n",
      "[32] loss: 1.412\n",
      "[33] loss: 1.403\n",
      "[34] loss: 1.389\n",
      "[35] loss: 1.375\n",
      "[36] loss: 1.368\n",
      "[37] loss: 1.360\n",
      "[38] loss: 1.339\n",
      "[39] loss: 1.331\n",
      "[40] loss: 1.319\n",
      "Finished training!\n",
      "\n",
      "Training Infomax Net. Run 15\n",
      "[1] loss: 2.138\n",
      "[2] loss: 2.127\n",
      "[3] loss: 2.154\n",
      "[4] loss: 2.150\n",
      "[5] loss: 2.155\n",
      "[6] loss: 2.182\n",
      "[7] loss: 2.204\n",
      "[8] loss: 2.183\n",
      "[9] loss: 2.220\n",
      "[10] loss: 2.237\n",
      "[11] loss: 2.227\n",
      "[12] loss: 2.252\n",
      "[13] loss: 2.231\n",
      "[14] loss: 2.228\n",
      "[15] loss: 2.233\n",
      "[16] loss: 2.245\n",
      "[17] loss: 2.236\n",
      "[18] loss: 2.239\n",
      "[19] loss: 2.212\n",
      "[20] loss: 2.225\n",
      "[21] loss: 2.237\n",
      "[22] loss: 2.257\n",
      "[23] loss: 2.236\n",
      "[24] loss: 2.252\n",
      "[25] loss: 2.242\n",
      "[26] loss: 2.235\n",
      "[27] loss: 2.217\n",
      "[28] loss: 2.185\n",
      "[29] loss: 2.204\n",
      "[30] loss: 2.255\n",
      "[31] loss: 2.247\n",
      "[32] loss: 2.230\n",
      "[33] loss: 2.212\n",
      "[34] loss: 2.209\n",
      "[35] loss: 2.211\n",
      "[36] loss: 2.202\n",
      "[37] loss: 2.203\n",
      "[38] loss: 2.201\n",
      "[39] loss: 2.221\n",
      "[40] loss: 2.211\n",
      "Finished training!\n",
      "\n",
      "Training Standard Net. Run 15\n",
      "[1] loss: 2.060\n",
      "[2] loss: 1.902\n",
      "[3] loss: 1.833\n",
      "[4] loss: 1.788\n",
      "[5] loss: 1.754\n",
      "[6] loss: 1.735\n",
      "[7] loss: 1.718\n",
      "[8] loss: 1.698\n",
      "[9] loss: 1.669\n",
      "[10] loss: 1.653\n",
      "[11] loss: 1.648\n",
      "[12] loss: 1.625\n",
      "[13] loss: 1.606\n",
      "[14] loss: 1.602\n",
      "[15] loss: 1.573\n",
      "[16] loss: 1.557\n",
      "[17] loss: 1.554\n",
      "[18] loss: 1.544\n",
      "[19] loss: 1.535\n",
      "[20] loss: 1.530\n",
      "[21] loss: 1.511\n",
      "[22] loss: 1.518\n",
      "[23] loss: 1.503\n",
      "[24] loss: 1.496\n",
      "[25] loss: 1.489\n",
      "[26] loss: 1.478\n",
      "[27] loss: 1.473\n",
      "[28] loss: 1.457\n",
      "[29] loss: 1.453\n",
      "[30] loss: 1.449\n",
      "[31] loss: 1.444\n",
      "[32] loss: 1.436\n",
      "[33] loss: 1.428\n",
      "[34] loss: 1.422\n",
      "[35] loss: 1.421\n",
      "[36] loss: 1.412\n",
      "[37] loss: 1.405\n",
      "[38] loss: 1.409\n",
      "[39] loss: 1.384\n",
      "[40] loss: 1.384\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3bee618e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 150\n",
    "epochs = 40\n",
    "\n",
    "test_runs = 15\n",
    "\n",
    "int_lr = 0.03\n",
    "syn_lr = 0.001\n",
    "\n",
    "run_cifar_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
