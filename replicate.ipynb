{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))\n",
    "\n",
    "# for i, data in enumerate(trainloader, 0):\n",
    "#     print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def show_batch(batch):\n",
    "#     im = torchvision.utils.make_grid(batch)\n",
    "#     plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# print('Labels: ', labels)\n",
    "# print('Batch shape: ', images.size())\n",
    "# show_batch(images)\n",
    "\n",
    "# images.view(batch_size, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUTSIZE = 28*28\n",
    "NBLAYERS = 9\n",
    "LAYERSIZE = 25\n",
    "\n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if (self.testing is False):\n",
    "            mu = x.mean(1, keepdim=True)\n",
    "            sigma = ((x-mu)**2).mean(1, keepdim=True).sqrt()\n",
    "            \n",
    "            # Normalize\n",
    "            nx = (x-mu)/sigma\n",
    "            \n",
    "            # Adjust using learned parameters\n",
    "            o = self.gain*nx + self.bias\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Testing mode batch normalization has not been implemented\")\n",
    "\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize):\n",
    "        super(IP, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        self.alpha = nn.Buffer(torch.ones(layersize))\n",
    "        self.beta = nn.Buffer(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        \n",
    "    def update(self, u, v):\n",
    "        \n",
    "        Eu\n",
    "        \n",
    "                  \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, layer1size)\n",
    "        self.fc2 = nn.Linear(layer1size, layer2size)\n",
    "        self.fc3 = nn.Linear(layer2size, layer3size)\n",
    "        \n",
    "#         self.ILR = nn.Parameter(torch.tensor(0.001)) # Unnecessary?\n",
    "        \n",
    "        self.gain1 = nn.Parameter(torch.ones(layer1size))\n",
    "        self.gain2 = nn.Parameter(torch.ones(layer2size))\n",
    "        self.gain3 = nn.Parameter(torch.ones(layer3size))\n",
    "        \n",
    "        self.bias1 = nn.Parameter(torch.zeros(layer1size))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(layer2size))\n",
    "        self.bias3 = nn.Parameter(torch.zeros(layer3size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc1(x.view(batch_size, -1))\n",
    "        y1 = F.tanh(self.gain1.data * u1 + self.bias1.data)\n",
    "        u2 = self.fc2(y1)\n",
    "        y2 = F.tanh(self.gain2.data * u2 + self.bias2.data)\n",
    "        u3 = self.fc3(y2)\n",
    "        y3 = F.relu(self.gain3.data * u3 + self.bias3.data)\n",
    "        return u1, y1, u2, y2, u3, y3\n",
    "    \n",
    "dlayersize = 100\n",
    "    \n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, IP=False):\n",
    "        super(DNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, dlayersize)\n",
    "        self.fc2 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc3 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc4 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc5 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc6 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc7 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc8 = nn.Linear(dlayersize, dlayersize)\n",
    "        self.fc9 = nn.Linear(dlayersize, 10)\n",
    "        \n",
    "        self.gain1 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain2 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain3 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain4 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain5 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain6 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain7 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain8 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "        self.gain9 = nn.Parameter(torch.ones(10), requires_grad=False)\n",
    "        \n",
    "        self.bias1 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias2 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias3 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias4 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias5 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias6 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias7 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias8 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "        self.bias9 = nn.Parameter(torch.zeros(10), requires_grad=False)\n",
    "        \n",
    "        if (IP == True):\n",
    "            self.alpha1 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha2 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha3 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha4 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha5 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha6 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha7 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha8 = nn.Parameter(torch.ones(dlayersize), requires_grad=True)\n",
    "            self.alpha9 = nn.Parameter(torch.ones(10), requires_grad=True)\n",
    "\n",
    "            self.beta1 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta2 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta3 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta4 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta5 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta6 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta7 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta8 = nn.Parameter(torch.zeros(dlayersize), requires_grad=True)\n",
    "            self.beta9 = nn.Parameter(torch.zeros(10), requires_grad=True)\n",
    "            \n",
    "        if (IP == False):\n",
    "            self.alpha1 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha2 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha3 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha4 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha5 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha6 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha7 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha8 = nn.Parameter(torch.ones(dlayersize), requires_grad=False)\n",
    "            self.alpha9 = nn.Parameter(torch.ones(10), requires_grad=False)\n",
    "\n",
    "            self.beta1 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta2 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta3 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta4 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta5 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta6 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta7 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta8 = nn.Parameter(torch.zeros(dlayersize), requires_grad=False)\n",
    "            self.beta9 = nn.Parameter(torch.zeros(10), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc1(x.view(batch_size, -1))\n",
    "        y1 = F.tanh(self.alpha1.data*(1/self.gain1.data * (u1 - self.bias1.data)) + self.beta1.data)\n",
    "        u2 = self.fc2(y1)\n",
    "        y2 = F.tanh(self.alpha2.data*(1/self.gain2.data * (u2 - self.bias2.data)) + self.beta2.data)\n",
    "        u3 = self.fc3(y2)\n",
    "        y3 = F.relu(self.alpha3.data*(1/self.gain3.data * (u3 - self.bias3.data)) + self.beta3.data)\n",
    "        u4 = self.fc4(y3)\n",
    "        y4 = F.relu(self.alpha4.data*(1/self.gain4.data * (u4 - self.bias4.data)) + self.beta4.data)\n",
    "        u5 = self.fc5(y4)\n",
    "        y5 = F.relu(self.alpha5.data*(1/self.gain5.data * (u5 - self.bias5.data)) + self.beta5.data)\n",
    "        u6 = self.fc6(y5)\n",
    "        y6 = F.relu(self.alpha6.data*(1/self.gain6.data * (u6 - self.bias6.data)) + self.beta6.data)\n",
    "        u7 = self.fc7(y6)\n",
    "        y7 = F.relu(self.alpha7.data*(1/self.gain7.data * (u7 - self.bias7.data)) + self.beta7.data)\n",
    "        u8 = self.fc8(y7)\n",
    "        y8 = F.relu(self.alpha8.data*(1/self.gain8.data * (u8 - self.bias8.data)) + self.beta8.data)\n",
    "        u9 = self.fc9(y8)\n",
    "        y9 = F.relu(self.alpha9.data*(1/self.gain9.data * (u9 - self.bias9.data)) + self.beta9.data)\n",
    "        return u1, y1, u2, y2, u3, y3, u4, y4, u5, y5, u6, y6, u7, y7, u8, y8, u9, y9\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "def train_deep_model(network, optimization, seed, IP=True):\n",
    "    torch.manual_seed(seed)\n",
    "#     print(\"Training\", str(network))\n",
    "    lambda1 = lambda epoch: epoch\n",
    "    lambda2 = lambda epoch: 0.996 ** epoch\n",
    "    eta = 0.003\n",
    "    scheduler = LR.LambdaLR(optimization, lr_lambda=lambda2)\n",
    "    for epoch in range(30):  # loop over the dataset multiple times\n",
    "        scheduler.step()\n",
    "        if(IP):\n",
    "            Eu1 = torch.zeros(dlayersize).to(device)\n",
    "            Ey1 = torch.zeros(dlayersize).to(device)\n",
    "            Euy1 = torch.zeros(dlayersize).to(device)\n",
    "            Eu2 = torch.zeros(dlayersize).to(device)\n",
    "            Ey2 = torch.zeros(dlayersize).to(device)\n",
    "            Euy2 = torch.zeros(dlayersize).to(device)\n",
    "            Eu3 = torch.zeros(dlayersize).to(device)\n",
    "            Ey3 = torch.zeros(dlayersize).to(device)\n",
    "            Euy3 = torch.zeros(dlayersize).to(device)\n",
    "            Eu4 = torch.zeros(dlayersize).to(device)\n",
    "            Ey4 = torch.zeros(dlayersize).to(device)\n",
    "            Euy4 = torch.zeros(dlayersize).to(device)\n",
    "            Eu5 = torch.zeros(dlayersize).to(device)\n",
    "            Ey5 = torch.zeros(dlayersize).to(device)\n",
    "            Euy5 = torch.zeros(dlayersize).to(device)\n",
    "            Eu6 = torch.zeros(dlayersize).to(device)\n",
    "            Ey6 = torch.zeros(dlayersize).to(device)\n",
    "            Euy6 = torch.zeros(dlayersize).to(device)\n",
    "            Eu7 = torch.zeros(dlayersize).to(device)\n",
    "            Ey7 = torch.zeros(dlayersize).to(device)\n",
    "            Euy7 = torch.zeros(dlayersize).to(device)\n",
    "            Eu8 = torch.zeros(dlayersize).to(device)\n",
    "            Ey8 = torch.zeros(dlayersize).to(device)\n",
    "            Euy8 = torch.zeros(dlayersize).to(device)\n",
    "            Eu9 = torch.zeros(10).to(device)\n",
    "            Ey9 = torch.zeros(10).to(device)\n",
    "            Euy9 = torch.zeros(10).to(device)\n",
    "            \n",
    "            Euu1 = torch.zeros(dlayersize).to(device)\n",
    "            Euu2 = torch.zeros(dlayersize).to(device)\n",
    "            Euu3 = torch.zeros(dlayersize).to(device)\n",
    "            Euu4 = torch.zeros(dlayersize).to(device)\n",
    "            Euu5 = torch.zeros(dlayersize).to(device)\n",
    "            Euu6 = torch.zeros(dlayersize).to(device)\n",
    "            Euu7 = torch.zeros(dlayersize).to(device)\n",
    "            Euu8 = torch.zeros(dlayersize).to(device)\n",
    "            Euu9 = torch.zeros(10).to(device)\n",
    "            \n",
    "            Eyy1 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy2 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy3 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy4 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy5 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy6 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy7 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy8 = torch.zeros(dlayersize).to(device)\n",
    "            Eyy9 = torch.zeros(10).to(device)\n",
    "            \n",
    "            iter_num = 0\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            if(IP):\n",
    "                iter_num = iter_num+1\n",
    "\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            u1, y1, u2, y2, u3, y3, u4, y4, u5, y5, u6, y6, u7, y7, u8, y8, u9, y9 = network(inputs)\n",
    "            loss = criterion(y9, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "            \n",
    "            if(IP):\n",
    "                Eu1 = Eu1 + u1/batch_size\n",
    "                Ey1 = Ey1 + y1/batch_size\n",
    "                Euy1 = Euy1 + u1*y1/batch_size\n",
    "                Eu2 = Eu2 + u2/batch_size\n",
    "                Ey2 = Ey2 + y2/batch_size\n",
    "                Euy2 = Euy2 + u2*y2/batch_size\n",
    "                Eu3 = Eu3 + u3/batch_size\n",
    "                Ey3 = Ey3 + y3/batch_size\n",
    "                Euy3 = Euy3 + u3*y3/batch_size\n",
    "                Eu4 = Eu4 + u4/batch_size\n",
    "                Ey4 = Ey4 + y4/batch_size\n",
    "                Euy4 = Euy4 + u4*y4/batch_size\n",
    "                Eu5 = Eu5 + u5/batch_size\n",
    "                Ey5 = Ey5 + y5/batch_size\n",
    "                Euy5 = Euy5 + u5*y5/batch_size\n",
    "                Eu6 = Eu6 + u6/batch_size\n",
    "                Ey6 = Ey6 + y6/batch_size\n",
    "                Euy6 = Euy6 + u6*y6/batch_size\n",
    "                Eu7 = Eu7 + u7/batch_size\n",
    "                Ey7 = Ey7 + y7/batch_size\n",
    "                Euy7 = Euy7 + u7*y7/batch_size\n",
    "                Eu8 = Eu8 + u8/batch_size\n",
    "                Ey8 = Ey8 + y8/batch_size\n",
    "                Euy8 = Euy8 + u8*y8/batch_size\n",
    "                Eu9 = Eu9 + u9/batch_size\n",
    "                Ey9 = Ey9 + y9/batch_size\n",
    "                Euy9 = Euy9 + u9*y9/batch_size\n",
    "                \n",
    "                Euu1 = Euu1 + u1*u1/batch_size\n",
    "                Euu2 = Euu2 + u2*u2/batch_size\n",
    "                Euu3 = Euu3 + u3*u3/batch_size\n",
    "                Euu4 = Euu4 + u4*u4/batch_size\n",
    "                Euu5 = Euu5 + u5*u5/batch_size\n",
    "                Euu6 = Euu6 + u6*u6/batch_size\n",
    "                Euu7 = Euu7 + u7*u7/batch_size\n",
    "                Euu8 = Euu8 + u8*u8/batch_size\n",
    "                Euu9 = Euu9 + u9*u9/batch_size\n",
    "                \n",
    "                Eyy1 = Eyy1 + y1*y1/batch_size\n",
    "                Eyy2 = Eyy2 + y2*y2/batch_size\n",
    "                Eyy3 = Eyy3 + y3*y3/batch_size\n",
    "                Eyy4 = Eyy4 + y4*y4/batch_size\n",
    "                Eyy5 = Eyy5 + y5*y5/batch_size\n",
    "                Eyy6 = Eyy6 + y6*y6/batch_size\n",
    "                Eyy7 = Eyy7 + y7*y7/batch_size\n",
    "                Eyy8 = Eyy8 + y8*y8/batch_size\n",
    "                Eyy9 = Eyy9 + y9*y9/batch_size\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        if(IP):\n",
    "#             # Original\n",
    "#             network.gain1.data = network.gain1.data + eta * (1/network.gain1.data - 2*Euy1/iter_num - 0.5*network.gain1.data)\n",
    "#             network.gain2.data = network.gain2.data + eta * (1/network.gain2.data - 2*Euy2/iter_num - 0.5*network.gain2.data)\n",
    "#             network.gain3.data = network.gain3.data + eta * (1/network.gain3.data - 2*Euy3/iter_num - 0.5*network.gain3.data)\n",
    "#             network.gain4.data = network.gain4.data + eta * (1/network.gain4.data - 2*Euy4/iter_num - 0.5*network.gain4.data)\n",
    "#             network.gain5.data = network.gain5.data + eta * (1/network.gain5.data - 2*Euy5/iter_num - 0.5*network.gain5.data)\n",
    "#             network.gain6.data = network.gain6.data + eta * (1/network.gain6.data - 2*Euy6/iter_num - 0.5*network.gain6.data)\n",
    "#             network.gain7.data = network.gain7.data + eta * (1/network.gain7.data - 2*Euy7/iter_num - 0.5*network.gain7.data)\n",
    "#             network.gain8.data = network.gain8.data + eta * (1/network.gain8.data - 2*Euy8/iter_num - 0.5*network.gain8.data)\n",
    "#             network.gain9.data = network.gain9.data + eta * (1/network.gain9.data - 2*Euy9/iter_num - 0.5*network.gain9.data)\n",
    "            \n",
    "#             # Covariance\n",
    "#             network.gain1.data = network.gain1.data + eta * (1/network.gain1.data - 2*(Euy1/iter_num - Eu1*Ey1/iter_num))\n",
    "#             network.gain2.data = network.gain2.data + eta * (1/network.gain2.data - 2*(Euy2/iter_num - Eu2*Ey2/iter_num))\n",
    "#             network.gain3.data = network.gain3.data + eta * (1/network.gain3.data - 2*(Euy3/iter_num - Eu3*Ey3/iter_num))\n",
    "#             network.gain4.data = network.gain4.data + eta * (1/network.gain4.data - 2*(Euy4/iter_num - Eu4*Ey4/iter_num))\n",
    "#             network.gain5.data = network.gain5.data + eta * (1/network.gain5.data - 2*(Euy5/iter_num - Eu5*Ey5/iter_num))\n",
    "#             network.gain6.data = network.gain6.data + eta * (1/network.gain6.data - 2*(Euy6/iter_num - Eu6*Ey6/iter_num))\n",
    "#             network.gain7.data = network.gain7.data + eta * (1/network.gain7.data - 2*(Euy7/iter_num - Eu7*Ey7/iter_num))\n",
    "#             network.gain8.data = network.gain8.data + eta * (1/network.gain8.data - 2*(Euy8/iter_num - Eu8*Ey8/iter_num))\n",
    "#             network.gain9.data = network.gain9.data + eta * (1/network.gain9.data - 2*(Euy9/iter_num - Eu9*Ey9/iter_num))\n",
    "        \n",
    "            # Variance\n",
    "            network.gain1.data = network.gain1.data + eta * (1/network.gain1.data - 2*(Euu1/iter_num - Eu1*Eu1/iter_num))\n",
    "            network.gain2.data = network.gain2.data + eta * (1/network.gain2.data - 2*(Euu2/iter_num - Eu2*Eu2/iter_num))\n",
    "            network.gain3.data = network.gain3.data + eta * (1/network.gain3.data - 2*(Euu3/iter_num - Eu3*Eu3/iter_num))\n",
    "            network.gain4.data = network.gain4.data + eta * (1/network.gain4.data - 2*(Euu4/iter_num - Eu4*Eu4/iter_num))\n",
    "            network.gain5.data = network.gain5.data + eta * (1/network.gain5.data - 2*(Euu5/iter_num - Eu5*Eu5/iter_num))\n",
    "            network.gain6.data = network.gain6.data + eta * (1/network.gain6.data - 2*(Euu6/iter_num - Eu6*Eu6/iter_num))\n",
    "            network.gain7.data = network.gain7.data + eta * (1/network.gain7.data - 2*(Euu7/iter_num - Eu7*Eu7/iter_num))\n",
    "            network.gain8.data = network.gain8.data + eta * (1/network.gain8.data - 2*(Euu8/iter_num - Eu8*Eu8/iter_num))\n",
    "            network.gain9.data = network.gain9.data + eta * (1/network.gain9.data - 2*(Euu9/iter_num - Eu9*Eu9/iter_num))\n",
    "            \n",
    "            network.bias1.data = network.bias1.data + eta * (-2 * Ey1/iter_num)\n",
    "            network.bias2.data = network.bias2.data + eta * (-2 * Ey2/iter_num)\n",
    "            network.bias3.data = network.bias3.data + eta * (-2 * Ey3/iter_num)\n",
    "            network.bias4.data = network.bias4.data + eta * (-2 * Ey4/iter_num)\n",
    "            network.bias5.data = network.bias5.data + eta * (-2 * Ey5/iter_num)\n",
    "            network.bias6.data = network.bias6.data + eta * (-2 * Ey6/iter_num)\n",
    "            network.bias7.data = network.bias7.data + eta * (-2 * Ey7/iter_num)\n",
    "            network.bias8.data = network.bias8.data + eta * (-2 * Ey8/iter_num)\n",
    "            network.bias9.data = network.bias9.data + eta * (-2 * Ey9/iter_num)\n",
    "            \n",
    "            eta = 0.996*eta\n",
    "            \n",
    "    print(\"Finished training!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep IP Net\n",
      "[1,   100] loss: 0.088\n",
      "[1,   200] loss: 0.061\n",
      "[1,   300] loss: 0.058\n",
      "[2,   100] loss: 0.055\n",
      "[2,   200] loss: 0.054\n",
      "[2,   300] loss: 0.055\n",
      "[3,   100] loss: 0.054\n",
      "[3,   200] loss: 0.053\n",
      "[3,   300] loss: 0.053\n",
      "[4,   100] loss: 0.052\n",
      "[4,   200] loss: 0.053\n",
      "[4,   300] loss: 0.052\n",
      "[5,   100] loss: 0.051\n",
      "[5,   200] loss: 0.052\n",
      "[5,   300] loss: 0.052\n",
      "[6,   100] loss: 0.052\n",
      "[6,   200] loss: 0.052\n",
      "[6,   300] loss: 0.051\n",
      "[7,   100] loss: 0.050\n",
      "[7,   200] loss: 0.052\n",
      "[7,   300] loss: 0.051\n",
      "[8,   100] loss: 0.051\n",
      "[8,   200] loss: 0.051\n",
      "[8,   300] loss: 0.050\n",
      "[9,   100] loss: 0.050\n",
      "[9,   200] loss: 0.051\n",
      "[9,   300] loss: 0.050\n",
      "[10,   100] loss: 0.050\n",
      "[10,   200] loss: 0.051\n",
      "[10,   300] loss: 0.051\n",
      "[11,   100] loss: 0.051\n",
      "[11,   200] loss: 0.051\n",
      "[11,   300] loss: 0.051\n",
      "[12,   100] loss: 0.050\n",
      "[12,   200] loss: 0.051\n",
      "[12,   300] loss: 0.050\n",
      "[13,   100] loss: 0.050\n",
      "[13,   200] loss: 0.050\n",
      "[13,   300] loss: 0.051\n",
      "[14,   100] loss: 0.050\n",
      "[14,   200] loss: 0.050\n",
      "[14,   300] loss: 0.050\n",
      "[15,   100] loss: 0.050\n",
      "[15,   200] loss: 0.050\n",
      "[15,   300] loss: 0.050\n",
      "[16,   100] loss: 0.050\n",
      "[16,   200] loss: 0.050\n",
      "[16,   300] loss: 0.050\n",
      "[17,   100] loss: 0.050\n",
      "[17,   200] loss: 0.051\n",
      "[17,   300] loss: 0.050\n",
      "[18,   100] loss: 0.050\n",
      "[18,   200] loss: 0.051\n",
      "[18,   300] loss: 0.049\n",
      "[19,   100] loss: 0.050\n",
      "[19,   200] loss: 0.050\n",
      "[19,   300] loss: 0.050\n",
      "[20,   100] loss: 0.050\n",
      "[20,   200] loss: 0.050\n",
      "[20,   300] loss: 0.050\n",
      "[21,   100] loss: 0.050\n",
      "[21,   200] loss: 0.050\n",
      "[21,   300] loss: 0.049\n",
      "[22,   100] loss: 0.049\n",
      "[22,   200] loss: 0.051\n",
      "[22,   300] loss: 0.050\n",
      "[23,   100] loss: 0.050\n",
      "[23,   200] loss: 0.049\n",
      "[23,   300] loss: 0.050\n",
      "[24,   100] loss: 0.050\n",
      "[24,   200] loss: 0.050\n",
      "[24,   300] loss: 0.050\n",
      "[25,   100] loss: 0.050\n",
      "[25,   200] loss: 0.050\n",
      "[25,   300] loss: 0.049\n",
      "[26,   100] loss: 0.049\n",
      "[26,   200] loss: 0.049\n",
      "[26,   300] loss: 0.051\n",
      "[27,   100] loss: 0.051\n",
      "[27,   200] loss: 0.050\n",
      "[27,   300] loss: 0.050\n",
      "[28,   100] loss: 0.049\n",
      "[28,   200] loss: 0.050\n",
      "[28,   300] loss: 0.050\n",
      "[29,   100] loss: 0.049\n",
      "[29,   200] loss: 0.050\n",
      "[29,   300] loss: 0.050\n",
      "[30,   100] loss: 0.050\n",
      "[30,   200] loss: 0.050\n",
      "[30,   300] loss: 0.052\n",
      "Finished training!\n",
      "\n",
      "Training Deep Standard Net\n",
      "[1,   100] loss: 0.088\n",
      "[1,   200] loss: 0.061\n",
      "[1,   300] loss: 0.058\n",
      "[2,   100] loss: 0.055\n",
      "[2,   200] loss: 0.054\n",
      "[2,   300] loss: 0.055\n",
      "[3,   100] loss: 0.054\n",
      "[3,   200] loss: 0.052\n",
      "[3,   300] loss: 0.053\n",
      "[4,   100] loss: 0.052\n",
      "[4,   200] loss: 0.053\n",
      "[4,   300] loss: 0.052\n",
      "[5,   100] loss: 0.051\n",
      "[5,   200] loss: 0.052\n",
      "[5,   300] loss: 0.052\n",
      "[6,   100] loss: 0.052\n",
      "[6,   200] loss: 0.052\n",
      "[6,   300] loss: 0.052\n",
      "[7,   100] loss: 0.051\n",
      "[7,   200] loss: 0.052\n",
      "[7,   300] loss: 0.051\n",
      "[8,   100] loss: 0.051\n",
      "[8,   200] loss: 0.051\n",
      "[8,   300] loss: 0.050\n",
      "[9,   100] loss: 0.050\n",
      "[9,   200] loss: 0.051\n",
      "[9,   300] loss: 0.051\n",
      "[10,   100] loss: 0.050\n",
      "[10,   200] loss: 0.051\n",
      "[10,   300] loss: 0.050\n",
      "[11,   100] loss: 0.051\n",
      "[11,   200] loss: 0.051\n",
      "[11,   300] loss: 0.050\n",
      "[12,   100] loss: 0.051\n",
      "[12,   200] loss: 0.051\n",
      "[12,   300] loss: 0.050\n",
      "[13,   100] loss: 0.051\n",
      "[13,   200] loss: 0.050\n",
      "[13,   300] loss: 0.051\n",
      "[14,   100] loss: 0.050\n",
      "[14,   200] loss: 0.050\n",
      "[14,   300] loss: 0.050\n",
      "[15,   100] loss: 0.050\n",
      "[15,   200] loss: 0.051\n",
      "[15,   300] loss: 0.051\n",
      "[16,   100] loss: 0.050\n",
      "[16,   200] loss: 0.050\n",
      "[16,   300] loss: 0.050\n",
      "[17,   100] loss: 0.050\n",
      "[17,   200] loss: 0.051\n",
      "[17,   300] loss: 0.049\n",
      "[18,   100] loss: 0.050\n",
      "[18,   200] loss: 0.051\n",
      "[18,   300] loss: 0.049\n",
      "[19,   100] loss: 0.050\n",
      "[19,   200] loss: 0.050\n",
      "[19,   300] loss: 0.050\n",
      "[20,   100] loss: 0.051\n",
      "[20,   200] loss: 0.051\n",
      "[20,   300] loss: 0.051\n",
      "[21,   100] loss: 0.051\n",
      "[21,   200] loss: 0.050\n",
      "[21,   300] loss: 0.050\n",
      "[22,   100] loss: 0.050\n",
      "[22,   200] loss: 0.051\n",
      "[22,   300] loss: 0.050\n",
      "[23,   100] loss: 0.049\n",
      "[23,   200] loss: 0.049\n",
      "[23,   300] loss: 0.050\n",
      "[24,   100] loss: 0.049\n",
      "[24,   200] loss: 0.050\n",
      "[24,   300] loss: 0.045\n",
      "[25,   100] loss: 0.044\n",
      "[25,   200] loss: 0.043\n",
      "[25,   300] loss: 0.041\n",
      "[26,   100] loss: 0.040\n",
      "[26,   200] loss: 0.040\n",
      "[26,   300] loss: 0.041\n",
      "[27,   100] loss: 0.040\n",
      "[27,   200] loss: 0.040\n",
      "[27,   300] loss: 0.040\n",
      "[28,   100] loss: 0.040\n",
      "[28,   200] loss: 0.040\n",
      "[28,   300] loss: 0.040\n",
      "[29,   100] loss: 0.040\n",
      "[29,   200] loss: 0.040\n",
      "[29,   300] loss: 0.040\n",
      "[30,   100] loss: 0.041\n",
      "[30,   200] loss: 0.041\n",
      "[30,   300] loss: 0.039\n",
      "Finished training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 986\n",
    "\n",
    "# #Train IP Model\n",
    "# torch.manual_seed(seed)\n",
    "# IPnet = Net()\n",
    "# IPnet = IPnet.to(device)\n",
    "\n",
    "# optimizer1 = optim.Adam(IPnet.parameters(), lr=0.001)\n",
    "# print(\"Training IP Net\")\n",
    "# train_model(IPnet, optimizer1, seed, True)\n",
    "\n",
    "# #Train Standard Model\n",
    "# torch.manual_seed(seed)\n",
    "# net = Net()\n",
    "# net = net.to(device)\n",
    "\n",
    "# optimizer2 = optim.Adam(net.parameters(), lr=0.001)\n",
    "# print(\"Training Standard Net\")\n",
    "# train_model(net, optimizer2, seed, False)\n",
    "\n",
    "#Train Deep IP Model\n",
    "torch.manual_seed(seed)\n",
    "DIPnet = DNet(IP=True)\n",
    "DIPnet = DIPnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, DIPnet.parameters()), lr=3e-4)\n",
    "print(\"Training Deep IP Net\")\n",
    "train_deep_model(DIPnet, optimizer, seed, True)\n",
    "\n",
    "#Train Deep Standard Model\n",
    "torch.manual_seed(seed)\n",
    "Dnet = DNet(IP=False)\n",
    "Dnet = Dnet.to(device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, Dnet.parameters()), lr=3e-4)\n",
    "print(\"Training Deep Standard Net\")\n",
    "train_deep_model(Dnet, optimizer, seed, False)\n",
    "\n",
    "# for param in IPnet.parameters():\n",
    "#     print(param.data)\n",
    "    \n",
    "# for param in IPnet.parameters():\n",
    "#     print(param.data)\n",
    "\n",
    "# c_net = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the IP network on the 10000 test images: 66 %\n",
      "Accuracy of the standard network on the 10000 test images: 75 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        u1, y1, u2, y2, u3, y3, u4, y4, u5, y5, u6, y6, u7, y7, u8, y8, u9, y9  = DIPnet(images)\n",
    "        _, predicted = torch.max(y9.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the IP network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        u1, y1, u2, y2, u3, y3, u4, y4, u5, y5, u6, y6, u7, y7, u8, y8, u9, y9  = Dnet(images)\n",
    "        _, predicted = torch.max(y9.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the standard network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# val1, ind1 = DIPnet.fc1.weight.max(0)\n",
    "# max_weight = val1.max(0)\n",
    "# print(max_weight)\n",
    "\n",
    "# val1, ind1 = Dnet.fc1.weight.max(0)\n",
    "# max_weight = val1.max(0)\n",
    "# print(max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
