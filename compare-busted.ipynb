{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "#         self.gain = nn.Parameter(torch.ones(layersize))\n",
    "#         self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*(Euv))\n",
    "        self.beta = (1-eta)*self.beta + eta * (4*Ev*Euv)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "class IP2(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP2, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "#         self.gain = nn.Parameter(torch.ones(layersize))\n",
    "#         self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*(Euv))\n",
    "        self.beta = (1-eta)*self.beta + eta * (2*Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "class IP3(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP3, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "#         self.gain = nn.Parameter(torch.ones(layersize))\n",
    "#         self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        return  nx\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (Euv)\n",
    "        self.beta = (1-eta)*self.beta + eta * (Ev)\n",
    "        \n",
    "#         self.eta = eta * 0.998\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(DNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o\n",
    "    \n",
    "class CDNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(CDNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(3*32*32, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, layersize)\n",
    "        self.fc4 = nn.Linear(layersize, layersize)\n",
    "        self.fc5 = nn.Linear(layersize, layersize)\n",
    "        self.fc6 = nn.Linear(layersize, layersize)\n",
    "        self.fc7 = nn.Linear(layersize, layersize)\n",
    "        self.fc8 = nn.Linear(layersize, layersize)\n",
    "        self.fc9 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        self.n3 = norm(layersize, eta)\n",
    "        self.n4 = norm(layersize, eta)\n",
    "        self.n5 = norm(layersize, eta)\n",
    "        self.n6 = norm(layersize, eta)\n",
    "        self.n7 = norm(layersize, eta)\n",
    "        self.n8 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        u3 = self.fc3(v2)\n",
    "        v3 = F.tanh(self.n3(u3))\n",
    "        u4 = self.fc4(v3)\n",
    "        v4 = F.tanh(self.n4(u4))\n",
    "        u5 = self.fc5(v4)\n",
    "        v5 = F.tanh(self.n5(u5))\n",
    "        u6 = self.fc6(v5)\n",
    "        v6 = F.tanh(self.n6(u6))\n",
    "        u7 = self.fc7(v6)\n",
    "        v7 = F.tanh(self.n7(u7))\n",
    "        u8 = self.fc8(v7)\n",
    "        v8 = F.tanh(self.n8(u8))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc9(v8))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        self.n3.update(u3, v3, eta)\n",
    "        self.n4.update(u4, v4, eta)\n",
    "        self.n5.update(u5, v5, eta)\n",
    "        self.n6.update(u6, v6, eta)\n",
    "        self.n7.update(u7, v7, eta)\n",
    "        self.n8.update(u8, v8, eta)\n",
    "        \n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = DNet(LAYERSIZE, IP2, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Busted IP. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "    \n",
    "    #Train IP3\n",
    "    torch.manual_seed(seed)\n",
    "    net3 = DNet(LAYERSIZE, IP3, eta=int_lr)\n",
    "    net3 = net3.to(device)\n",
    "\n",
    "    optimizer3 = optim.Adam(net3.parameters(), lr=syn_lr)\n",
    "    print(\"Training Centered. Run 1\")\n",
    "    ip3_losses = train_deep_model(net3, optimizer3, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = DNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = DNet(LAYERSIZE, IP2, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Busted IP. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "        \n",
    "        #Train IP3\n",
    "        torch.manual_seed(seed)\n",
    "        net3 = DNet(LAYERSIZE, IP3, eta=int_lr)\n",
    "        net3 = net3.to(device)\n",
    "\n",
    "        optimizer3 = optim.Adam(net3.parameters(), lr=syn_lr)\n",
    "        print(\"Training Centered. Run %d\" % (i+2))\n",
    "        ip3_losses += train_deep_model(net3, optimizer3, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "    ip3_losses = ip3_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Busted\")\n",
    "    plt.plot(ip3_losses[0], ip3_losses[1], label=\"IP3\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cifar_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP Net. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "    \n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = CDNet(LAYERSIZE, IP2, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training Busted. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "    \n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    net3 = CDNet(LAYERSIZE, IP3, eta=int_lr)\n",
    "    net3 = net3.to(device)\n",
    "\n",
    "    optimizer3 = optim.Adam(net3.parameters(), lr=syn_lr)\n",
    "    print(\"Training IP3 Net. Run %d\" % (1))\n",
    "    ip3_losses = train_deep_model(net3, optimizer3, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = CDNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP Net. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "        \n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = CDNet(LAYERSIZE, IP2, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training Busted. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        net3 = CDNet(LAYERSIZE, IP3, eta=int_lr)\n",
    "        net3 = net3.to(device)\n",
    "\n",
    "        optimizer3 = optim.Adam(net3.parameters(), lr=syn_lr)\n",
    "        print(\"Training IP3 Net. Run %d\" % (i+2))\n",
    "        ip3_losses = train_deep_model(net3, optimizer3, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "    ip3_losses = ip3_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.ylim([-0.1, 3])\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"IP\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Busted\")\n",
    "    plt.plot(ip3_losses[0], ip3_losses[1], label=\"IP3\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 0.625\n",
      "[2] loss: 0.329\n",
      "[3] loss: 0.263\n",
      "[4] loss: 0.233\n",
      "[5] loss: 0.206\n",
      "[6] loss: 0.187\n",
      "[7] loss: 0.174\n",
      "[8] loss: 0.163\n",
      "[9] loss: 0.153\n",
      "[10] loss: 0.145\n",
      "[11] loss: 0.138\n",
      "[12] loss: 0.134\n",
      "[13] loss: 0.127\n",
      "[14] loss: 0.125\n",
      "[15] loss: 0.118\n",
      "[16] loss: 0.115\n",
      "[17] loss: 0.109\n",
      "[18] loss: 0.106\n",
      "[19] loss: 0.101\n",
      "[20] loss: 0.102\n",
      "Finished training!\n",
      "\n",
      "Training Busted IP. Run 1\n",
      "[1] loss: 0.654\n",
      "[2] loss: 0.296\n",
      "[3] loss: 0.237\n",
      "[4] loss: 0.205\n",
      "[5] loss: 0.182\n",
      "[6] loss: 0.164\n",
      "[7] loss: 0.153\n",
      "[8] loss: 0.143\n",
      "[9] loss: 0.132\n",
      "[10] loss: 0.122\n",
      "[11] loss: 0.115\n",
      "[12] loss: 0.108\n",
      "[13] loss: 0.103\n",
      "[14] loss: 0.100\n",
      "[15] loss: 0.094\n",
      "[16] loss: 0.091\n",
      "[17] loss: 0.083\n",
      "[18] loss: 0.082\n",
      "[19] loss: 0.078\n",
      "[20] loss: 0.075\n",
      "Finished training!\n",
      "\n",
      "Training Centered. Run 1\n",
      "[1] loss: 0.461\n",
      "[2] loss: 0.207\n",
      "[3] loss: 0.162\n",
      "[4] loss: 0.132\n",
      "[5] loss: 0.116\n",
      "[6] loss: 0.104\n",
      "[7] loss: 0.093\n",
      "[8] loss: 0.087\n",
      "[9] loss: 0.078\n",
      "[10] loss: 0.071\n",
      "[11] loss: 0.069\n",
      "[12] loss: 0.059\n",
      "[13] loss: 0.059\n",
      "[14] loss: 0.056\n",
      "[15] loss: 0.052\n",
      "[16] loss: 0.048\n",
      "[17] loss: 0.046\n",
      "[18] loss: 0.042\n",
      "[19] loss: 0.039\n",
      "[20] loss: 0.041\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 0.657\n",
      "[2] loss: 0.371\n",
      "[3] loss: 0.295\n",
      "[4] loss: 0.250\n",
      "[5] loss: 0.224\n",
      "[6] loss: 0.205\n",
      "[7] loss: 0.188\n",
      "[8] loss: 0.177\n",
      "[9] loss: 0.170\n",
      "[10] loss: 0.160\n",
      "[11] loss: 0.157\n",
      "[12] loss: 0.151\n",
      "[13] loss: 0.145\n",
      "[14] loss: 0.139\n",
      "[15] loss: 0.136\n",
      "[16] loss: 0.129\n",
      "[17] loss: 0.125\n",
      "[18] loss: 0.124\n",
      "[19] loss: 0.116\n",
      "[20] loss: 0.118\n",
      "Finished training!\n",
      "\n",
      "Training Busted IP. Run 2\n",
      "[1] loss: 0.614\n",
      "[2] loss: 0.299\n",
      "[3] loss: 0.240\n",
      "[4] loss: 0.199\n",
      "[5] loss: 0.172\n",
      "[6] loss: 0.153\n",
      "[7] loss: 0.140\n",
      "[8] loss: 0.128\n",
      "[9] loss: 0.119\n",
      "[10] loss: 0.112\n",
      "[11] loss: 0.103\n",
      "[12] loss: 0.099\n",
      "[13] loss: 0.092\n",
      "[14] loss: 0.088\n",
      "[15] loss: 0.086\n",
      "[16] loss: 0.079\n",
      "[17] loss: 0.075\n",
      "[18] loss: 0.074\n",
      "[19] loss: 0.070\n",
      "[20] loss: 0.068\n",
      "Finished training!\n",
      "\n",
      "Training Centered. Run 2\n",
      "[1] loss: 0.449\n",
      "[2] loss: 0.200\n",
      "[3] loss: 0.156\n",
      "[4] loss: 0.132\n",
      "[5] loss: 0.114\n",
      "[6] loss: 0.101\n",
      "[7] loss: 0.092\n",
      "[8] loss: 0.084\n",
      "[9] loss: 0.076\n",
      "[10] loss: 0.070\n",
      "[11] loss: 0.067\n",
      "[12] loss: 0.061\n",
      "[13] loss: 0.058\n",
      "[14] loss: 0.056\n",
      "[15] loss: 0.052\n",
      "[16] loss: 0.048\n",
      "[17] loss: 0.044\n",
      "[18] loss: 0.043\n",
      "[19] loss: 0.044\n",
      "[20] loss: 0.041\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 3\n",
      "[1] loss: 0.687\n",
      "[2] loss: 0.379\n",
      "[3] loss: 0.310\n",
      "[4] loss: 0.261\n",
      "[5] loss: 0.234\n",
      "[6] loss: 0.216\n",
      "[7] loss: 0.203\n",
      "[8] loss: 0.188\n",
      "[9] loss: 0.181\n",
      "[10] loss: 0.173\n",
      "[11] loss: 0.168\n",
      "[12] loss: 0.158\n",
      "[13] loss: 0.157\n",
      "[14] loss: 0.148\n",
      "[15] loss: 0.146\n",
      "[16] loss: 0.140\n",
      "[17] loss: 0.134\n",
      "[18] loss: 0.134\n",
      "[19] loss: 0.130\n",
      "[20] loss: 0.124\n",
      "Finished training!\n",
      "\n",
      "Training Busted IP. Run 3\n",
      "[1] loss: 0.591\n",
      "[2] loss: 0.313\n",
      "[3] loss: 0.246\n",
      "[4] loss: 0.204\n",
      "[5] loss: 0.176\n",
      "[6] loss: 0.157\n",
      "[7] loss: 0.141\n",
      "[8] loss: 0.132\n",
      "[9] loss: 0.121\n",
      "[10] loss: 0.114\n",
      "[11] loss: 0.108\n",
      "[12] loss: 0.100\n",
      "[13] loss: 0.096\n",
      "[14] loss: 0.094\n",
      "[15] loss: 0.088\n",
      "[16] loss: 0.084\n",
      "[17] loss: 0.081\n",
      "[18] loss: 0.078\n",
      "[19] loss: 0.074\n",
      "[20] loss: 0.071\n",
      "Finished training!\n",
      "\n",
      "Training Centered. Run 3\n",
      "[1] loss: 0.463\n",
      "[2] loss: 0.205\n",
      "[3] loss: 0.162\n",
      "[4] loss: 0.133\n",
      "[5] loss: 0.118\n",
      "[6] loss: 0.104\n",
      "[7] loss: 0.095\n",
      "[8] loss: 0.085\n",
      "[9] loss: 0.079\n",
      "[10] loss: 0.072\n",
      "[11] loss: 0.065\n",
      "[12] loss: 0.061\n",
      "[13] loss: 0.061\n",
      "[14] loss: 0.055\n",
      "[15] loss: 0.050\n",
      "[16] loss: 0.048\n",
      "[17] loss: 0.045\n",
      "[18] loss: 0.044\n",
      "[19] loss: 0.041\n",
      "[20] loss: 0.038\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 4\n",
      "[1] loss: 0.894\n",
      "[2] loss: 0.472\n",
      "[3] loss: 0.336\n",
      "[4] loss: 0.270\n",
      "[5] loss: 0.234\n",
      "[6] loss: 0.217\n",
      "[7] loss: 0.197\n",
      "[8] loss: 0.185\n",
      "[9] loss: 0.171\n",
      "[10] loss: 0.161\n",
      "[11] loss: 0.158\n",
      "[12] loss: 0.154\n",
      "[13] loss: 0.144\n",
      "[14] loss: 0.139\n",
      "[15] loss: 0.133\n",
      "[16] loss: 0.130\n",
      "[17] loss: 0.124\n",
      "[18] loss: 0.118\n",
      "[19] loss: 0.116\n",
      "[20] loss: 0.115\n",
      "Finished training!\n",
      "\n",
      "Training Busted IP. Run 4\n",
      "[1] loss: 0.722\n",
      "[2] loss: 0.354\n",
      "[3] loss: 0.273\n",
      "[4] loss: 0.220\n",
      "[5] loss: 0.186\n",
      "[6] loss: 0.167\n",
      "[7] loss: 0.150\n",
      "[8] loss: 0.138\n",
      "[9] loss: 0.128\n",
      "[10] loss: 0.121\n",
      "[11] loss: 0.112\n",
      "[12] loss: 0.106\n",
      "[13] loss: 0.101\n",
      "[14] loss: 0.095\n",
      "[15] loss: 0.090\n",
      "[16] loss: 0.087\n",
      "[17] loss: 0.082\n",
      "[18] loss: 0.080\n",
      "[19] loss: 0.079\n",
      "[20] loss: 0.072\n",
      "Finished training!\n",
      "\n",
      "Training Centered. Run 4\n",
      "[1] loss: 0.514\n",
      "[2] loss: 0.223\n",
      "[3] loss: 0.170\n",
      "[4] loss: 0.142\n",
      "[5] loss: 0.126\n",
      "[6] loss: 0.112\n",
      "[7] loss: 0.102\n",
      "[8] loss: 0.095\n",
      "[9] loss: 0.084\n",
      "[10] loss: 0.083\n",
      "[11] loss: 0.076\n",
      "[12] loss: 0.070\n",
      "[13] loss: 0.064\n",
      "[14] loss: 0.061\n",
      "[15] loss: 0.060\n",
      "[16] loss: 0.054\n",
      "[17] loss: 0.051\n",
      "[18] loss: 0.049\n",
      "[19] loss: 0.045\n",
      "[20] loss: 0.044\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHkCAYAAAAnwrYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCU3qSimlVJT4fxzJZCuSeSWbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5bc6d8da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 4\n",
    "\n",
    "int_lr = 0.1\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IP Net. Run 1\n",
      "[1] loss: 2.003\n",
      "[2] loss: 1.899\n",
      "[3] loss: 1.844\n",
      "[4] loss: 1.835\n",
      "[5] loss: 1.829\n",
      "[6] loss: 1.814\n",
      "[7] loss: 1.797\n",
      "[8] loss: 1.797\n",
      "[9] loss: 1.801\n",
      "[10] loss: 1.775\n",
      "[11] loss: 1.766\n",
      "[12] loss: 1.764\n",
      "[13] loss: 1.750\n",
      "[14] loss: 1.734\n",
      "[15] loss: 1.731\n",
      "[16] loss: 1.737\n",
      "[17] loss: 1.737\n",
      "[18] loss: 1.729\n",
      "[19] loss: 1.703\n",
      "[20] loss: 1.694\n",
      "[21] loss: 1.685\n",
      "[22] loss: 1.675\n",
      "[23] loss: 1.675\n",
      "[24] loss: 1.672\n",
      "[25] loss: 1.664\n",
      "[26] loss: 1.655\n",
      "[27] loss: 1.659\n",
      "[28] loss: 1.649\n",
      "[29] loss: 1.641\n",
      "[30] loss: 1.646\n",
      "[31] loss: 1.654\n",
      "[32] loss: 1.651\n",
      "[33] loss: 1.645\n",
      "[34] loss: 1.639\n",
      "[35] loss: 1.634\n",
      "[36] loss: 1.625\n",
      "[37] loss: 1.608\n",
      "[38] loss: 1.614\n",
      "[39] loss: 1.608\n",
      "[40] loss: 1.601\n",
      "Finished training!\n",
      "\n",
      "Training Busted. Run 1\n",
      "[1] loss: 2.072\n",
      "[2] loss: 1.926\n",
      "[3] loss: 1.847\n",
      "[4] loss: 1.819\n",
      "[5] loss: 1.791\n",
      "[6] loss: 1.748\n",
      "[7] loss: 1.729\n",
      "[8] loss: 1.704\n",
      "[9] loss: 1.684\n",
      "[10] loss: 1.665\n",
      "[11] loss: 1.642\n",
      "[12] loss: 1.620\n",
      "[13] loss: 1.600\n",
      "[14] loss: 1.588\n",
      "[15] loss: 1.574\n",
      "[16] loss: 1.559\n",
      "[17] loss: 1.546\n",
      "[18] loss: 1.537\n",
      "[19] loss: 1.519\n",
      "[20] loss: 1.508\n",
      "[21] loss: 1.495\n",
      "[22] loss: 1.489\n",
      "[23] loss: 1.473\n",
      "[24] loss: 1.454\n",
      "[25] loss: 1.445\n",
      "[26] loss: 1.434\n",
      "[27] loss: 1.427\n",
      "[28] loss: 1.408\n",
      "[29] loss: 1.397\n",
      "[30] loss: 1.387\n",
      "[31] loss: 1.375\n",
      "[32] loss: 1.367\n",
      "[33] loss: 1.356\n",
      "[34] loss: 1.345\n",
      "[35] loss: 1.337\n",
      "[36] loss: 1.327\n",
      "[37] loss: 1.309\n",
      "[38] loss: 1.303\n",
      "[39] loss: 1.296\n",
      "[40] loss: 1.279\n",
      "Finished training!\n",
      "\n",
      "Training IP3 Net. Run 1\n",
      "[1] loss: 2.004\n",
      "[2] loss: 1.855\n",
      "[3] loss: 1.782\n",
      "[4] loss: 1.725\n",
      "[5] loss: 1.661\n",
      "[6] loss: 1.620\n",
      "[7] loss: 1.587\n",
      "[8] loss: 1.555\n",
      "[9] loss: 1.519\n",
      "[10] loss: 1.497\n",
      "[11] loss: 1.467\n",
      "[12] loss: 1.440\n",
      "[13] loss: 1.421\n",
      "[14] loss: 1.399\n",
      "[15] loss: 1.378\n",
      "[16] loss: 1.360\n",
      "[17] loss: 1.335\n",
      "[18] loss: 1.321\n",
      "[19] loss: 1.300\n",
      "[20] loss: 1.287\n",
      "[21] loss: 1.267\n",
      "[22] loss: 1.253\n",
      "[23] loss: 1.232\n",
      "[24] loss: 1.216\n",
      "[25] loss: 1.195\n",
      "[26] loss: 1.188\n",
      "[27] loss: 1.173\n",
      "[28] loss: 1.154\n",
      "[29] loss: 1.140\n",
      "[30] loss: 1.121\n",
      "[31] loss: 1.111\n",
      "[32] loss: 1.095\n",
      "[33] loss: 1.073\n",
      "[34] loss: 1.061\n",
      "[35] loss: 1.042\n",
      "[36] loss: 1.034\n",
      "[37] loss: 1.012\n",
      "[38] loss: 1.002\n",
      "[39] loss: 0.990\n",
      "[40] loss: 0.970\n",
      "Finished training!\n",
      "\n",
      "Training IP Net. Run 2\n",
      "[1] loss: 2.036\n",
      "[2] loss: 1.935\n",
      "[3] loss: 1.895\n",
      "[4] loss: 1.876\n",
      "[5] loss: 1.862\n",
      "[6] loss: 1.848\n",
      "[7] loss: 1.844\n",
      "[8] loss: 1.845\n",
      "[9] loss: 1.838\n",
      "[10] loss: 1.828\n",
      "[11] loss: 1.804\n",
      "[12] loss: 1.799\n",
      "[13] loss: 1.812\n",
      "[14] loss: 1.795\n",
      "[15] loss: 1.774\n",
      "[16] loss: 1.781\n",
      "[17] loss: 1.786\n",
      "[18] loss: 1.780\n",
      "[19] loss: 1.761\n",
      "[20] loss: 1.748\n",
      "[21] loss: 1.731\n",
      "[22] loss: 1.726\n",
      "[23] loss: 1.716\n",
      "[24] loss: 1.714\n",
      "[25] loss: 1.713\n",
      "[26] loss: 1.728\n",
      "[27] loss: 1.724\n",
      "[28] loss: 1.699\n",
      "[29] loss: 1.694\n",
      "[30] loss: 1.693\n",
      "[31] loss: 1.682\n",
      "[32] loss: 1.675\n",
      "[33] loss: 1.656\n",
      "[34] loss: 1.653\n",
      "[35] loss: 1.654\n",
      "[36] loss: 1.639\n",
      "[37] loss: 1.633\n",
      "[38] loss: 1.644\n",
      "[39] loss: 1.628\n",
      "[40] loss: 1.638\n",
      "Finished training!\n",
      "\n",
      "Training Busted. Run 2\n",
      "[1] loss: 2.063\n",
      "[2] loss: 1.924\n",
      "[3] loss: 1.864\n",
      "[4] loss: 1.826\n",
      "[5] loss: 1.794\n",
      "[6] loss: 1.771\n",
      "[7] loss: 1.736\n",
      "[8] loss: 1.712\n",
      "[9] loss: 1.680\n",
      "[10] loss: 1.655\n",
      "[11] loss: 1.637\n",
      "[12] loss: 1.616\n",
      "[13] loss: 1.601\n",
      "[14] loss: 1.586\n",
      "[15] loss: 1.573\n",
      "[16] loss: 1.560\n",
      "[17] loss: 1.550\n",
      "[18] loss: 1.530\n",
      "[19] loss: 1.520\n",
      "[20] loss: 1.505\n",
      "[21] loss: 1.496\n",
      "[22] loss: 1.492\n",
      "[23] loss: 1.472\n",
      "[24] loss: 1.467\n",
      "[25] loss: 1.447\n",
      "[26] loss: 1.439\n",
      "[27] loss: 1.427\n",
      "[28] loss: 1.417\n",
      "[29] loss: 1.409\n",
      "[30] loss: 1.396\n",
      "[31] loss: 1.382\n",
      "[32] loss: 1.373\n",
      "[33] loss: 1.364\n",
      "[34] loss: 1.350\n",
      "[35] loss: 1.342\n",
      "[36] loss: 1.331\n",
      "[37] loss: 1.324\n",
      "[38] loss: 1.313\n",
      "[39] loss: 1.305\n",
      "[40] loss: 1.292\n",
      "Finished training!\n",
      "\n",
      "Training IP3 Net. Run 2\n",
      "[1] loss: 1.986\n",
      "[2] loss: 1.824\n",
      "[3] loss: 1.767\n",
      "[4] loss: 1.712\n",
      "[5] loss: 1.666\n",
      "[6] loss: 1.621\n",
      "[7] loss: 1.592\n",
      "[8] loss: 1.562\n",
      "[9] loss: 1.532\n",
      "[10] loss: 1.500\n",
      "[11] loss: 1.478\n",
      "[12] loss: 1.453\n",
      "[13] loss: 1.434\n",
      "[14] loss: 1.415\n",
      "[15] loss: 1.399\n",
      "[16] loss: 1.377\n",
      "[17] loss: 1.359\n",
      "[18] loss: 1.337\n",
      "[19] loss: 1.322\n",
      "[20] loss: 1.303\n",
      "[21] loss: 1.288\n",
      "[22] loss: 1.272\n",
      "[23] loss: 1.245\n",
      "[24] loss: 1.237\n",
      "[25] loss: 1.211\n",
      "[26] loss: 1.198\n",
      "[27] loss: 1.182\n",
      "[28] loss: 1.167\n",
      "[29] loss: 1.153\n",
      "[30] loss: 1.137\n",
      "[31] loss: 1.118\n",
      "[32] loss: 1.107\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 150\n",
    "epochs = 40\n",
    "\n",
    "test_runs = 4\n",
    "\n",
    "int_lr = 0.1\n",
    "syn_lr = 0.001\n",
    "\n",
    "run_cifar_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
