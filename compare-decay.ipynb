{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as LR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "    \n",
    "class BN(nn.Module):\n",
    "    def __init__(self, layersize, eta=None):\n",
    "        super(BN, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        beta = x.mean(0, keepdim=True)\n",
    "        alpha = ((x-beta)**2).mean(0, keepdim=True).sqrt()\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-beta)/alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return o\n",
    "\n",
    "    def update(self, u, v, eta=None):\n",
    "        pass\n",
    "\n",
    "class IP(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(IP, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return  o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*(Euv))\n",
    "        self.beta = (1-eta)*self.beta + eta * (2*Ev)\n",
    "        \n",
    "        self.eta = eta * 0.99\n",
    "        \n",
    "        \n",
    "class ND(nn.Module):\n",
    "    def __init__(self, layersize, eta=1):\n",
    "        super(ND, self).__init__()\n",
    "        self.eta = eta\n",
    "        \n",
    "        # gain/bias are the learned output distribution params\n",
    "        self.gain = nn.Parameter(torch.ones(layersize))\n",
    "        self.bias = nn.Parameter(torch.zeros(layersize))\n",
    "        \n",
    "        # Alpha and beta are the ip normalization parameters\n",
    "        self.register_buffer('alpha', torch.ones(layersize))\n",
    "        self.register_buffer('beta', torch.zeros(layersize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize\n",
    "        nx = (x-self.beta)/self.alpha\n",
    "\n",
    "        # Adjust using learned parameters\n",
    "        o = self.gain*nx + self.bias\n",
    "        return  o\n",
    "        \n",
    "    def update(self, u, v, eta=None):\n",
    "\n",
    "        if (eta is None):\n",
    "            eta = self.eta\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Eu = u.mean(0, keepdim=True)\n",
    "            Euu = (u**2).mean(0, keepdim=True)\n",
    "            Ev = v.mean(0, keepdim=True)\n",
    "            Evv = (v**2).mean(0, keepdim=True)\n",
    "            Euv = (u*v).mean(0, keepdim=True)\n",
    "\n",
    "        self.alpha = (1-eta)*self.alpha + eta * (2*(Euv))\n",
    "        self.beta = (1-eta)*self.beta + eta * (2*Ev)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(28*28, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o\n",
    "    \n",
    "class CNet(nn.Module):\n",
    "    def __init__(self, layersize, norm=None, eta=1):\n",
    "        super(CNet, self).__init__()\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(3*32*32, layersize)\n",
    "        self.fc2 = nn.Linear(layersize, layersize)\n",
    "        self.fc3 = nn.Linear(layersize, 10)\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.n1 = norm(layersize, eta)\n",
    "        self.n2 = norm(layersize, eta)\n",
    "        \n",
    "    def forward(self, x, eta=None):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        u1 = self.fc1(x)\n",
    "        v1 = F.tanh(self.n1(u1))\n",
    "        u2 = self.fc2(v1)\n",
    "        v2 = F.tanh(self.n2(u2))\n",
    "        # Note you should not normalize after the last linear layer (you delete info)\n",
    "        o = F.relu(self.fc3(v2))\n",
    "        \n",
    "        # Lets do the updates to the normalizations\n",
    "        self.n1.update(u1, v1, eta)\n",
    "        self.n2.update(u2, v2, eta)\n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep_model(network, optimization, seed, epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    episode = 1\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimization.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y = network(inputs)\n",
    "            loss = criterion(y, labels)\n",
    "            loss.backward()\n",
    "            optimization.step()\n",
    "\n",
    "            # update statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 0\n",
    "            \n",
    "            loss_tracker.append([episode,loss.item()])\n",
    "            episode += 1\n",
    "            \n",
    "        print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1,running_loss / i))\n",
    "            \n",
    "    print(\"Finished training!\\n\")\n",
    "    return(np.transpose(loss_tracker))\n",
    "\n",
    "\n",
    "def run_mnist_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = Net(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training with decay. Run %d\" % (1))\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = Net(LAYERSIZE, ND, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training without decay. Run %d\" % (1))\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = Net(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training with decay. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = Net(LAYERSIZE, ND, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training without decay. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.ylim([-0.1, 3])\n",
    "#     plt.title(\"Learning curves for deep networks\")\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"With decay\")\n",
    "    # plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Without decay\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cifar_experiment(int_lr, syn_lr, epochs, test_runs):\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "    #Train IP Model\n",
    "    torch.manual_seed(seed)\n",
    "    IPnet = CNet(LAYERSIZE, IP, eta=int_lr)\n",
    "    IPnet = IPnet.to(device)\n",
    "\n",
    "    optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "    print(\"Training with decay. Run 1\")\n",
    "    ip_losses = train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "    #Train Standard Model\n",
    "    torch.manual_seed(seed)\n",
    "    net = CNet(LAYERSIZE, ND, eta=int_lr)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "    print(\"Training without decay. Run 1\")\n",
    "    standard_losses = train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    for i in range(test_runs-1):\n",
    "        seed = random.randint(0, 1000000)\n",
    "\n",
    "        #Train IP Model\n",
    "        torch.manual_seed(seed)\n",
    "        IPnet = CNet(LAYERSIZE, IP, eta=int_lr)\n",
    "        IPnet = IPnet.to(device)\n",
    "\n",
    "        optimizer1 = optim.Adam(IPnet.parameters(), lr=syn_lr)\n",
    "        print(\"Training with decay. Run %d\" % (i+2))\n",
    "        ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
    "\n",
    "\n",
    "        #Train Standard Model\n",
    "        torch.manual_seed(seed)\n",
    "        net = CNet(LAYERSIZE, ND, eta=int_lr)\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer2 = optim.Adam(net.parameters(), lr=syn_lr)\n",
    "        print(\"Training without decay. Run %d\" % (i+2))\n",
    "        standard_losses += train_deep_model(net, optimizer2, seed, epochs)\n",
    "\n",
    "    ip_losses = ip_losses/test_runs\n",
    "    standard_losses = standard_losses/test_runs\n",
    "\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.ylim([-0.1, 3])\n",
    "#     plt.title(\"Learning curves for shallow networks\")\n",
    "    plt.plot(ip_losses[0], ip_losses[1], label=\"With decay\")\n",
    "    # plt.plot(bn_losses[0], bn_losses[1], label=\"BN\")\n",
    "    plt.plot(standard_losses[0], standard_losses[1], label=\"Without decay\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# MNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total training batch number: 300\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print('==>>> total training batch number: {}'.format(len(trainloader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with decay. Run 1\n",
      "[1] loss: 0.312\n",
      "[2] loss: 0.148\n",
      "[3] loss: 0.115\n",
      "[4] loss: 0.100\n",
      "[5] loss: 0.083\n",
      "[6] loss: 0.075\n",
      "[7] loss: 0.069\n",
      "[8] loss: 0.066\n",
      "[9] loss: 0.055\n",
      "[10] loss: 0.056\n",
      "[11] loss: 0.053\n",
      "[12] loss: 0.046\n",
      "[13] loss: 0.050\n",
      "[14] loss: 0.044\n",
      "[15] loss: 0.038\n",
      "[16] loss: 0.042\n",
      "[17] loss: 0.044\n",
      "[18] loss: 0.033\n",
      "[19] loss: 0.035\n",
      "[20] loss: 0.039\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 1\n",
      "[1] loss: 0.305\n",
      "[2] loss: 0.141\n",
      "[3] loss: 0.108\n",
      "[4] loss: 0.090\n",
      "[5] loss: 0.074\n",
      "[6] loss: 0.068\n",
      "[7] loss: 0.059\n",
      "[8] loss: 0.054\n",
      "[9] loss: 0.048\n",
      "[10] loss: 0.045\n",
      "[11] loss: 0.043\n",
      "[12] loss: 0.042\n",
      "[13] loss: 0.041\n",
      "[14] loss: 0.031\n",
      "[15] loss: 0.035\n",
      "[16] loss: 0.033\n",
      "[17] loss: 0.031\n",
      "[18] loss: 0.028\n",
      "[19] loss: 0.032\n",
      "[20] loss: 0.027\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 2\n",
      "[1] loss: 0.317\n",
      "[2] loss: 0.149\n",
      "[3] loss: 0.110\n",
      "[4] loss: 0.096\n",
      "[5] loss: 0.086\n",
      "[6] loss: 0.080\n",
      "[7] loss: 0.064\n",
      "[8] loss: 0.062\n",
      "[9] loss: 0.058\n",
      "[10] loss: 0.053\n",
      "[11] loss: 0.047\n",
      "[12] loss: 0.049\n",
      "[13] loss: 0.044\n",
      "[14] loss: 0.041\n",
      "[15] loss: 0.043\n",
      "[16] loss: 0.037\n",
      "[17] loss: 0.040\n",
      "[18] loss: 0.044\n",
      "[19] loss: 0.039\n",
      "[20] loss: 0.036\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 2\n",
      "[1] loss: 0.312\n",
      "[2] loss: 0.141\n",
      "[3] loss: 0.102\n",
      "[4] loss: 0.086\n",
      "[5] loss: 0.073\n",
      "[6] loss: 0.065\n",
      "[7] loss: 0.059\n",
      "[8] loss: 0.052\n",
      "[9] loss: 0.044\n",
      "[10] loss: 0.046\n",
      "[11] loss: 0.044\n",
      "[12] loss: 0.038\n",
      "[13] loss: 0.040\n",
      "[14] loss: 0.034\n",
      "[15] loss: 0.031\n",
      "[16] loss: 0.026\n",
      "[17] loss: 0.032\n",
      "[18] loss: 0.026\n",
      "[19] loss: 0.023\n",
      "[20] loss: 0.024\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 3\n",
      "[1] loss: 0.303\n",
      "[2] loss: 0.149\n",
      "[3] loss: 0.116\n",
      "[4] loss: 0.098\n",
      "[5] loss: 0.085\n",
      "[6] loss: 0.078\n",
      "[7] loss: 0.069\n",
      "[8] loss: 0.059\n",
      "[9] loss: 0.057\n",
      "[10] loss: 0.053\n",
      "[11] loss: 0.056\n",
      "[12] loss: 0.058\n",
      "[13] loss: 0.043\n",
      "[14] loss: 0.044\n",
      "[15] loss: 0.043\n",
      "[16] loss: 0.041\n",
      "[17] loss: 0.036\n",
      "[18] loss: 0.035\n",
      "[19] loss: 0.031\n",
      "[20] loss: 0.039\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 3\n",
      "[1] loss: 0.297\n",
      "[2] loss: 0.142\n",
      "[3] loss: 0.108\n",
      "[4] loss: 0.086\n",
      "[5] loss: 0.077\n",
      "[6] loss: 0.069\n",
      "[7] loss: 0.059\n",
      "[8] loss: 0.054\n",
      "[9] loss: 0.052\n",
      "[10] loss: 0.044\n",
      "[11] loss: 0.045\n",
      "[12] loss: 0.040\n",
      "[13] loss: 0.038\n",
      "[14] loss: 0.036\n",
      "[15] loss: 0.035\n",
      "[16] loss: 0.027\n",
      "[17] loss: 0.030\n",
      "[18] loss: 0.030\n",
      "[19] loss: 0.028\n",
      "[20] loss: 0.026\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 4\n",
      "[1] loss: 0.296\n",
      "[2] loss: 0.149\n",
      "[3] loss: 0.117\n",
      "[4] loss: 0.101\n",
      "[5] loss: 0.089\n",
      "[6] loss: 0.078\n",
      "[7] loss: 0.069\n",
      "[8] loss: 0.063\n",
      "[9] loss: 0.060\n",
      "[10] loss: 0.055\n",
      "[11] loss: 0.056\n",
      "[12] loss: 0.053\n",
      "[13] loss: 0.046\n",
      "[14] loss: 0.041\n",
      "[15] loss: 0.041\n",
      "[16] loss: 0.044\n",
      "[17] loss: 0.039\n",
      "[18] loss: 0.036\n",
      "[19] loss: 0.035\n",
      "[20] loss: 0.040\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 4\n",
      "[1] loss: 0.289\n",
      "[2] loss: 0.142\n",
      "[3] loss: 0.110\n",
      "[4] loss: 0.090\n",
      "[5] loss: 0.078\n",
      "[6] loss: 0.070\n",
      "[7] loss: 0.062\n",
      "[8] loss: 0.054\n",
      "[9] loss: 0.050\n",
      "[10] loss: 0.048\n",
      "[11] loss: 0.045\n",
      "[12] loss: 0.042\n",
      "[13] loss: 0.038\n",
      "[14] loss: 0.039\n",
      "[15] loss: 0.030\n",
      "[16] loss: 0.032\n",
      "[17] loss: 0.031\n",
      "[18] loss: 0.030\n",
      "[19] loss: 0.027\n",
      "[20] loss: 0.027\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 5\n",
      "[1] loss: 0.321\n",
      "[2] loss: 0.154\n",
      "[3] loss: 0.116\n",
      "[4] loss: 0.102\n",
      "[5] loss: 0.086\n",
      "[6] loss: 0.077\n",
      "[7] loss: 0.065\n",
      "[8] loss: 0.063\n",
      "[9] loss: 0.058\n",
      "[10] loss: 0.050\n",
      "[11] loss: 0.053\n",
      "[12] loss: 0.049\n",
      "[13] loss: 0.048\n",
      "[14] loss: 0.047\n",
      "[15] loss: 0.044\n",
      "[16] loss: 0.042\n",
      "[17] loss: 0.038\n",
      "[18] loss: 0.036\n",
      "[19] loss: 0.031\n",
      "[20] loss: 0.038\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 5\n",
      "[1] loss: 0.314\n",
      "[2] loss: 0.145\n",
      "[3] loss: 0.108\n",
      "[4] loss: 0.092\n",
      "[5] loss: 0.078\n",
      "[6] loss: 0.068\n",
      "[7] loss: 0.057\n",
      "[8] loss: 0.056\n",
      "[9] loss: 0.051\n",
      "[10] loss: 0.046\n",
      "[11] loss: 0.043\n",
      "[12] loss: 0.037\n",
      "[13] loss: 0.035\n",
      "[14] loss: 0.034\n",
      "[15] loss: 0.032\n",
      "[16] loss: 0.035\n",
      "[17] loss: 0.033\n",
      "[18] loss: 0.029\n",
      "[19] loss: 0.023\n",
      "[20] loss: 0.028\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 6\n",
      "[1] loss: 0.314\n",
      "[2] loss: 0.152\n",
      "[3] loss: 0.122\n",
      "[4] loss: 0.099\n",
      "[5] loss: 0.090\n",
      "[6] loss: 0.080\n",
      "[7] loss: 0.070\n",
      "[8] loss: 0.062\n",
      "[9] loss: 0.061\n",
      "[10] loss: 0.056\n",
      "[11] loss: 0.051\n",
      "[12] loss: 0.054\n",
      "[13] loss: 0.049\n",
      "[14] loss: 0.044\n",
      "[15] loss: 0.045\n",
      "[16] loss: 0.041\n",
      "[17] loss: 0.038\n",
      "[18] loss: 0.035\n",
      "[19] loss: 0.036\n",
      "[20] loss: 0.036\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 6\n",
      "[1] loss: 0.308\n",
      "[2] loss: 0.145\n",
      "[3] loss: 0.113\n",
      "[4] loss: 0.090\n",
      "[5] loss: 0.079\n",
      "[6] loss: 0.067\n",
      "[7] loss: 0.058\n",
      "[8] loss: 0.052\n",
      "[9] loss: 0.049\n",
      "[10] loss: 0.044\n",
      "[11] loss: 0.042\n",
      "[12] loss: 0.043\n",
      "[13] loss: 0.038\n",
      "[14] loss: 0.035\n",
      "[15] loss: 0.032\n",
      "[16] loss: 0.033\n",
      "[17] loss: 0.031\n",
      "[18] loss: 0.028\n",
      "[19] loss: 0.027\n",
      "[20] loss: 0.028\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 7\n",
      "[1] loss: 0.303\n",
      "[2] loss: 0.144\n",
      "[3] loss: 0.112\n",
      "[4] loss: 0.096\n",
      "[5] loss: 0.088\n",
      "[6] loss: 0.071\n",
      "[7] loss: 0.066\n",
      "[8] loss: 0.064\n",
      "[9] loss: 0.057\n",
      "[10] loss: 0.055\n",
      "[11] loss: 0.045\n",
      "[12] loss: 0.047\n",
      "[13] loss: 0.053\n",
      "[14] loss: 0.044\n",
      "[15] loss: 0.040\n",
      "[16] loss: 0.035\n",
      "[17] loss: 0.034\n",
      "[18] loss: 0.033\n",
      "[19] loss: 0.044\n",
      "[20] loss: 0.037\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 7\n",
      "[1] loss: 0.298\n",
      "[2] loss: 0.137\n",
      "[3] loss: 0.104\n",
      "[4] loss: 0.086\n",
      "[5] loss: 0.075\n",
      "[6] loss: 0.066\n",
      "[7] loss: 0.059\n",
      "[8] loss: 0.053\n",
      "[9] loss: 0.048\n",
      "[10] loss: 0.043\n",
      "[11] loss: 0.041\n",
      "[12] loss: 0.037\n",
      "[13] loss: 0.035\n",
      "[14] loss: 0.038\n",
      "[15] loss: 0.032\n",
      "[16] loss: 0.031\n",
      "[17] loss: 0.026\n",
      "[18] loss: 0.026\n",
      "[19] loss: 0.030\n",
      "[20] loss: 0.029\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 8\n",
      "[1] loss: 0.323\n",
      "[2] loss: 0.156\n",
      "[3] loss: 0.123\n",
      "[4] loss: 0.101\n",
      "[5] loss: 0.089\n",
      "[6] loss: 0.082\n",
      "[7] loss: 0.071\n",
      "[8] loss: 0.063\n",
      "[9] loss: 0.064\n",
      "[10] loss: 0.059\n",
      "[11] loss: 0.054\n",
      "[12] loss: 0.053\n",
      "[13] loss: 0.049\n",
      "[14] loss: 0.050\n",
      "[15] loss: 0.041\n",
      "[16] loss: 0.043\n",
      "[17] loss: 0.044\n",
      "[18] loss: 0.045\n",
      "[19] loss: 0.042\n",
      "[20] loss: 0.030\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 8\n",
      "[1] loss: 0.318\n",
      "[2] loss: 0.147\n",
      "[3] loss: 0.113\n",
      "[4] loss: 0.092\n",
      "[5] loss: 0.078\n",
      "[6] loss: 0.070\n",
      "[7] loss: 0.063\n",
      "[8] loss: 0.054\n",
      "[9] loss: 0.051\n",
      "[10] loss: 0.047\n",
      "[11] loss: 0.043\n",
      "[12] loss: 0.040\n",
      "[13] loss: 0.041\n",
      "[14] loss: 0.039\n",
      "[15] loss: 0.029\n",
      "[16] loss: 0.029\n",
      "[17] loss: 0.035\n",
      "[18] loss: 0.032\n",
      "[19] loss: 0.031\n",
      "[20] loss: 0.024\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 9\n",
      "[1] loss: 0.333\n",
      "[2] loss: 0.161\n",
      "[3] loss: 0.126\n",
      "[4] loss: 0.106\n",
      "[5] loss: 0.086\n",
      "[6] loss: 0.080\n",
      "[7] loss: 0.072\n",
      "[8] loss: 0.069\n",
      "[9] loss: 0.063\n",
      "[10] loss: 0.055\n",
      "[11] loss: 0.051\n",
      "[12] loss: 0.052\n",
      "[13] loss: 0.050\n",
      "[14] loss: 0.042\n",
      "[15] loss: 0.046\n",
      "[16] loss: 0.036\n",
      "[17] loss: 0.042\n",
      "[18] loss: 0.038\n",
      "[19] loss: 0.036\n",
      "[20] loss: 0.039\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 9\n",
      "[1] loss: 0.328\n",
      "[2] loss: 0.153\n",
      "[3] loss: 0.115\n",
      "[4] loss: 0.097\n",
      "[5] loss: 0.078\n",
      "[6] loss: 0.070\n",
      "[7] loss: 0.061\n",
      "[8] loss: 0.056\n",
      "[9] loss: 0.049\n",
      "[10] loss: 0.045\n",
      "[11] loss: 0.044\n",
      "[12] loss: 0.038\n",
      "[13] loss: 0.038\n",
      "[14] loss: 0.033\n",
      "[15] loss: 0.036\n",
      "[16] loss: 0.032\n",
      "[17] loss: 0.036\n",
      "[18] loss: 0.027\n",
      "[19] loss: 0.024\n",
      "[20] loss: 0.025\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 10\n",
      "[1] loss: 0.313\n",
      "[2] loss: 0.153\n",
      "[3] loss: 0.120\n",
      "[4] loss: 0.102\n",
      "[5] loss: 0.084\n",
      "[6] loss: 0.077\n",
      "[7] loss: 0.073\n",
      "[8] loss: 0.063\n",
      "[9] loss: 0.062\n",
      "[10] loss: 0.051\n",
      "[11] loss: 0.050\n",
      "[12] loss: 0.047\n",
      "[13] loss: 0.045\n",
      "[14] loss: 0.042\n",
      "[15] loss: 0.047\n",
      "[16] loss: 0.038\n",
      "[17] loss: 0.037\n",
      "[18] loss: 0.035\n",
      "[19] loss: 0.042\n",
      "[20] loss: 0.030\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 10\n",
      "[1] loss: 0.303\n",
      "[2] loss: 0.145\n",
      "[3] loss: 0.109\n",
      "[4] loss: 0.089\n",
      "[5] loss: 0.076\n",
      "[6] loss: 0.066\n",
      "[7] loss: 0.053\n",
      "[8] loss: 0.054\n",
      "[9] loss: 0.049\n",
      "[10] loss: 0.041\n",
      "[11] loss: 0.041\n",
      "[12] loss: 0.040\n",
      "[13] loss: 0.037\n",
      "[14] loss: 0.033\n",
      "[15] loss: 0.032\n",
      "[16] loss: 0.030\n",
      "[17] loss: 0.026\n",
      "[18] loss: 0.028\n",
      "[19] loss: 0.027\n",
      "[20] loss: 0.027\n",
      "Finished training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7MAAAFBCAYAAACl2/hUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiknaUzIqIiIiIiEja+X8q1etEU9lEEQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e2085e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 50\n",
    "epochs = 20\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.25\n",
    "syn_lr = 0.03\n",
    "\n",
    "run_mnist_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with decay. Run 1\n",
      "[1] loss: 1.790\n",
      "[2] loss: 1.598\n",
      "[3] loss: 1.481\n",
      "[4] loss: 1.392\n",
      "[5] loss: 1.323\n",
      "[6] loss: 1.263\n",
      "[7] loss: 1.209\n",
      "[8] loss: 1.158\n",
      "[9] loss: 1.106\n",
      "[10] loss: 1.054\n",
      "[11] loss: 1.010\n",
      "[12] loss: 0.971\n",
      "[13] loss: 0.923\n",
      "[14] loss: 0.883\n",
      "[15] loss: 0.849\n",
      "[16] loss: 0.810\n",
      "[17] loss: 0.775\n",
      "[18] loss: 0.742\n",
      "[19] loss: 0.710\n",
      "[20] loss: 0.683\n",
      "[21] loss: 0.653\n",
      "[22] loss: 0.634\n",
      "[23] loss: 0.600\n",
      "[24] loss: 0.574\n",
      "[25] loss: 0.552\n",
      "[26] loss: 0.531\n",
      "[27] loss: 0.512\n",
      "[28] loss: 0.487\n",
      "[29] loss: 0.470\n",
      "[30] loss: 0.448\n",
      "[31] loss: 0.436\n",
      "[32] loss: 0.424\n",
      "[33] loss: 0.416\n",
      "[34] loss: 0.390\n",
      "[35] loss: 0.377\n",
      "[36] loss: 0.360\n",
      "[37] loss: 0.366\n",
      "[38] loss: 0.346\n",
      "[39] loss: 0.326\n",
      "[40] loss: 0.323\n",
      "[41] loss: 0.311\n",
      "[42] loss: 0.293\n",
      "[43] loss: 0.297\n",
      "[44] loss: 0.287\n",
      "[45] loss: 0.286\n",
      "[46] loss: 0.269\n",
      "[47] loss: 0.243\n",
      "[48] loss: 0.274\n",
      "[49] loss: 0.263\n",
      "[50] loss: 0.252\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 1\n",
      "[1] loss: 1.787\n",
      "[2] loss: 1.591\n",
      "[3] loss: 1.477\n",
      "[4] loss: 1.393\n",
      "[5] loss: 1.330\n",
      "[6] loss: 1.273\n",
      "[7] loss: 1.229\n",
      "[8] loss: 1.181\n",
      "[9] loss: 1.136\n",
      "[10] loss: 1.092\n",
      "[11] loss: 1.054\n",
      "[12] loss: 1.022\n",
      "[13] loss: 0.977\n",
      "[14] loss: 0.941\n",
      "[15] loss: 0.911\n",
      "[16] loss: 0.877\n",
      "[17] loss: 0.839\n",
      "[18] loss: 0.808\n",
      "[19] loss: 0.778\n",
      "[20] loss: 0.740\n",
      "[21] loss: 0.708\n",
      "[22] loss: 0.683\n",
      "[23] loss: 0.652\n",
      "[24] loss: 0.624\n",
      "[25] loss: 0.597\n",
      "[26] loss: 0.571\n",
      "[27] loss: 0.548\n",
      "[28] loss: 0.522\n",
      "[29] loss: 0.498\n",
      "[30] loss: 0.476\n",
      "[31] loss: 0.453\n",
      "[32] loss: 0.434\n",
      "[33] loss: 0.409\n",
      "[34] loss: 0.393\n",
      "[35] loss: 0.374\n",
      "[36] loss: 0.359\n",
      "[37] loss: 0.336\n",
      "[38] loss: 0.339\n",
      "[39] loss: 0.311\n",
      "[40] loss: 0.299\n",
      "[41] loss: 0.287\n",
      "[42] loss: 0.281\n",
      "[43] loss: 0.276\n",
      "[44] loss: 0.259\n",
      "[45] loss: 0.247\n",
      "[46] loss: 0.234\n",
      "[47] loss: 0.223\n",
      "[48] loss: 0.217\n",
      "[49] loss: 0.215\n",
      "[50] loss: 0.213\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 2\n",
      "[1] loss: 1.787\n",
      "[2] loss: 1.592\n",
      "[3] loss: 1.473\n",
      "[4] loss: 1.382\n",
      "[5] loss: 1.318\n",
      "[6] loss: 1.253\n",
      "[7] loss: 1.198\n",
      "[8] loss: 1.146\n",
      "[9] loss: 1.098\n",
      "[10] loss: 1.049\n",
      "[11] loss: 1.009\n",
      "[12] loss: 0.974\n",
      "[13] loss: 0.924\n",
      "[14] loss: 0.897\n",
      "[15] loss: 0.852\n",
      "[16] loss: 0.811\n",
      "[17] loss: 0.780\n",
      "[18] loss: 0.745\n",
      "[19] loss: 0.722\n",
      "[20] loss: 0.685\n",
      "[21] loss: 0.661\n",
      "[22] loss: 0.632\n",
      "[23] loss: 0.608\n",
      "[24] loss: 0.583\n",
      "[25] loss: 0.554\n",
      "[26] loss: 0.536\n",
      "[27] loss: 0.519\n",
      "[28] loss: 0.502\n",
      "[29] loss: 0.476\n",
      "[30] loss: 0.454\n",
      "[31] loss: 0.445\n",
      "[32] loss: 0.428\n",
      "[33] loss: 0.418\n",
      "[34] loss: 0.404\n",
      "[35] loss: 0.379\n",
      "[36] loss: 0.365\n",
      "[37] loss: 0.365\n",
      "[38] loss: 0.360\n",
      "[39] loss: 0.333\n",
      "[40] loss: 0.321\n",
      "[41] loss: 0.322\n",
      "[42] loss: 0.303\n",
      "[43] loss: 0.288\n",
      "[44] loss: 0.301\n",
      "[45] loss: 0.282\n",
      "[46] loss: 0.292\n",
      "[47] loss: 0.268\n",
      "[48] loss: 0.267\n",
      "[49] loss: 0.237\n",
      "[50] loss: 0.232\n",
      "Finished training!\n",
      "\n",
      "Training without decay. Run 2\n",
      "[1] loss: 1.787\n",
      "[2] loss: 1.591\n",
      "[3] loss: 1.477\n",
      "[4] loss: 1.391\n",
      "[5] loss: 1.330\n",
      "[6] loss: 1.270\n",
      "[7] loss: 1.223\n",
      "[8] loss: 1.174\n",
      "[9] loss: 1.132\n",
      "[10] loss: 1.090\n",
      "[11] loss: 1.052\n",
      "[12] loss: 1.018\n",
      "[13] loss: 0.981\n",
      "[14] loss: 0.944\n",
      "[15] loss: 0.909\n",
      "[16] loss: 0.870\n",
      "[17] loss: 0.845\n",
      "[18] loss: 0.805\n",
      "[19] loss: 0.781\n",
      "[20] loss: 0.747\n",
      "[21] loss: 0.718\n",
      "[22] loss: 0.684\n",
      "[23] loss: 0.660\n",
      "[24] loss: 0.631\n",
      "[25] loss: 0.609\n",
      "[26] loss: 0.575\n",
      "[27] loss: 0.555\n",
      "[28] loss: 0.532\n",
      "[29] loss: 0.500\n",
      "[30] loss: 0.482\n",
      "[31] loss: 0.459\n",
      "[32] loss: 0.443\n",
      "[33] loss: 0.418\n",
      "[34] loss: 0.411\n",
      "[35] loss: 0.381\n",
      "[36] loss: 0.366\n",
      "[37] loss: 0.358\n",
      "[38] loss: 0.335\n",
      "[39] loss: 0.317\n",
      "[40] loss: 0.312\n",
      "[41] loss: 0.302\n",
      "[42] loss: 0.296\n",
      "[43] loss: 0.265\n",
      "[44] loss: 0.271\n",
      "[45] loss: 0.255\n",
      "[46] loss: 0.251\n",
      "[47] loss: 0.242\n",
      "[48] loss: 0.229\n",
      "[49] loss: 0.226\n",
      "[50] loss: 0.197\n",
      "Finished training!\n",
      "\n",
      "Training with decay. Run 3\n",
      "[1] loss: 1.774\n",
      "[2] loss: 1.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1205:\n",
      "Process Process-1206:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/datasets/cifar.py\", line 99, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/transforms.py\", line 29, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/transforms.py\", line 112, in __call__\n",
      "    t.sub_(m).div_(s)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/datasets/cifar.py\", line 99, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/transforms.py\", line 29, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torchvision/transforms.py\", line 61, in __call__\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-27b4e5be891e>\", line 11, in <module>\n",
      "    run_cifar_experiment(int_lr, syn_lr, epochs, test_runs)\n",
      "  File \"<ipython-input-3-994fd0592e7d>\", line 133, in run_cifar_experiment\n",
      "    ip_losses += train_deep_model(IPnet, optimizer1, seed, epochs)\n",
      "  File \"<ipython-input-3-994fd0592e7d>\", line 14, in train_deep_model\n",
      "    for i, data in enumerate(trainloader):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 280, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 259, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/inspect.py\", line 671, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/inspect.py\", line 717, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/posixpath.py\", line 373, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/posixpath.py\", line 407, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/home/n5shaw/.conda/envs/nps/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 4441) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAYERSIZE = 150\n",
    "epochs = 50\n",
    "\n",
    "test_runs = 10\n",
    "\n",
    "int_lr = 0.25\n",
    "syn_lr = 0.005\n",
    "\n",
    "run_cifar_experiment(int_lr, syn_lr, epochs, test_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
